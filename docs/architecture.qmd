---
title: "Architecture Overview"
---

`naq` is designed to be a simple yet powerful distributed task queue with a comprehensive event-driven architecture. Its architecture is fundamentally built around [NATS](https://nats.io) and its persistence layer, [JetStream](https://docs.nats.io/jetstream/jetstream), which serve as the central nervous system for communication between clients, workers, and the scheduler.

## Core Components

The `naq` ecosystem consists of four main components:

1.  **The Client (Producer)**: Any application that enqueues jobs. This could be a web server, a script, or any other part of your system that needs to offload work.
2.  **NATS Server (with JetStream)**: The message broker that provides persistence, message delivery, and storage for job results and worker metadata.
3.  **The Worker(s)**: The processes that subscribe to queues, execute jobs, and report back their results. You can run as many workers as you need, on as many machines as you want.
4.  **The Scheduler**: A dedicated process that handles time-based events, such as scheduled jobs and recurring tasks.

## How It Works: The Job Lifecycle

Here is a high-level overview of what happens when a job is enqueued and processed, including the comprehensive event-driven architecture:


```{mermaid}
graph TD
    subgraph "Your Application"
        Client[Client]
        PythonCode[Python Code]
        EventHandler[Event Handler]
    end

    subgraph "NAQ Processes"
        Worker[Worker]
        Scheduler[Scheduler]
        Client[Client]
        EventProcessor[Event Processor]
    end

    subgraph "Service Layer"
        ServiceManager[ServiceManager]
        ConnectionService[ConnectionService]
        StreamService[StreamService]
        KVStoreService[KVStoreService]
        JobService[JobService]
        EventService[EventService]
        SchedulerService[SchedulerService]
    end

    subgraph "NATS Server (JetStream)"
        QueueStream[Queue Stream]
        ResultStore[Result KV Store]
        ScheduledJobs[Scheduled Jobs KV]
        EventStream[Event Stream]
    end

    Client -- "1. Enqueue Job" --> ServiceManager
    Worker -- "3. Fetch Job" --> ServiceManager
    Scheduler -- "8. Check for Due Jobs" --> ServiceManager
    
    ServiceManager -- "Initializes/Manages" --> ConnectionService
    ServiceManager -- "Initializes/Manages" --> StreamService
    ServiceManager -- "Initializes/Manages" --> KVStoreService
    ServiceManager -- "Initializes/Manages" --> JobService
    ServiceManager -- "Initializes/Manages" --> EventService
    ServiceManager -- "Initializes/Manages" --> SchedulerService

    ConnectionService -- "Manages NATS Connections" --> NATSClient[NATS Client]
    StreamService -- "Manages Streams" --> QueueStream
    KVStoreService -- "Manages KV Stores" --> ResultStore
    KVStoreService -- "Manages KV Stores" --> ScheduledJobs

    JobService -- "Enqueues Job" --> QueueStream
    JobService -- "Stores Result" --> ResultStore
    SchedulerService -- "Enqueues Due Job" --> QueueStream
    SchedulerService -- "Checks Scheduled Jobs" --> ScheduledJobs

    Client -- "2. Log ENQUEUED Event" --> EventService
    Worker -- "Log STARTED/COMPLETED/FAILED Events" --> EventService
    Scheduler -- "Log SCHEDULED/TRIGGERED Events" --> EventService
    EventService -- "Writes Events" --> EventStream

    EventStream -- "Stream Events" --> EventProcessor
    EventProcessor -- "Dispatch to Handlers" --> EventHandler

    Client --> PythonCode[Python Code]
    Worker --> PythonCode
    PythonCode -- "5. Return Result" --> Worker
```

## The Central Event Stream Approach

At the heart of naq's architecture is a centralized event stream that captures all job lifecycle events. This event-driven approach enables:

- **Complete visibility**: Every job state change is captured and stored durably
- **Real-time monitoring**: Events are available immediately as they occur
- **Decoupled processing**: Components can react to events without direct coupling
- **Shared event logging**: All components use the same event logging infrastructure

### Shared Event Logging

All naq components (Client, Worker, Scheduler) use the shared event logging system provided by the `SharedEventLoggerManager`. This ensures:

- **Consistency**: All events follow the same format and structure
- **Reliability**: Events are buffered and flushed efficiently to prevent loss
- **Performance**: Non-blocking logging with configurable batching
- **Centralized configuration**: Event logging is managed through a single point

### The Complete Event Lifecycle

1.  **Enqueueing**: The client calls an `enqueue` function (e.g., `enqueue_sync`). The function, its arguments, and other metadata are serialized into a job payload. This payload is then published as a message to a NATS subject that corresponds to the target queue.

2.  **Event Logging (Enqueue)**: Upon successful enqueuing, the client (or `Queue` instance) logs an `ENQUEUED` event to the dedicated **Event Stream** using the shared event logger.

3.  **Persistence**: NATS JetStream receives this message and persists it in a **Stream**. Each queue in `naq` maps directly to a JetStream Stream. This ensures that even if no workers are online, the job is safely stored and will be processed later.

4.  **Fetching**: A `naq` worker process is constantly listening on the queue's Stream. When a new job is available, the worker consumes the message, acknowledging it to NATS so that it isn't delivered to another worker.

5.  **Status Change**: As the worker begins processing, it logs a `STATUS_CHANGED` event to indicate the job has moved from PENDING to RUNNING state.

6.  **Event Logging (Worker - Start)**: The worker logs a `STARTED` event when it begins processing a job.

7.  **Execution**: The worker deserializes the job payload and executes the specified Python function with the provided arguments.

8.  **Result Handling**: Once the function completes, its return value is serialized. The worker then stores this result in a NATS **Key-Value (KV) Store**, using the unique job ID as the key. This result has a configurable Time-To-Live (TTL), after which it is automatically purged by NATS.

9.  **Status Change**: Upon completion, the worker logs another `STATUS_CHANGED` event to indicate the final state (COMPLETED or FAILED).

10. **Event Logging (Worker - Completion)**: The worker logs `COMPLETED` events when jobs finish successfully, or `FAILED` events when jobs encounter errors. If a job fails and is configured for retry, a `RETRY_SCHEDULED` event is logged. If a job is cancelled, a `CANCELLED` event is logged.

11. **Scheduled Jobs**: The `naq scheduler` process periodically scans a dedicated KV store for jobs that are due to run. When it finds one, it enqueues it into the appropriate queue, and the job follows the normal lifecycle from there.

12. **Event Logging (Scheduler)**: The scheduler logs `SCHEDULED` events when a new scheduled job is created and `SCHEDULE_TRIGGERED` events when a scheduled job is enqueued for execution.

13. **Event Processing**: The `AsyncJobEventProcessor` subscribes to the **Event Stream**. It reads events in real-time and dispatches them to any registered event handlers, allowing for reactive, event-driven applications.

### New Event Types

The event-driven architecture includes several new event types that provide comprehensive visibility into job processing:

- **CANCELLED**: Logged when a job is cancelled before or during execution
- **STATUS_CHANGED**: Logged whenever a job's status changes, providing detailed state transition tracking

These new events, combined with the existing event types (ENQUEUED, STARTED, COMPLETED, FAILED, RETRY_SCHEDULED, SCHEDULED, SCHEDULE_TRIGGERED), provide a complete picture of the job lifecycle from creation to completion.

### Event Correlation

All events include the job ID, enabling easy correlation of events across the entire lifecycle. This allows you to:

- Track the complete journey of a single job
- Measure timing between different stages
- Identify bottlenecks in processing
- Build comprehensive monitoring and alerting systems

## Why NATS?

Using NATS and JetStream as the foundation provides several key advantages:

-   **Decoupling**: Clients, workers, and the scheduler are completely decoupled. They only need to know how to talk to NATS, not to each other directly.
-   **Scalability**: You can add more workers at any time to increase your processing capacity. NATS handles the load balancing of jobs to available workers automatically.
-   **Resilience**: If a worker crashes, JetStream ensures that the job it was processing will be re-delivered to another worker after a timeout. If the entire `naq` system goes down, the jobs are safe in the NATS stream, ready to be processed when the system comes back online.
-   **Simplicity**: By offloading the complexities of persistence, delivery guarantees, and storage to NATS, the `naq` codebase can remain focused on the core logic of job execution and scheduling.

## Service Layer Architecture

The service layer architecture is a recent addition to `naq` that addresses the challenge of connection management and resource utilization across the system. Previously, the codebase suffered from over 44+ duplicated NATS connection patterns, leading to inefficient resource usage and complex connection management.

### Service Dependency Structure

```
CLI Commands
    ↓
Queue/Worker/Scheduler Classes
    ↓
Business Services (JobService, EventService, SchedulerService)
    ↓
Core Services (ConnectionService, StreamService, KVStoreService)
    ↓
NATS Infrastructure
```

### The Problem: Connection Pattern Duplication

Before the service layer, each component in `naq` (Queue, Worker, Scheduler, etc.) was responsible for creating and managing its own NATS connections. This approach led to several issues:

- **Resource Inefficiency**: Multiple connections to the same NATS server from the same process, leading to unnecessary resource consumption.
- **Code Duplication**: Similar connection logic repeated across 44+ locations in the codebase, increasing maintenance overhead.
- **Inconsistent Configuration**: Connection parameters and retry logic varied between components, making it difficult to enforce consistent behavior.
- **Difficult Maintenance**: Changes to connection handling required updates in multiple files, increasing the risk of errors.
- **Connection Leaks**: No centralized management for connection lifecycle, potentially leading to leaked connections and resource exhaustion.

### The Solution: Centralized Service Layer

The service layer architecture introduces a centralized approach to managing resources and dependencies through a set of specialized services. This layer acts as an intermediary, centralizing resource management and eliminating connection duplication by providing a single point of access for NATS interactions.

#### Core Services

1. **ConnectionService**: Manages NATS connections and JetStream contexts.
   - Provides connection pooling and reuse, reducing the number of open connections.
   - Handles connection retries with exponential backoff, improving resilience.
   - Centralizes connection configuration, ensuring consistent parameters across the application.
   - Manages connection lifecycle and health checks, preventing stale connections.

2. **StreamService**: Handles NATS JetStream stream operations.
   - Stream creation, configuration, and management.
   - Stream purging and message deletion.
   - Stream information retrieval.

3. **KVStoreService**: Manages NATS Key-Value store operations.
   - Centralized KV store access patterns.
   - TTL management for stored values.
   - Transaction support for atomic operations.

4. **JobService**: Handles job execution and result management.
   - Job execution with proper error handling.
   - Result storage and retrieval.
   - Job failure handling and retry logic.

5. **EventService**: Manages event logging and processing.
   - Event batching and buffering.
   - Event stream management.
   - Event correlation and tracking.

6. **SchedulerService**: Handles scheduled job management.
   - Job scheduling and triggering.
   - Recurring job management.
   - Schedule persistence and retrieval.

#### Service Manager

The [`ServiceManager`](src/naq/services/base.py:61) is the central component that:
- Creates and manages service instances.
- Handles service dependencies, ensuring services are initialized in the correct order.
- Ensures proper initialization and cleanup of all services.
- Provides a unified interface for accessing services, simplifying client code.

### Benefits of the Service Layer

The introduction of the service layer provides significant benefits:

1.  **Centralized Resource Management and Elimination of Connection Duplication**: The `ServiceManager` ensures that NATS connections, JetStream contexts, and other resources are managed centrally. Instead of each component creating its own connections, they obtain them from the `ConnectionService`. This pooling and reuse mechanism drastically reduces the number of open connections to the NATS server, optimizing resource utilization and preventing issues like connection leaks.

2.  **Consistent Error Handling**: All services are built with consistent error handling patterns and integrated logging. This ensures that errors are caught, logged, and propagated uniformly across the application, simplifying debugging and improving system reliability.

3.  **Reduced Code Duplication**: By encapsulating common NATS operations and business logic within services, a significant amount of repetitive code is eliminated. Components now interact with the service layer, leading to cleaner, more concise, and easier-to-maintain codebases.

4.  **Improved Testability**: The service layer design promotes dependency injection, making it easier to mock services during testing. This allows for isolated unit testing of individual components without requiring actual NATS connections or complex setups, leading to more robust and reliable tests.

5.  **Configuration-Driven Behavior**: Services are designed to be highly configurable, allowing their behavior to be driven by application settings. This means connection parameters, retry policies, and other operational aspects can be easily adjusted without code changes, providing greater flexibility and adaptability for different deployment environments.

#### Example: Obtaining NATS Connections and JetStream Contexts via `ServiceManager`

```python
# Before: Each component creates its own connection
class Queue:
    async def _get_js(self):
        self._nc = await nats.connect(self._nats_url)
        self._js = self._nc.jetstream()
        return self._js

# After: Use centralized ConnectionService via ServiceManager
class Queue:
    async def _get_js(self):
        if self._services is None:
            self._services = ServiceManager(self._config)
            await self._services.initialize() # Ensure ServiceManager is initialized
        
        connection_service = await self._services.get_service(ConnectionService)
        # Obtain NATS connection
        nc = await connection_service.get_connection(self._nats_url)
        # Obtain JetStream context from the managed connection
        self._js = nc.jetstream()
        return self._js
```

### Migration from Old Patterns

#### Before: Direct Connection Management

```python
# Old pattern - direct connection management
class OldQueue:
    def __init__(self, name, nats_url):
        self.name = name
        self.nats_url = nats_url
        self._nc = None
        self._js = None
    
    async def _get_js(self):
        if self._js is None:
            self._nc = await nats.connect(self.nats_url)
            self._js = self._nc.jetstream()
        return self._js
    
    async def close(self):
        if self._nc:
            await self._nc.close()
```

#### After: Service-Based Architecture

```python
# New pattern - service-based architecture
class NewQueue:
    def __init__(self, name, nats_url, services=None):
        self.name = name
        self.nats_url = nats_url
        self._services = services or ServiceManager({'nats': {'url': nats_url}})
        self._js = None
    
    async def _get_js(self):
        if self._js is None:
            connection_service = await self._services.get_service(ConnectionService)
            self._js = await connection_service.get_jetstream(self.nats_url)
        return self._js
    
    async def close(self):
        await self._services.cleanup_all()
```

### Service Layer in Action

Here's how the service layer works across different components:

```python
# Queue using services
async def enqueue_example():
    config = {'nats': {'url': 'nats://localhost:4222'}}
    
    async with ServiceManager(config) as services:
        queue = Queue(name='example', services=services)
        job = await queue.enqueue(my_function, arg1, arg2)
        
        # All connections managed by the service layer
        # No need to manually close connections

# Worker using services
async def worker_example():
    config = {'nats': {'url': 'nats://localhost:4222'}}
    
    async with ServiceManager(config) as services:
        worker = Worker(queues=['example'], services=services)
        await worker.run()
        
        # Services automatically cleaned up when context exits
```

### Best Practices for Using the Service Layer

1. **Use ServiceManager for Lifecycle Management**: Always use the ServiceManager as a context manager to ensure proper cleanup.

2. **Share ServiceManager Instances**: When possible, share a single ServiceManager instance across components in the same process.

3. **Configure Services Properly**: Provide appropriate configuration for connection parameters, retry logic, and timeouts.

4. **Handle Service Errors**: Implement proper error handling for service-related exceptions.

5. **Use Dependency Injection**: Leverage the service dependency system rather than manually creating service instances.

The service layer architecture represents a significant improvement in the design of `naq`, eliminating code duplication, improving resource efficiency, and providing a solid foundation for future enhancements.

## Connection Management

The connection management system in `naq` has been significantly enhanced to provide robust, efficient, and centralized handling of NATS connections. With the introduction of the service layer, the `ConnectionService` is now the primary mechanism for managing connections, superseding direct usage of `nats_helpers` for core components.

The system now provides:

1.  **Service-Oriented Management**: `ConnectionService` acts as the single source of truth for NATS connections, ensuring consistency and efficient resource utilization.
2.  **Context Manager API**: Modern, resource-safe connection management for transient operations, leveraging the underlying `ConnectionService`.
3.  **Monitoring Capabilities**: Built-in connection metrics and health monitoring for better operational visibility.
4.  **Configuration-Driven**: All connection parameters are driven by a centralized configuration system, allowing for flexible deployment.

### Architecture Flow

```
Application Code
    ↓ (Interacts with Service Layer)
ConnectionService (src/naq/services/connection.py)
    ↓
NATS Client
```

### Key Benefits

-   **Centralized Control**: All NATS connection logic is managed by the `ConnectionService`, eliminating duplication and ensuring consistent behavior.
-   **Automatic Resource Management**: Context managers, powered by `ConnectionService`, ensure proper acquisition and cleanup of NATS resources.
-   **Reduced Boilerplate**: Developers no longer need to write repetitive connection handling code.
-   **Consistent Patterns**: Uniform connection handling across all modules, improving code readability and maintainability.
-   **Production Ready**: Built-in monitoring, health checking, and robust error handling make the system suitable for production environments.

### `ConnectionService` in Action

The `ConnectionService` centralizes the logic for obtaining and managing NATS connections:

```python
from naq.services import ServiceManager, ConnectionService

async def get_nats_connection_via_service_manager(config):
    async with ServiceManager(config) as services:
        connection_service = await services.get_service(ConnectionService)
        nc = await connection_service.get_connection()
        print(f"Connected to NATS: {nc.is_connected}")
        return nc

async def get_jetstream_context_via_service_manager(config):
    async with ServiceManager(config) as services:
        connection_service = await services.get_service(ConnectionService)
        js = await connection_service.get_jetstream()
        print(f"JetStream context obtained: {js is not None}")
        return js
```

### Context Manager Implementation (Leveraging `ConnectionService`)

While `ConnectionService` is the primary mechanism, context managers are still available for convenience, now internally leveraging the service layer:

```python
# Basic NATS connection
from naq.connection.context_managers import nats_connection

async with nats_connection() as conn:
    await conn.publish("subject", b"message")

# JetStream context
from naq.connection.context_managers import nats_jetstream

async with nats_jetstream() as (conn, js):
    await js.add_stream(name="stream", subjects=["stream.*"])

# KeyValue operations
from naq.connection.context_managers import nats_kv_store

async with nats_kv_store("bucket") as kv:
    await kv.put("key", "value")
```

### Connection Monitoring

The system includes comprehensive monitoring capabilities, which are now more tightly integrated with the `ConnectionService`'s centralized management:

```python
from naq.services import ServiceManager, ConnectionService

async def monitor_connections(config):
    async with ServiceManager(config) as services:
        connection_service = await services.get_service(ConnectionService)
        metrics = connection_service.get_metrics() # Example method
        print(f"Total connections: {metrics.total_connections}")
        print(f"Active connections: {metrics.active_connections}")
        print(f"Failed connections: {metrics.failed_connections}")
```

### Migration Path

The connection management system provides a clear migration path from legacy patterns, encouraging the adoption of the service layer approach:

#### Legacy Pattern (Discouraged for new core development)
```python
# Old approach - manual connection management
nc = await get_nats_connection(url) # Direct utility import
try:
    js = await get_jetstream_context(nc) # Direct utility import
    await js.add_stream(config)
finally:
    await close_nats_connection(nc) # Direct utility import
```

#### Modern Pattern (Service-based)
```python
# New approach - automatic resource management via ServiceManager
async with ServiceManager(config) as services:
    connection_service = await services.get_service(ConnectionService)
    js = await connection_service.get_jetstream()
    await js.add_stream(config)
```

### Backward Compatibility

The new connection management system maintains full backward compatibility with existing code. While all legacy functions continue to work unchanged, new code should leverage the `ConnectionService` and the improved context manager API for optimal performance and maintainability.

### Production Considerations

For production deployments, the `ConnectionService` and the refactored connection management system provide:

1.  **Connection Pooling**: Efficient reuse of connections across components, reducing overhead.
2.  **Health Monitoring**: Built-in connection health checks and metrics for proactive issue detection.
3.  **Error Handling**: Comprehensive error handling and retry logic, enhancing system resilience.
4.  **Configuration Management**: Centralized configuration for all connection parameters, simplifying deployment and scaling.

This refactored connection management system significantly improves the reliability and maintainability of `naq` while providing a modern, Pythonic API for developers.

## Utils Package Architecture

The `utils` package is a comprehensive collection of utilities extracted from various modules throughout the codebase as part of Task 08. It provides common functionality used across all components of `naq`, eliminating code duplication and providing a consistent set of tools for common operations.

### Package Structure

The `utils` package is organized into specialized modules, each focusing on a specific area of functionality:

```
src/naq/utils/
├── __init__.py
├── async_helpers.py      # Async/sync conversion utilities
├── config.py            # Configuration management
├── context_managers.py  # Common context managers
├── decorators.py        # Reusable decorators
├── error_handling.py    # Centralized error handling
├── logging.py           # Logging utilities
├── nats_helpers.py      # NATS-specific utilities
├── retry.py             # Retry mechanisms
├── serialization.py     # Serialization helpers
├── timing.py            # Timing and benchmarking
├── types.py             # Common type definitions
└── validation.py        # Validation utilities
```

### Core Utilities

#### 1. Async Helpers (`async_helpers.py`)

Provides utilities for bridging synchronous and asynchronous code:

- **`run_async()`**: Execute async functions from sync code
- **`run_sync()`**: Execute sync functions from async code
- **`async_to_sync()`**: Convert async functions to sync functions
- **`sync_to_async()`**: Convert sync functions to async functions
- **`asyncify()`**: Decorator to make sync functions async-compatible

#### 2. Configuration Management (`config.py`)

Centralized configuration system with multiple source support:

- **`ConfigManager`**: Manages configuration from multiple sources with priority-based merging
- **`ConfigSource`**: Base class for configuration sources
- **EnvironmentConfigSource`**: Reads from environment variables
- **`FileConfigSource`**: Reads from YAML/JSON files
- **`DictConfigSource`**: Uses provided dictionaries
- **Configuration dataclasses**: Predefined configuration structures for different components

#### 3. Context Managers (`context_managers.py`)

Common context managers for resource management:

- **`managed_resource()`**: Generic resource management with cleanup
- **`timeout_context()`**: Timeout management for operations
- **`retry_context()`**: Retry logic with context management
- **`benchmark_context()`**: Performance benchmarking

#### 4. Decorators (`decorators.py`)

Reusable decorators for common patterns:

- **`retry_decorator()`**: Automatic retry logic with configurable strategies
- **`timeout_decorator()`**: Timeout enforcement for functions
- **`benchmark_decorator()`**: Performance measurement
- **`log_errors()`**: Automatic error logging
- **`validate_args()`**: Argument validation

#### 5. Error Handling (`error_handling.py`)

Centralized error handling and reporting:

- **`ErrorContext`**: Rich error context with metadata
- **`ErrorCategory`**: Categorization of errors
- **`ErrorHandler`**: Centralized error handling strategies
- **`ErrorReporter`**: Structured error reporting
- **Recovery strategies**: Configurable recovery mechanisms

#### 6. Logging (`logging.py`)

Comprehensive logging utilities:

- **Structured logging**: Context-aware logging with rich metadata
- **Performance logging**: Specialized logging for performance metrics
- **Log handler management**: Centralized log handler configuration
- **Log correlation**: Request/operation ID tracking

#### 7. NATS Helpers (`nats_helpers.py`)

The `nats_helpers.py` module contains NATS-specific utilities extracted from previous connection modules. With the introduction of the service layer, these functionalities are largely consumed internally by the `ConnectionService` and other core services (`StreamService`, `KVStoreService`), rather than being directly imported and used by application code for NATS interactions.

-   **Internal Consumption**: Provides foundational NATS utilities primarily for the service layer's internal operations.
-   **Service Layer Integration**: Functions within this module are now primarily orchestrated by the `ConnectionService` and other specialized services to ensure centralized and consistent NATS interactions.
-   **Reduced Direct Usage**: Application-level code is encouraged to interact with the `ServiceManager` and specific services (e.g., `ConnectionService`, `StreamService`) instead of directly importing from `nats_helpers.py`.

#### 8. Retry Mechanisms (`retry.py`)

Comprehensive retry system:

- **`RetryConfig`**: Configurable retry strategies
- **`retry_async()`**: Async retry with exponential backoff
- **`retry()`**: Sync retry with exponential backoff
- **Retry strategies**: Linear, exponential, custom strategies
- **Jitter support**: Avoid thundering herd problems

#### 9. Serialization (`serialization.py`)

Serialization utilities for jobs and results:

- **`JobSerializer`**: Job serialization with multiple formats
- **`ResultSerializer`**: Result serialization with TTL support
- **`SecureSerializer`**: Security-focused serialization
- **Serializer protocol**: Extensible serialization interface

#### 10. Timing and Benchmarking (`timing.py`)

Performance measurement and timing utilities:

- **`Timer`**: High-resolution timing for operations
- **`AsyncTimer`**: Async-compatible timing
- **`Benchmark`**: Comprehensive benchmarking tools
- **`Scheduler`**: Time-based scheduling utilities
- **Performance tracking**: Long-term performance monitoring

#### 11. Type Definitions (`types.py`)

Comprehensive type definitions for the entire system:

- **Basic type aliases**: Common types used throughout the codebase
- **Protocol definitions**: Interface definitions for common patterns
- **Generic types**: Reusable generic types
- **Type validation helpers**: Runtime type checking utilities
- **Model types**: Type definitions for core models
- **Configuration types**: Type definitions for configuration

#### 12. Validation (`validation.py`)

Comprehensive validation utilities:

- **Type validation**: Runtime type checking
- **Range validation**: Numeric range checking
- **Choice validation**: Enum/value choice validation
- **String validation**: String format validation
- **URL/email validation**: Specialized string validation
- **Dictionary validation**: Nested dictionary validation
- **Dataclass validation**: Automatic dataclass validation

### Integration with Core Components

The utils package integrates seamlessly with all core components of `naq`:

#### Queue Integration
```python
from naq.utils import retry_async, ErrorContext, ErrorCategory
from naq.utils.nats_helpers import nats_jetstream_context

class Queue:
    async def enqueue(self, func, *args, **kwargs):
        async with nats_jetstream_context() as (conn, js):
            return await retry_async(
                self._enqueue_impl,
                func, args, kwargs, js
            )
```

#### Worker Integration
```python
from naq.utils import Timer, ErrorContext
from naq.utils.logging import get_structured_logger

class Worker:
    async def process_job(self, job):
        with Timer() as timer:
            logger = get_structured_logger("worker")
            try:
                result = await job.execute()
                logger.info("Job completed", job_id=job.id, duration=timer.duration)
                return result
            except Exception as e:
                error_context = ErrorContext(
                    operation="job_execution",
                    exception=e,
                    job_id=job.id
                )
                error_context.category = ErrorCategory.EXECUTION
                logger.error("Job failed", error_context=error_context)
                raise
```

#### Service Layer Integration
```python
from naq.config import ConfigManager, load_naq_config
from naq.utils.types import Result, Error

class ServiceManager:
    def __init__(self, config=None):
        self.config = config or load_naq_config()
        self._config_manager = ConfigManager([DictConfigSource(self.config)])
```

### Benefits of the Utils Package

#### 1. Code Consolidation
- Eliminates duplication of common patterns across 44+ locations
- Provides single source of truth for common functionality
- Reduces maintenance burden and bug potential

#### 2. Consistency
- Unified error handling across all components
- Consistent logging formats and correlation
- Standardized retry and timeout behavior

#### 3. Testability
- All utilities are thoroughly tested
- Mockable interfaces for easy unit testing
- Clear separation of concerns

#### 4. Extensibility
- Protocol-based design allows easy extension
- Plugin architecture for custom serializers, validators, etc.
- Configurable behavior through settings

#### 5. Performance
- Optimized implementations for critical paths
- Minimal overhead for common operations
- Efficient resource management

### Migration Path

The utils package provides a clear migration path from the old scattered utilities:

#### Before: Scattered Utilities
```python
# In queue.py
def _get_nats_connection(self):
    # Custom connection logic
    pass

# In worker.py
def _get_nats_connection(self):
    # Similar but slightly different connection logic
    pass

# In scheduler.py
def _get_nats_connection(self):
    # Another variation of connection logic
    pass
```

#### After: Centralized Utils
```python
# In all components
from naq.utils.nats_helpers import nats_connection_context

async def _get_nats_connection(self):
    async with nats_connection_context() as conn:
        return conn
```

### Best Practices

#### 1. Using Utils in Components
```python
# Import specific utilities
from naq.utils import retry_async, Timer, ErrorContext
from naq.utils.nats_helpers import nats_jetstream_context
from naq.utils.logging import get_structured_logger

# Use in component methods
async def process_request(self, request):
    with Timer() as timer:
        logger = get_structured_logger("component")
        
        try:
            async with nats_jetstream_context() as (conn, js):
                result = await retry_async(
                    self._process_impl,
                    request, js
                )
                
                logger.info(
                    "Request processed",
                    request_id=request.id,
                    duration=timer.duration
                )
                return result
                
        except Exception as e:
            error_context = ErrorContext(
                operation="request_processing",
                exception=e,
                request_id=request.id
            )
            logger.error("Request failed", error_context=error_context)
            raise
```


#### 3. Error Handling
```python
from naq.utils import ErrorContext, ErrorCategory, ErrorReporter

async def risky_operation(self):
    try:
        return await self._do_risky_thing()
    except Exception as e:
        error_context = ErrorContext(
            operation="risky_operation",
            exception=e,
            component=self.__class__.__name__
        )
        error_context.category = ErrorCategory.EXECUTION
        error_context.add_metadata({"attempt": self._attempt})
        
        reporter = ErrorReporter()
        await reporter.report_error(error_context)
        raise
```

#### 4. Performance Monitoring
```python
from naq.utils import Timer, Benchmark
from naq.utils.timing import Scheduler

# Time individual operations
async def process_item(self, item):
    with Timer() as timer:
        result = await self._process_item_impl(item)
        
        # Log timing metrics
        self.metrics.observe("process_item_duration", timer.duration)
        return result

# Benchmark operations
async def benchmark_processing(self):
    benchmark = Benchmark()
    
    async with benchmark:
        for i in range(1000):
            await self.process_item(f"item_{i}")
    
    print(f"Processed 1000 items in {benchmark.total_time:.2f}s")
    print(f"Average time: {benchmark.average_time:.4f}s")
    print(f"Min time: {benchmark.min_time:.4f}s")
    print(f"Max time: {benchmark.max_time:.4f}s")
```

The utils package represents a significant architectural improvement for `naq`, providing a solid foundation of reusable, well-tested utilities that eliminate code duplication and ensure consistency across all components of the system.