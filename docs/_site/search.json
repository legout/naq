[
  {
    "objectID": "quickstart.html",
    "href": "quickstart.html",
    "title": "Quickstart Guide",
    "section": "",
    "text": "This guide will walk you through the basics of setting up a task, enqueuing it, and running a worker to process it."
  },
  {
    "objectID": "quickstart.html#set-up-configuration-optional",
    "href": "quickstart.html#set-up-configuration-optional",
    "title": "Quickstart Guide",
    "section": "1. Set Up Configuration (Optional)",
    "text": "1. Set Up Configuration (Optional)\nWhile not required for this quickstart, NAQ supports comprehensive YAML-based configuration that‚Äôs useful for production deployments. For now, we‚Äôll use the defaults, but you can create a configuration file:\n# Create a development configuration file (optional)\nnaq system config-init --environment development\nThis creates a naq.yaml file with sensible defaults. For this quickstart, the built-in defaults will work fine."
  },
  {
    "objectID": "quickstart.html#define-a-task",
    "href": "quickstart.html#define-a-task",
    "title": "Quickstart Guide",
    "section": "2. Define a Task",
    "text": "2. Define a Task\nFirst, create a Python file to define the function you want to run in the background. Let‚Äôs call this file tasks.py.\nThis function can be any regular Python function. For this example, we‚Äôll create a simple function that simulates some work and counts the words in a given text.\n# tasks.py\nimport time\nimport random\n\ndef count_words(text):\n    \"\"\"\n    A simple function that counts the words in a string.\n    \"\"\"\n    print(f\"Processing text: '{text[:30]}...'\")\n    # Simulate some I/O or CPU-bound work\n    time.sleep(random.randint(1, 3))\n    word_count = len(text.split())\n    print(f\"Found {word_count} words.\")\n    return word_count"
  },
  {
    "objectID": "quickstart.html#enqueue-the-job",
    "href": "quickstart.html#enqueue-the-job",
    "title": "Quickstart Guide",
    "section": "3. Enqueue the Job",
    "text": "3. Enqueue the Job\nNow, let‚Äôs enqueue the count_words function to be executed by a worker. Create another file, main.py, to send the job to the queue.\nWe‚Äôll use the enqueue_sync function, which is a simple, blocking way to add a job to the queue.\n# main.py\nfrom naq import enqueue_sync  # Main API (backward compatible)\nfrom tasks import count_words\n\n# Alternative: Use new modular imports (optional)\n# from naq.queue.sync_api import enqueue_sync\n\n# The text we want to process\nlong_text = (\n    \"A journey of a thousand miles begins with a single step. \"\n    \"The best time to plant a tree was 20 years ago. \"\n    \"The second best time is now.\"\n)\n\nprint(\"Enqueuing job to count words...\")\n\n# Enqueue the function `count_words` with `long_text` as its argument\njob = enqueue_sync(count_words, long_text)\n\nprint(f\"Successfully enqueued job {job.job_id}.\")\nprint(\"To process the job, run a worker with: naq worker default\")"
  },
  {
    "objectID": "quickstart.html#run-the-worker",
    "href": "quickstart.html#run-the-worker",
    "title": "Quickstart Guide",
    "section": "4. Run the Worker",
    "text": "4. Run the Worker\nWith the job enqueued, the final step is to start a worker process. The worker will connect to NATS, fetch the job from the queue, and execute the count_words function.\nOpen your terminal and run the following command:\nnaq worker default\nThe default argument tells the worker to listen to the default queue, which is where enqueue_sync sends jobs.\nYou should see output similar to this in your worker‚Äôs terminal:\n14:30:15.123 INFO     Worker listening on queue: naq_default_queue\nProcessing text: 'A journey of a thousand miles...'\nFound 25 words.\n14:30:18.245 INFO     Job 1a2b3c4d completed. Result: 25\nCongratulations! You‚Äôve successfully enqueued and processed your first background job with naq."
  },
  {
    "objectID": "quickstart.html#monitor-your-jobs",
    "href": "quickstart.html#monitor-your-jobs",
    "title": "Quickstart Guide",
    "section": "5. Monitor Your Jobs",
    "text": "5. Monitor Your Jobs\nOne of NAQ‚Äôs most powerful features is its comprehensive event-driven monitoring system. Every job operation is logged as structured events that you can monitor in real-time.\n\nReal-time Job Monitoring\nOpen a new terminal and run the events monitor to see your jobs in action:\n# Watch all job events in real-time\nnaq events\nThis will show a live stream of all job lifecycle events as they happen. Now go back to your main.py and run it again to see the events flowing:\npython main.py\nYou should see events like:\n12:34:56.789  job-abc123   enqueued      default  -        Job enqueued to default\n12:34:56.823  job-abc123   started       default  worker-1 Job started by worker-1  \n12:34:59.156  job-abc123   completed     default  worker-1 Job completed in 2333ms\n\n\nMonitor Specific Jobs\nYou can also monitor events for a specific job:\n# Monitor events for a specific job ID\nnaq events --job-id job-abc123\n\n# Monitor only failed jobs\nnaq events --event-type failed\n\n# Monitor events for a specific queue\nnaq events --queue my-queue\n\n\nJob Event History\nGet the complete event history for any job:\n# Get event history for a specific job\nnaq event-history job-abc123\nThis shows the complete lifecycle of a job with timestamps, durations, and detailed information.\n\n\nWorker Status Monitoring\nMonitor your workers‚Äô health and activity:\n# Watch worker lifecycle events\nnaq worker-events\n\n# Check worker status\nnaq list-workers\n\n\nSystem Health\nGet overall system statistics:\n# Show system event statistics\nnaq event-stats\n\n\nWhy This Matters\nThis comprehensive monitoring gives you:\n\nInstant Visibility: See exactly what‚Äôs happening with your jobs\nEasy Debugging: Trace job failures with complete event history\n\nPerformance Insights: Monitor job durations and worker utilization\nProduction Readiness: Built-in observability for production deployments"
  },
  {
    "objectID": "quickstart.html#whats-next",
    "href": "quickstart.html#whats-next",
    "title": "Quickstart Guide",
    "section": "What‚Äôs Next?",
    "text": "What‚Äôs Next?\n\nLearn about Event-Driven State Management for advanced monitoring.\nExplore how to schedule jobs to run in the future.\nLearn about the architecture of naq.\nCheck out more complex examples."
  },
  {
    "objectID": "api/scheduler.html",
    "href": "api/scheduler.html",
    "title": "Scheduler API",
    "section": "",
    "text": "The scheduler module contains the Scheduler class, which is responsible for finding and enqueuing scheduled and recurring jobs."
  },
  {
    "objectID": "api/scheduler.html#naq.scheduler.scheduler",
    "href": "api/scheduler.html#naq.scheduler.scheduler",
    "title": "Scheduler API",
    "section": "naq.scheduler.Scheduler",
    "text": "naq.scheduler.Scheduler\nYou typically run the scheduler from the command line using naq scheduler, but you can also create and run a Scheduler instance programmatically.\n\nnaq.scheduler.Scheduler(nats_url, poll_interval, instance_id, enable_ha)\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\nnats_url\nstr\nThe URL of the NATS server.\n\n\npoll_interval\nfloat\nThe interval (in seconds) at which the scheduler checks for due jobs. Defaults to 1.0.\n\n\ninstance_id\nstr | None\nA unique ID for the scheduler instance, used for High Availability. A unique ID is generated if not provided.\n\n\nenable_ha\nbool\nWhether to enable High Availability (HA) mode with leader election. Defaults to True.\n\n\n\n\n\nMethods\n\nrun()\nStarts the scheduler‚Äôs main processing loop. This is an async method.\nasync def run(self) -&gt; None\nThe scheduler will connect to NATS and, if it becomes the leader (or if HA is disabled), it will start polling for jobs that are ready to be enqueued.\n\n\n\nHigh Availability (HA)\nWhen enable_ha is True, you can run multiple Scheduler instances for redundancy. They will use a leader election protocol built on a NATS KV store to ensure that only one instance is actively scheduling jobs at any given time. If the leader instance goes down, another instance will automatically take over."
  },
  {
    "objectID": "api/job.html",
    "href": "api/job.html",
    "title": "Job API",
    "section": "",
    "text": "The Job class represents a unit of work that is enqueued and executed by a worker. You typically don‚Äôt create Job instances directly; they are created for you when you call functions like naq.enqueue().\nA Job instance is returned every time you enqueue a task, and it serves as a handle to that task."
  },
  {
    "objectID": "api/job.html#naq.job.job",
    "href": "api/job.html#naq.job.job",
    "title": "Job API",
    "section": "naq.job.Job",
    "text": "naq.job.Job\n\nProperties\n\n\n\n\n\n\n\n\nProperty\nType\nDescription\n\n\n\n\njob_id\nstr\nA unique identifier for the job.\n\n\nfunction\nCallable\nThe function that will be executed.\n\n\nargs\ntuple\nThe positional arguments passed to the function.\n\n\nkwargs\ndict\nThe keyword arguments passed to the function.\n\n\nqueue_name\nstr\nThe name of the queue the job belongs to.\n\n\nstatus\nJOB_STATUS\nThe current status of the job (pending, running, completed, failed).\n\n\nmax_retries\nint\nThe maximum number of times the job will be retried if it fails.\n\n\nretry_delay\nint | float | list\nThe delay (in seconds) between retries. Can be a single value or a list.\n\n\nretry_strategy\nstr\nThe retry strategy (linear or exponential).\n\n\ndepends_on\nlist[str] | None\nA list of job IDs that this job depends on.\n\n\nresult_ttl\nint | None\nThe time-to-live (in seconds) for the job‚Äôs result.\n\n\ntimeout\nint | None\nThe maximum time (in seconds) the job is allowed to run.\n\n\nenqueue_time\nfloat\nThe timestamp when the job was enqueued.\n\n\nerror\nstr | None\nThe error message if the job failed.\n\n\ntraceback\nstr | None\nThe traceback if the job failed.\n\n\n\n\n\nMethods\n\nfetch_result()\nA static method to fetch the result of a completed job.\n@staticmethod\nasync def fetch_result(job_id: str, nats_url: str = DEFAULT_NATS_URL) -&gt; Any\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\njob_id\nstr\nThe ID of the job whose result you want to fetch.\n\n\nnats_url\nstr\nThe URL of the NATS server.\n\n\n\nReturns: The return value of the job‚Äôs function.\nRaises:\n\nJobNotFoundError: If the job result is not found (it may not have completed, or the result may have expired).\nJobExecutionError: If the job failed. The exception message will contain the error and traceback from the worker.\n\n\n\nfetch_result_sync()\nA synchronous version of fetch_result().\n\n\n\n\n\n\nNote\n\n\n\nfetch_result_sync is deprecated and will be removed in a future version. Please use fetch_result in an async context."
  },
  {
    "objectID": "api/index.html",
    "href": "api/index.html",
    "title": "API Reference",
    "section": "",
    "text": "This section provides a detailed reference for the public API of the naq library.\nThe API is organized into modules, each providing specific functionality."
  },
  {
    "objectID": "api/index.html#core-modules",
    "href": "api/index.html#core-modules",
    "title": "API Reference",
    "section": "Core Modules",
    "text": "Core Modules\n\nqueue Module: The primary interface for enqueuing and scheduling jobs. Includes the Queue class and helper functions like enqueue, enqueue_at, and schedule.\njob Module: Defines the Job class, which represents a unit of work to be executed.\nworker Module: Contains the Worker class, responsible for executing jobs from one or more queues.\nscheduler Module: Contains the Scheduler class, responsible for enqueuing scheduled and recurring jobs.\nevents Module: Comprehensive event-driven state management system for job monitoring, logging, and reactive programming.\nexceptions Module: Defines custom exceptions raised by naq."
  },
  {
    "objectID": "api/queue.html",
    "href": "api/queue.html",
    "title": "Queue API",
    "section": "",
    "text": "The queue module provides the primary interface for adding jobs to naq."
  },
  {
    "objectID": "api/queue.html#queue-class",
    "href": "api/queue.html#queue-class",
    "title": "Queue API",
    "section": "Queue Class",
    "text": "Queue Class\nThe Queue class represents a job queue and is the main entry point for enqueuing tasks.\n\n\n\n\n\n\nNote\n\n\n\nFor simple, one-off enqueueing, you might prefer the helper functions like enqueue which manage the Queue instance for you.\n\n\n\nnaq.queue.Queue(name, nats_url, default_timeout)\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\nname\nstr\nThe name of the queue. Defaults to naq_default_queue.\n\n\nnats_url\nstr\nThe URL of the NATS server. Defaults to nats://localhost:4222.\n\n\ndefault_timeout\nint | None\nThe default timeout in seconds for jobs in this queue.\n\n\n\n\n\nMethods\n\nenqueue()\nEnqueues a job for immediate execution.\nasync def enqueue(\n    self,\n    func: Callable,\n    *args: Any,\n    max_retries: Optional[int] = 0,\n    retry_delay: RetryDelayType = 0,\n    depends_on: Optional[Union[str, List[str], Job, List[Job]]] = None,\n    timeout: Optional[int] = None,\n    **kwargs: Any\n) -&gt; Job\n\n\nenqueue_at()\nSchedules a job to be enqueued at a specific datetime.\nasync def enqueue_at(\n    self,\n    dt: datetime.datetime,\n    func: Callable,\n    *args: Any,\n    ...\n) -&gt; Job\n\n\nenqueue_in()\nSchedules a job to be enqueued after a timedelta.\nasync def enqueue_in(\n    self,\n    delta: timedelta,\n    func: Callable,\n    *args: Any,\n    ...\n) -&gt; Job\n\n\nschedule()\nSchedules a job to run on a recurring basis.\nasync def schedule(\n    self,\n    func: Callable,\n    *args: Any,\n    cron: Optional[str] = None,\n    interval: Optional[Union[timedelta, float, int]] = None,\n    repeat: Optional[int] = None,\n    ...\n) -&gt; Job\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\ncron\nstr\nA cron string (e.g., '*/5 * * * *') for the schedule.\n\n\ninterval\ntimedelta | float | int\nThe interval in seconds or as a timedelta between job runs.\n\n\nrepeat\nint | None\nThe number of times to repeat the job. None for indefinitely.\n\n\n\n\n\npurge()\nRemoves all jobs from the queue.\nasync def purge(self) -&gt; int\nReturns the number of jobs purged.\n\n\ncancel_scheduled_job()\nCancels a scheduled or recurring job.\nasync def cancel_scheduled_job(self, job_id: str) -&gt; bool\nReturns True if the job was found and canceled."
  },
  {
    "objectID": "api/queue.html#enqueue-functions",
    "href": "api/queue.html#enqueue-functions",
    "title": "Queue API",
    "section": "Enqueue Functions",
    "text": "Enqueue Functions\nThese helper functions provide a simpler way to enqueue jobs without needing to manage a Queue instance yourself. They are available in both async and sync versions.\n\nAsync Helpers\n\nnaq.enqueue()\nnaq.enqueue_at()\nnaq.enqueue_in()\nnaq.schedule()\nnaq.purge_queue()\nnaq.cancel_scheduled_job()\n\n\n\nSync Helpers\nFor use in synchronous code, naq provides sync versions of the enqueue functions. These functions automatically manage an event loop and use a thread-local connection for efficiency.\n\nnaq.enqueue_sync()\nnaq.enqueue_at_sync()\nnaq.enqueue_in_sync()\nnaq.schedule_sync()\nnaq.purge_queue_sync()\nnaq.cancel_scheduled_job_sync()\nnaq.close_sync_connections()"
  },
  {
    "objectID": "advanced.html",
    "href": "advanced.html",
    "title": "Advanced Usage",
    "section": "",
    "text": "This section covers advanced features and configuration options for optimizing naq in production environments."
  },
  {
    "objectID": "advanced.html#utility-patterns-for-robust-applications",
    "href": "advanced.html#utility-patterns-for-robust-applications",
    "title": "Advanced Usage",
    "section": "Utility Patterns for Robust Applications",
    "text": "Utility Patterns for Robust Applications\nNAQ provides a comprehensive set of utility patterns that help you build robust, maintainable applications. These utilities handle common patterns like error handling, retries, logging, and performance monitoring.\n\nError Handling and Retry Patterns\nUse NAQ‚Äôs retry decorators and error handling utilities to make your job functions more resilient:\nfrom naq.utils import retry, log_errors, async_error_handler_context\nfrom naq.utils.error_handling import get_global_error_handler\nfrom naq import enqueue_async\n\n# Configure global error handler\nerror_handler = get_global_error_handler()\nerror_handler.register_handler(\n    ConnectionError,\n    lambda err, ctx, extra: print(f\"Connection failed in {ctx}: {err}\")\n)\n\n@retry(max_attempts=3, delay=2.0, backoff=\"exponential\")\n@log_errors(reraise=True)\nasync def unreliable_api_call(user_id: int):\n    \"\"\"Job function with automatic retry and error logging.\"\"\"\n    async with async_error_handler_context(\n        error_handler, f\"api_call_user_{user_id}\"\n    ):\n        # Simulate unreliable API call\n        if random.random() &lt; 0.3:  # 30% failure rate\n            raise ConnectionError(\"API temporarily unavailable\")\n        return f\"Data for user {user_id}\"\n\n# Enqueue the job - retries happen automatically\nawait enqueue_async(unreliable_api_call, 123)\n\n\nPerformance Monitoring and Structured Logging\nTrack performance and use structured logging for better observability:\nfrom naq.utils import timing, StructuredLogger, performance_context\nfrom naq import enqueue_async\n\n# Create structured logger with context\nlogger = StructuredLogger(\"data_processor\", {\n    \"service\": \"background_jobs\",\n    \"version\": \"1.0.0\"\n})\n\n@timing(threshold_ms=5000)  # Log if takes more than 5 seconds\nasync def process_large_dataset(dataset_id: str, batch_size: int = 1000):\n    \"\"\"Process large dataset with performance monitoring.\"\"\"\n    \n    async with performance_context(\"dataset_processing\", logger) as perf:\n        # Add custom metrics to performance context\n        perf['dataset_id'] = dataset_id\n        perf['batch_size'] = batch_size\n        \n        with logger.operation_context(\"load_dataset\", dataset_id=dataset_id):\n            dataset = await load_dataset(dataset_id)\n            logger.info(f\"Loaded dataset with {len(dataset)} records\")\n        \n        with logger.operation_context(\"process_batches\", batch_count=len(dataset) // batch_size):\n            results = []\n            for i in range(0, len(dataset), batch_size):\n                batch = dataset[i:i + batch_size]\n                result = await process_batch(batch)\n                results.extend(result)\n                \n                # Log progress\n                logger.info(\n                    f\"Processed batch {i // batch_size + 1}\",\n                    batch_index=i // batch_size + 1,\n                    records_processed=len(results)\n                )\n        \n        return results\n\n# Enqueue with structured context\nlogger.info(\"Enqueueing dataset processing job\", dataset_id=\"dataset_123\")\nawait enqueue_async(process_large_dataset, \"dataset_123\", batch_size=500)\n\n\nAsync Utilities and Concurrency Control\nControl concurrency and manage async operations efficiently:\nfrom naq.utils.async_helpers import gather_with_concurrency, async_map\nfrom naq import enqueue_async\n\nasync def fetch_user_data(user_id: int):\n    \"\"\"Fetch data for a single user.\"\"\"\n    # Simulate API call\n    await asyncio.sleep(0.1)\n    return f\"Data for user {user_id}\"\n\nasync def bulk_user_processing(user_ids: list[int]):\n    \"\"\"Process multiple users with controlled concurrency.\"\"\"\n    \n    # Process users with max 10 concurrent operations\n    tasks = [fetch_user_data(uid) for uid in user_ids]\n    results = await gather_with_concurrency(tasks, concurrency=10)\n    \n    # Alternative: use async_map for the same result\n    # results = await async_map(fetch_user_data, user_ids, concurrency=10)\n    \n    return results\n\n# Enqueue bulk processing job\nuser_ids = list(range(1, 101))  # 100 users\nawait enqueue_async(bulk_user_processing, user_ids)\n\n\nContext Managers for Resource Management\nUse context managers for safe resource handling:\nfrom naq.utils.context_managers import timeout_context, managed_resource\nfrom naq import enqueue_async\n\nasync def acquire_database():\n    \"\"\"Acquire database connection.\"\"\"\n    return await database.connect()\n\nasync def release_database(conn):\n    \"\"\"Release database connection.\"\"\"\n    await conn.close()\n\nasync def database_heavy_job(query: str):\n    \"\"\"Job that requires database access with timeout.\"\"\"\n    \n    async with timeout_context(30.0, \"Database operation timeout\"):\n        async with managed_resource(acquire_database, release_database) as db:\n            # Database operations are automatically timed out and cleaned up\n            result = await db.execute(query)\n            return result.fetchall()\n\n# Enqueue database job with automatic resource management\nawait enqueue_async(database_heavy_job, \"SELECT * FROM large_table\")"
  },
  {
    "objectID": "advanced.html#efficient-connection-handling-batching",
    "href": "advanced.html#efficient-connection-handling-batching",
    "title": "Advanced Usage",
    "section": "Efficient Connection Handling & Batching",
    "text": "Efficient Connection Handling & Batching\nWhen enqueuing many jobs in a tight loop, creating a new NATS connection for each job is inefficient. naq provides several ways to manage connections for high-throughput scenarios.\n\nUsing a Queue Instance (Async)\nFor asynchronous applications, the most efficient way to enqueue jobs is to instantiate a Queue object and reuse it. The Queue instance manages a persistent connection to NATS.\n# async_batch_enqueue.py\nimport asyncio\nfrom naq.queue import Queue\n\nasync def my_task(i):\n    return f\"Processed item {i}\"\n\nasync def main():\n    # Create a single Queue instance for the 'high_volume' queue\n    queue = Queue(name=\"high_volume\")\n\n    print(\"Enqueuing 1,000 jobs using a single connection...\")\n    tasks = []\n    for i in range(1000):\n        task = queue.enqueue(my_task, i)\n        tasks.append(task)\n\n    await asyncio.gather(*tasks)\n    print(\"All jobs enqueued.\")\n\n    # The connection remains open until the Queue object is no longer in use\n    # or explicitly closed.\n    await queue.close()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\n\nThread-Local Connections (Sync)\nThe synchronous helper functions (enqueue_sync, enqueue_at_sync, etc.) are optimized for batching out of the box. They automatically use a thread-local NATS connection. This means that all calls to these functions from the same thread will reuse the same connection, avoiding the overhead of reconnecting each time.\n# sync_batch_enqueue.py\nfrom naq import enqueue_sync, close_sync_connections\n\ndef my_task(i):\n    return f\"Processed item {i}\"\n\ndef main():\n    print(\"Enqueuing 1,000 jobs using a thread-local connection...\")\n    for i in range(1000):\n        enqueue_sync(my_task, i)\n    print(\"All jobs enqueued.\")\n\n    # Optionally, you can explicitly close the thread-local connection\n    # when you are done with a batch. This is not required, as connections\n    # are also closed on process exit.\n    close_sync_connections()\n    print(\"Thread-local connection closed.\")\n\nif __name__ == \"__main__\":\n    main()"
  },
  {
    "objectID": "advanced.html#service-layer-architecture",
    "href": "advanced.html#service-layer-architecture",
    "title": "Advanced Usage",
    "section": "Service Layer Architecture",
    "text": "Service Layer Architecture\nNAQ features a comprehensive service layer that provides centralized connection management, dependency injection, and resource lifecycle management. This architecture improves performance, reliability, and maintainability.\n\nCore Service Components\nThe service layer consists of several key components:\n\nServiceManager: Central coordinator for all services with dependency injection\nConnectionService: Manages NATS connections with pooling and failover\nStreamService: Handles JetStream operations for job queues\nKVStoreService: Manages KeyValue store operations for results and metadata\nJobService: Orchestrates job execution and result handling\nEventService: Manages comprehensive event logging and processing\n\n\n\nUsing the Service Layer\nfrom naq.services import create_service_manager_from_config, BaseService\nfrom naq.config import load_config\n\n# Create service manager with typed configuration\nservice_manager = create_service_manager_from_config(\"./config.yaml\")\n\n# Use services in your application\nasync def process_jobs():\n    async with service_manager as services:\n        # Get job service (automatically gets other dependencies)\n        job_service = await services.get_service(JobService)\n        \n        # Process jobs with managed connections and resources\n        result = await job_service.execute_job(my_job)\n        return result\n\n# Run with proper resource management\nawait process_jobs()\n\n\nConnection Management Benefits\nThe centralized connection management provides several advantages:\n\nConnection Pooling: Efficient reuse of NATS connections across services\nAutomatic Failover: Transparent failover to backup NATS servers\nResource Cleanup: Automatic cleanup of connections and streams\nConfiguration Integration: Direct integration with YAML configuration system\nType Safety: Full type safety with IDE support\n\n\n\nCustom Services\nYou can create custom services that integrate with the NAQ service layer:\nfrom naq.services import BaseService\nfrom naq.config.types import NAQConfig\n\nclass CustomAnalyticsService(BaseService):\n    \"\"\"Custom service for analytics processing.\"\"\"\n    \n    def __init__(self, config: NAQConfig):\n        super().__init__(config)\n        self.analytics_enabled = config.get_nested(\"custom.analytics.enabled\", True)\n        \n    async def _do_initialize(self):\n        \"\"\"Initialize analytics connections.\"\"\"\n        # Access NATS config with type safety\n        nats_servers = self.nats_config.servers\n        self.connection = await self._create_analytics_connection(nats_servers)\n        \n    async def _do_cleanup(self):\n        \"\"\"Clean up analytics resources.\"\"\"\n        if hasattr(self, 'connection'):\n            await self.connection.close()\n    \n    async def track_job_metrics(self, job_id: str, metrics: dict):\n        \"\"\"Track job performance metrics.\"\"\"\n        if not self.analytics_enabled:\n            return\n            \n        await self.connection.publish(\n            f\"analytics.job.{job_id}\",\n            json.dumps(metrics).encode()\n        )\n\n# Register and use custom service\nservice_manager.register_service(CustomAnalyticsService)\nanalytics = await service_manager.get_service(CustomAnalyticsService)\nawait analytics.track_job_metrics(\"job-123\", {\"duration_ms\": 1500})"
  },
  {
    "objectID": "advanced.html#configuration-via-yaml-and-environment-variables",
    "href": "advanced.html#configuration-via-yaml-and-environment-variables",
    "title": "Advanced Usage",
    "section": "Configuration via YAML and Environment Variables",
    "text": "Configuration via YAML and Environment Variables\nNAQ supports comprehensive configuration through YAML files with full backward compatibility for environment variables. The new configuration system provides better structure, validation, and type safety while maintaining all existing functionality.\n\nYAML Configuration (Recommended)\nFor new projects, use YAML configuration files for better structure and maintainability:\n# naq.yaml\nnats:\n  servers: [\"nats://localhost:4222\"]\n  client_name: \"my-app\"\n  max_reconnect_attempts: 5\n  \nworkers:\n  concurrency: 10\n  heartbeat_interval: 15\n  pools:\n    high_priority:\n      concurrency: 20\n      queues: [\"urgent\", \"critical\"]\n      \nevents:\n  enabled: true\n  batch_size: 100\n  \nlogging:\n  level: \"INFO\"\n  format: \"json\"\nSee the complete Configuration Guide for detailed YAML configuration options.\n\n\nEnvironment Variables (Legacy Support)\nAll existing environment variables continue to work and will override YAML settings:\n\n\n\n\n\n\n\n\n\nVariable\nYAML Path\nDefault\nDescription\n\n\n\n\nNAQ_NATS_URL\nnats.servers[0]\nnats://localhost:4222\nThe URL of the NATS server.\n\n\nNAQ_DEFAULT_QUEUE\nqueues.default\nnaq_default_queue\nThe default queue name used when none is specified.\n\n\nNAQ_JOB_SERIALIZER\nserialization.job_serializer\npickle\nThe serializer for jobs. Can be pickle or json. See security note below.\n\n\nNAQ_DEFAULT_RESULT_TTL\nresults.ttl\n604800 (7 days)\nDefault time-to-live (in seconds) for job results stored in NATS.\n\n\nNAQ_SCHEDULER_LOCK_TTL\nscheduler.lock_ttl\n30\nTTL (in seconds) for the scheduler‚Äôs high-availability leader lock.\n\n\nNAQ_WORKER_TTL\nworkers.ttl\n60\nTTL (in seconds) for a worker‚Äôs heartbeat. If a worker is silent for this long, it‚Äôs considered dead.\n\n\nNAQ_WORKER_HEARTBEAT_INTERVAL\nworkers.heartbeat_interval\n15\nHow often (in seconds) a worker sends a heartbeat to NATS.\n\n\nNAQ_LOG_LEVEL\nlogging.level\nCRITICAL\nThe logging level for naq components. Can be DEBUG, INFO, WARNING, ERROR.\n\n\nNAQ_EVENTS_ENABLED\nevents.enabled\ntrue\nEnable/disable event logging system globally\n\n\nNAQ_EVENT_LOGGER_BATCH_SIZE\nevents.batch_size\n100\nNumber of events to batch before flushing\n\n\nNAQ_CONCURRENCY\nworkers.concurrency\n10\nDefault worker concurrency level\n\n\n\n\n\nConfiguration Priority\nConfiguration values are loaded with the following priority (highest to lowest):\n\nCommand-line config file (--config ./my-config.yaml)\nLocal config file (./naq.yaml or ./naq.yml)\nUser config file (~/.naq/config.yaml)\nSystem config file (/etc/naq/config.yaml)\nEnvironment variables (NAQ_* variables)\nBuilt-in defaults\n\n\n\nEnvironment Variable Interpolation\nYAML configuration supports environment variable interpolation:\nnats:\n  servers: [\"${NAQ_NATS_URL:nats://localhost:4222}\"]\n  auth:\n    username: \"${NATS_USER}\"\n    password: \"${NATS_PASS}\"\n    \nworkers:\n  concurrency: \"${NAQ_CONCURRENCY:10}\""
  },
  {
    "objectID": "advanced.html#job-serialization-pickle-vs.-json",
    "href": "advanced.html#job-serialization-pickle-vs.-json",
    "title": "Advanced Usage",
    "section": "Job Serialization (pickle vs.¬†json)",
    "text": "Job Serialization (pickle vs.¬†json)\nnaq uses a serializer to convert job data (the function and its arguments) into a format that can be stored in NATS. You can choose between two built-in serializers.\n\npickle (Default)\n\nPros: Can serialize almost any Python object, including complex custom classes, lambdas, and functions defined in a REPL.\nCons: Not secure. A malicious actor who can enqueue jobs could craft a pickle payload that executes arbitrary code on your workers.\n\n\n\njson (Recommended for Production)\n\nPros: Secure. Only serializes basic data types (strings, numbers, lists, dicts). Functions are referenced by their import path (e.g., my_app.tasks.process_data), not serialized directly. This prevents arbitrary code execution.\nCons: Less flexible. Cannot serialize complex Python objects that don‚Äôt have a natural JSON representation.\n\nTo use the json serializer, set the following environment variable:\nexport NAQ_JOB_SERIALIZER=json\n\n\n\n\n\n\nWarning\n\n\n\nSecurity Warning\nIt is strongly recommended to use the json serializer in any environment where the job producer is not fully trusted."
  },
  {
    "objectID": "advanced.html#event-system-configuration",
    "href": "advanced.html#event-system-configuration",
    "title": "Advanced Usage",
    "section": "Event System Configuration",
    "text": "Event System Configuration\nNAQ‚Äôs event-driven state management system is highly configurable for different deployment scenarios and performance requirements.\n\nCore Event Logging Settings\n\n\n\n\n\n\n\n\nEnvironment Variable\nDefault Value\nDescription\n\n\n\n\nNAQ_EVENT_LOGGING_ENABLED\ntrue\nEnable/disable event logging system globally\n\n\nNAQ_EVENT_STREAM_NAME\nNAQ_JOB_EVENTS\nName of the NATS JetStream stream for events\n\n\nNAQ_EVENT_SUBJECT_PREFIX\nnaq.jobs.events\nSubject prefix for event messages\n\n\nNAQ_EVENT_STREAM_MAX_AGE\n168h\nMaximum age for events (7 days)\n\n\nNAQ_EVENT_STREAM_MAX_BYTES\n1GB\nMaximum storage for event stream\n\n\n\n\n\nEvent Logger Performance\n\n\n\n\n\n\n\n\nEnvironment Variable\nDefault Value\nDescription\n\n\n\n\nNAQ_EVENT_LOGGER_BATCH_SIZE\n100\nNumber of events to batch before flushing\n\n\nNAQ_EVENT_LOGGER_FLUSH_INTERVAL\n5.0\nMaximum seconds between flushes\n\n\nNAQ_EVENT_LOGGER_MAX_PENDING\n1000\nMaximum pending events in memory\n\n\n\n\n\nExample Production Configuration\n# Enable event logging with optimized settings\nexport NAQ_EVENT_LOGGING_ENABLED=true\nexport NAQ_EVENT_STREAM_MAX_AGE=720h    # 30 days retention\nexport NAQ_EVENT_STREAM_MAX_BYTES=5GB   # Larger storage limit\n\n# Performance tuning for high-throughput scenarios\nexport NAQ_EVENT_LOGGER_BATCH_SIZE=250  # Larger batches\nexport NAQ_EVENT_LOGGER_FLUSH_INTERVAL=2.0  # More frequent flushes\nexport NAQ_EVENT_LOGGER_MAX_PENDING=2000    # Higher memory buffer\n\n# Reduce worker heartbeat noise\nexport NAQ_WORKER_HEARTBEAT_INTERVAL=30  # Less frequent heartbeats"
  },
  {
    "objectID": "advanced.html#custom-event-processing",
    "href": "advanced.html#custom-event-processing",
    "title": "Advanced Usage",
    "section": "Custom Event Processing",
    "text": "Custom Event Processing\n\nBuilding Custom Event Processors\nCreate specialized event processors for your specific needs:\n# custom_processor.py\nimport asyncio\nfrom naq.events import AsyncJobEventProcessor, JobEventType\nfrom datetime import datetime, timedelta\n\nclass PerformanceAnalyzer:\n    \"\"\"Custom processor for performance analysis.\"\"\"\n    \n    def __init__(self, alert_threshold_ms=10000):\n        self.alert_threshold = alert_threshold_ms\n        self.slow_jobs = []\n        self.queue_stats = {}\n    \n    async def start_monitoring(self):\n        \"\"\"Start the performance monitoring system.\"\"\"\n        processor = AsyncJobEventProcessor()\n        \n        # Register handlers\n        processor.add_handler(JobEventType.COMPLETED, self._analyze_completion)\n        processor.add_handler(JobEventType.FAILED, self._analyze_failure)\n        \n        await processor.start()\n        \n        try:\n            # Process events continuously\n            async for event in processor.stream_job_events():\n                pass\n        finally:\n            await processor.stop()\n    \n    async def _analyze_completion(self, event):\n        \"\"\"Analyze completed job performance.\"\"\"\n        if event.duration_ms and event.duration_ms &gt; self.alert_threshold:\n            self.slow_jobs.append({\n                'job_id': event.job_id,\n                'duration': event.duration_ms,\n                'queue': event.queue_name,\n                'worker': event.worker_id,\n                'timestamp': event.timestamp\n            })\n            \n            # Send alert\n            await self._send_performance_alert(event)\n        \n        # Update queue statistics\n        queue = event.queue_name or 'default'\n        if queue not in self.queue_stats:\n            self.queue_stats[queue] = {\n                'completed': 0,\n                'total_duration': 0,\n                'avg_duration': 0\n            }\n        \n        stats = self.queue_stats[queue]\n        stats['completed'] += 1\n        stats['total_duration'] += (event.duration_ms or 0)\n        stats['avg_duration'] = stats['total_duration'] / stats['completed']\n    \n    async def _analyze_failure(self, event):\n        \"\"\"Analyze job failures.\"\"\"\n        print(f\"üö® Job failure analysis: {event.job_id}\")\n        print(f\"   Error: {event.error_message}\")\n        print(f\"   Queue: {event.queue_name}\")\n        print(f\"   Worker: {event.worker_id}\")\n    \n    async def _send_performance_alert(self, event):\n        \"\"\"Send performance alert.\"\"\"\n        print(f\"‚ö†Ô∏è  SLOW JOB ALERT: {event.job_id}\")\n        print(f\"   Duration: {event.duration_ms}ms (threshold: {self.alert_threshold}ms)\")\n        print(f\"   Queue: {event.queue_name}\")\n        \n        # In production: send to Slack, PagerDuty, etc.\n    \n    def get_performance_report(self):\n        \"\"\"Generate performance report.\"\"\"\n        return {\n            'slow_jobs_count': len(self.slow_jobs),\n            'queue_performance': self.queue_stats,\n            'recent_slow_jobs': self.slow_jobs[-10:]\n        }\n\n# Usage\nanalyzer = PerformanceAnalyzer(alert_threshold_ms=5000)\nasyncio.run(analyzer.start_monitoring())\n\n\nIntegration with Monitoring Systems\n\nPrometheus Metrics\n# prometheus_integration.py\nimport time\nfrom prometheus_client import Counter, Histogram, Gauge, start_http_server\nfrom naq.events import AsyncJobEventProcessor, JobEventType\n\n# Define metrics\njob_total = Counter('naq_jobs_total', 'Total jobs processed', ['queue', 'status'])\njob_duration = Histogram('naq_job_duration_seconds', 'Job duration', ['queue'])\nactive_workers = Gauge('naq_active_workers', 'Number of active workers')\n\nclass PrometheusExporter:\n    \"\"\"Export NAQ metrics to Prometheus.\"\"\"\n    \n    def __init__(self, port=8000):\n        self.port = port\n        self.active_worker_count = 0\n    \n    async def start_exporter(self):\n        \"\"\"Start the Prometheus metrics exporter.\"\"\"\n        # Start HTTP server for metrics\n        start_http_server(self.port)\n        print(f\"üìä Prometheus metrics available at http://localhost:{self.port}/metrics\")\n        \n        # Start event processing\n        processor = AsyncJobEventProcessor()\n        \n        processor.add_handler(JobEventType.COMPLETED, self._handle_completed)\n        processor.add_handler(JobEventType.FAILED, self._handle_failed)\n        processor.add_handler(JobEventType.ENQUEUED, self._handle_enqueued)\n        processor.add_global_handler(self._handle_worker_events)\n        \n        await processor.start()\n        \n        try:\n            async for event in processor.stream_job_events():\n                pass\n        finally:\n            await processor.stop()\n    \n    async def _handle_completed(self, event):\n        \"\"\"Handle completed job metrics.\"\"\"\n        queue = event.queue_name or 'default'\n        job_total.labels(queue=queue, status='completed').inc()\n        \n        if event.duration_ms:\n            duration_seconds = event.duration_ms / 1000.0\n            job_duration.labels(queue=queue).observe(duration_seconds)\n    \n    async def _handle_failed(self, event):\n        \"\"\"Handle failed job metrics.\"\"\"\n        queue = event.queue_name or 'default'\n        job_total.labels(queue=queue, status='failed').inc()\n    \n    async def _handle_enqueued(self, event):\n        \"\"\"Handle enqueued job metrics.\"\"\"\n        queue = event.queue_name or 'default'\n        job_total.labels(queue=queue, status='enqueued').inc()\n    \n    async def _handle_worker_events(self, event):\n        \"\"\"Handle worker events for active count.\"\"\"\n        if hasattr(event, 'details') and event.details:\n            if event.details.get('event_category') == 'worker':\n                if 'worker_started' in event.event_type.value:\n                    self.active_worker_count += 1\n                elif 'worker_stopped' in event.event_type.value:\n                    self.active_worker_count -= 1\n                \n                active_workers.set(self.active_worker_count)\n\n# Usage\nexporter = PrometheusExporter(port=8000)\nasyncio.run(exporter.start_exporter())"
  },
  {
    "objectID": "advanced.html#performance-optimization",
    "href": "advanced.html#performance-optimization",
    "title": "Advanced Usage",
    "section": "Performance Optimization",
    "text": "Performance Optimization\n\nHigh-Volume Event Logging\nFor high-throughput systems, optimize event logging performance:\n# high_performance_config.py\nimport os\n\n# Optimize for high volume\nos.environ.update({\n    'NAQ_EVENT_LOGGER_BATCH_SIZE': '500',        # Larger batches\n    'NAQ_EVENT_LOGGER_FLUSH_INTERVAL': '1.0',    # Faster flushes\n    'NAQ_EVENT_LOGGER_MAX_PENDING': '5000',      # Higher memory buffer\n    \n    # Reduce event noise\n    'NAQ_WORKER_HEARTBEAT_INTERVAL': '60',       # Less frequent heartbeats\n    \n    # Optimize NATS JetStream\n    'NAQ_EVENT_STREAM_MAX_BYTES': '10GB',        # Larger stream\n    'NAQ_EVENT_STREAM_MAX_AGE': '48h',           # Shorter retention\n})\n\n\nMemory Management\nMonitor and control memory usage:\n# memory_monitoring.py\nimport asyncio\nimport psutil\nfrom naq.events import AsyncJobEventLogger\n\nclass MemoryAwareLogger:\n    \"\"\"Event logger with memory monitoring.\"\"\"\n    \n    def __init__(self, memory_threshold_mb=1000):\n        self.threshold = memory_threshold_mb\n        self.logger = AsyncJobEventLogger()\n        self._monitoring = True\n    \n    async def start_with_monitoring(self):\n        \"\"\"Start logger with memory monitoring.\"\"\"\n        await self.logger.start()\n        \n        # Start memory monitoring task\n        monitor_task = asyncio.create_task(self._monitor_memory())\n        \n        try:\n            # Your application logic here\n            pass\n        finally:\n            self._monitoring = False\n            monitor_task.cancel()\n            await self.logger.stop()\n    \n    async def _monitor_memory(self):\n        \"\"\"Monitor memory usage and adjust batch size.\"\"\"\n        while self._monitoring:\n            process = psutil.Process()\n            memory_mb = process.memory_info().rss / 1024 / 1024\n            \n            if memory_mb &gt; self.threshold:\n                print(f\"‚ö†Ô∏è  High memory usage: {memory_mb:.1f}MB\")\n                # Force flush to reduce memory\n                await self.logger.flush()\n            \n            await asyncio.sleep(30)  # Check every 30 seconds\n\n# Usage\nmemory_logger = MemoryAwareLogger(memory_threshold_mb=800)\nasyncio.run(memory_logger.start_with_monitoring())"
  },
  {
    "objectID": "advanced.html#troubleshooting",
    "href": "advanced.html#troubleshooting",
    "title": "Advanced Usage",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nCommon Event System Issues\nEvents not appearing in stream:\n\nCheck if event logging is enabled:\necho $NAQ_EVENT_LOGGING_ENABLED\nVerify NATS connectivity:\nnats stream info NAQ_JOB_EVENTS\nCheck event logger lifecycle:\nlogger = AsyncJobEventLogger()\nawait logger.start()  # Must call start()\n# ... log events\nawait logger.stop()   # Must call stop()\n\nHigh memory usage:\n\nReduce batch size:\nexport NAQ_EVENT_LOGGER_BATCH_SIZE=50\nIncrease flush frequency:\nexport NAQ_EVENT_LOGGER_FLUSH_INTERVAL=2.0\nMonitor pending events:\n# In your application\nif logger._pending_events_count &gt; 500:\n    await logger.flush()\n\nSlow event processing:\n\nIncrease batch size for better throughput:\nexport NAQ_EVENT_LOGGER_BATCH_SIZE=300\nUse dedicated NATS cluster for events\nConsider event sampling for very high-volume scenarios\n\n\n\nDebug Commands\n# Check event stream status\nnats stream info NAQ_JOB_EVENTS\n\n# Monitor event rate\nnaq events --format raw | pv -l\n\n# Test event logging\npython -c \"\nimport asyncio\nfrom naq.events import AsyncJobEventLogger\n\nasync def test():\n    logger = AsyncJobEventLogger()\n    await logger.start()\n    await logger.log_job_enqueued('test-job', 'test-queue')\n    await logger.flush()\n    await logger.stop()\n    print('Event logged successfully')\n\nasyncio.run(test())\n\""
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NAQ - NATS Asynchronous Queue",
    "section": "",
    "text": "NAQ is a simple, asynchronous job queueing library for Python, inspired by RQ (Redis Queue), but built entirely on top of NATS and its JetStream persistence layer.\nIt allows you to easily enqueue Python functions to be executed asynchronously by worker processes, leveraging the power and resilience of NATS JetStream for message persistence and delivery."
  },
  {
    "objectID": "index.html#get-started",
    "href": "index.html#get-started",
    "title": "NAQ - NATS Asynchronous Queue",
    "section": "Get Started",
    "text": "Get Started\nReady to dive in? Check out the Quickstart Guide to get your first worker up and running in minutes.\n\nNew in Version 0.2.0\nNAQ 0.2.0 introduces a new modular structure with complete backward compatibility: - All existing imports continue to work unchanged - New modular imports available for better organization - Enhanced service layer for advanced use cases - See the Import Migration Guide for details"
  },
  {
    "objectID": "index.html#what-makes-naq-different",
    "href": "index.html#what-makes-naq-different",
    "title": "NAQ - NATS Asynchronous Queue",
    "section": "What Makes NAQ Different",
    "text": "What Makes NAQ Different\nNAQ isn‚Äôt just another task queue - it‚Äôs a complete event-driven job processing platform built on NATS. Every state change, from job enqueue to worker heartbeats, is captured as a structured event and stored in durable streams.\nSee your jobs in action:\n# Watch jobs being processed in real-time  \nnaq events\n\n# Monitor worker status and health\nnaq worker-events\n\n# Analyze job performance and history\nnaq event-history job-abc-123\nThis event-driven approach gives you unprecedented visibility into your job processing pipeline, making debugging, monitoring, and scaling dramatically easier."
  },
  {
    "objectID": "index.html#key-features",
    "href": "index.html#key-features",
    "title": "NAQ - NATS Asynchronous Queue",
    "section": "Key Features",
    "text": "Key Features\n\nSimple API: Familiar and easy-to-use API, similar to RQ.\nAsynchronous Core: Built with asyncio and nats-py for high performance.\nPersistent & Reliable: Uses NATS JetStream for robust job persistence and guaranteed delivery.\nEvent-Driven State Management: Complete observability with real-time job, worker, and schedule event streaming.\nComprehensive Monitoring: Live monitoring of job lifecycle, worker status, and system health with rich CLI tools.\nScheduled & Recurring Jobs: Supports cron-style, interval-based, and one-time scheduled tasks with full event tracking.\nJob Dependencies: Create complex workflows by defining dependencies between jobs.\nAutomatic Retries: Configurable retry mechanism with exponential backoff for failed jobs.\nWorker Status Tracking: Real-time worker heartbeats, status changes, and performance monitoring.\nSchedule Management: Full lifecycle management of scheduled jobs with pause, resume, modify, and cancel operations.\nHistorical Analysis: Query job event history for debugging, analytics, and compliance.\nProduction-Ready Monitoring: Built-in alerting, metrics collection, and integration with monitoring systems.\nPowerful CLI: Comprehensive command-line interface with real-time monitoring, event history, and system analytics."
  },
  {
    "objectID": "testing.html",
    "href": "testing.html",
    "title": "Testing Guide",
    "section": "",
    "text": "NAQ uses a comprehensive testing strategy that validates both the new service layer architecture and maintains backward compatibility. This guide covers testing patterns, best practices, and how to work with the enhanced test infrastructure."
  },
  {
    "objectID": "testing.html#overview",
    "href": "testing.html#overview",
    "title": "Testing Guide",
    "section": "",
    "text": "NAQ uses a comprehensive testing strategy that validates both the new service layer architecture and maintains backward compatibility. This guide covers testing patterns, best practices, and how to work with the enhanced test infrastructure."
  },
  {
    "objectID": "testing.html#test-structure",
    "href": "testing.html#test-structure",
    "title": "Testing Guide",
    "section": "Test Structure",
    "text": "Test Structure\n\nTest Organization\ntests/\n‚îú‚îÄ‚îÄ conftest.py                    # Test configuration and fixtures\n‚îú‚îÄ‚îÄ test_models/                   # Model unit tests (planned)\n‚îú‚îÄ‚îÄ test_services/                 # Service layer tests (new)\n‚îÇ   ‚îú‚îÄ‚îÄ test_service_manager.py    # ServiceManager lifecycle\n‚îÇ   ‚îú‚îÄ‚îÄ test_connection_service.py # Connection management\n‚îÇ   ‚îî‚îÄ‚îÄ test_job_service.py        # Job execution service\n‚îú‚îÄ‚îÄ test_config/                   # Configuration tests (new)\n‚îÇ   ‚îú‚îÄ‚îÄ test_yaml_loading.py       # YAML config loading\n‚îÇ   ‚îî‚îÄ‚îÄ test_validation.py         # Config validation\n‚îú‚îÄ‚îÄ test_compatibility/            # Backward compatibility (new)\n‚îÇ   ‚îú‚îÄ‚îÄ test_imports.py            # Import compatibility\n‚îÇ   ‚îî‚îÄ‚îÄ test_user_workflows.py     # User workflow compatibility\n‚îú‚îÄ‚îÄ test_performance/              # Performance tests (new)\n‚îÇ   ‚îî‚îÄ‚îÄ test_service_overhead.py   # Service layer overhead\n‚îú‚îÄ‚îÄ unit/                          # Legacy unit tests (updated)\n‚îú‚îÄ‚îÄ integration/                   # Integration tests\n‚îî‚îÄ‚îÄ scenario/                      # Scenario tests"
  },
  {
    "objectID": "testing.html#testing-patterns",
    "href": "testing.html#testing-patterns",
    "title": "Testing Guide",
    "section": "Testing Patterns",
    "text": "Testing Patterns\n\nService Layer Testing\n\nServiceManager Testing\n@pytest.mark.asyncio\nasync def test_service_initialization(service_test_config):\n    \"\"\"Test ServiceManager initializes all services correctly.\"\"\"\n    manager = ServiceManager(service_test_config)\n    await manager.initialize_all()\n    \n    try:\n        # Test service retrieval\n        conn_service = await manager.get_service(ConnectionService)\n        job_service = await manager.get_service(JobService)\n        \n        assert conn_service is not None\n        assert job_service is not None\n    finally:\n        await manager.cleanup_all()\n\n\nService Mocking\nasync def test_with_mock_services(mock_service_manager):\n    \"\"\"Test using mock service manager.\"\"\"\n    mock_manager, service_map = mock_service_manager\n    \n    job_service = service_map[JobService]\n    job_service.enqueue_job.return_value = \"job-123\"\n    \n    result = await job_service.enqueue_job(job, \"test-queue\")\n    assert result == \"job-123\"\n\n\n\nConfiguration Testing\n\nYAML Configuration\ndef test_yaml_config_loading(temp_config_file):\n    \"\"\"Test loading configuration from YAML file.\"\"\"\n    config = load_config(temp_config_file)\n    \n    assert 'nats' in config.to_dict()\n    assert config.to_dict()['nats']['url'] == 'nats://localhost:4222'\n\n\nEnvironment Variable Overrides\ndef test_environment_overrides(temp_config_file, env_override_config):\n    \"\"\"Test environment variables override YAML config.\"\"\"\n    config = load_config(temp_config_file)\n    config_dict = config.to_dict()\n    \n    # Values should be overridden by environment variables\n    assert config_dict['workers']['concurrency'] == 20  # From NAQ_WORKERS_CONCURRENCY\n\n\n\nCompatibility Testing\n\nImport Compatibility\ndef test_main_imports():\n    \"\"\"Test main package imports still work.\"\"\"\n    from naq import Queue, Worker, Job, enqueue, enqueue_sync\n    \n    assert Queue is not None\n    assert callable(enqueue)\n\n\nWorkflow Compatibility\ndef test_basic_sync_workflow():\n    \"\"\"Test basic synchronous workflow still works.\"\"\"\n    from naq import enqueue_sync, JOB_STATUS\n    \n    def simple_task(x):\n        return x + 1\n    \n    # Should work exactly as before refactoring\n    job = enqueue_sync(simple_task, 5)\n    assert job.job_id is not None"
  },
  {
    "objectID": "testing.html#test-fixtures",
    "href": "testing.html#test-fixtures",
    "title": "Testing Guide",
    "section": "Test Fixtures",
    "text": "Test Fixtures\n\nCore Fixtures\n\nservice_test_config\nProvides a complete test configuration for ServiceManager:\n{\n    'nats': {\n        'url': 'nats://localhost:4222',\n        'client_name': 'naq-test'\n    },\n    'workers': {\n        'concurrency': 2,\n        'heartbeat_interval': 5\n    },\n    'events': {\n        'enabled': True,\n        'batch_size': 10\n    }\n}\n\n\nservice_manager\nReal ServiceManager instance for integration tests:\nasync def test_with_real_services(service_manager):\n    conn_service = await service_manager.get_service(ConnectionService)\n    # Use real service for integration testing\n\n\nmock_service_manager\nMock ServiceManager for unit tests:\ndef test_with_mock_services(mock_service_manager):\n    mock_manager, service_map = mock_service_manager\n    job_service = service_map[JobService]\n    # Configure and use mocked services\n\n\n\nConfiguration Fixtures\n\ntemp_config_file\nCreates temporary YAML configuration file:\ndef test_config_loading(temp_config_file):\n    config = load_config(temp_config_file)\n    # Test with real config file\n\n\nenv_override_config\nSets up environment variable overrides:\ndef test_env_overrides(temp_config_file, env_override_config):\n    # Environment variables are automatically set\n    config = load_config(temp_config_file)\n    # Test environment variable behavior\n\n\n\nCompatibility Fixtures\n\nservice_compatible_queue\nQueue instance using ServiceManager architecture:\nasync def test_new_queue_pattern(service_compatible_queue):\n    queue = service_compatible_queue\n    # Test with service-aware queue"
  },
  {
    "objectID": "testing.html#best-practices",
    "href": "testing.html#best-practices",
    "title": "Testing Guide",
    "section": "Best Practices",
    "text": "Best Practices\n\nTest Isolation\n\nUse proper fixtures to ensure test isolation\nMock external dependencies like NATS connections\nClean up resources after tests complete\n\n\n\nService Testing\n\nTest service interfaces not implementation details\nUse dependency injection for testing\nVerify service lifecycle (initialize/cleanup)\n\n\n\nConfiguration Testing\n\nTest all configuration sources (YAML, env vars, defaults)\nValidate configuration errors are handled properly\nTest configuration-driven behavior\n\n\n\nPerformance Testing\n\nMeasure service overhead to prevent regressions\nTest concurrent access patterns\nMonitor memory usage for leaks"
  },
  {
    "objectID": "testing.html#running-tests",
    "href": "testing.html#running-tests",
    "title": "Testing Guide",
    "section": "Running Tests",
    "text": "Running Tests\n\nBasic Test Execution\n# Run all tests\npytest\n\n# Run specific test categories\npytest tests/test_services/\npytest tests/test_config/\npytest tests/test_compatibility/\n\n# Run with coverage\npytest --cov=naq tests/\n\n\nPerformance Tests\n# Run performance tests\npytest tests/test_performance/ -v\n\n# Run with timing information\npytest tests/test_performance/ --durations=10\n\n\nIntegration Tests\n# Start NATS server first\ndocker compose up -d nats\n\n# Run integration tests\npytest tests/integration/ -v"
  },
  {
    "objectID": "testing.html#test-development-guidelines",
    "href": "testing.html#test-development-guidelines",
    "title": "Testing Guide",
    "section": "Test Development Guidelines",
    "text": "Test Development Guidelines\n\nWriting Service Tests\n\nTest public interfaces of services\nMock dependencies appropriately\nTest error conditions and edge cases\nVerify service interactions\n\n\n\nWriting Configuration Tests\n\nTest configuration loading from all sources\nValidate error handling for invalid configs\nTest environment variable overrides\nVerify service integration with config\n\n\n\nWriting Compatibility Tests\n\nTest all public APIs continue to work\nVerify import patterns are maintained\nTest typical user workflows\nCheck error behavior compatibility"
  },
  {
    "objectID": "testing.html#migration-from-legacy-tests",
    "href": "testing.html#migration-from-legacy-tests",
    "title": "Testing Guide",
    "section": "Migration from Legacy Tests",
    "text": "Migration from Legacy Tests\n\nUpdating Existing Tests\nWhen updating legacy tests to work with the service layer:\n\nAdd service fixtures alongside existing ones\nCreate service-compatible versions of test cases\nMaintain backward compatibility during transition\nDocument migration patterns for future reference\n\n\n\nExample Migration\n# Legacy pattern\n@pytest_asyncio.fixture\nasync def queue(mock_nats, mocker):\n    mock_nc, mock_js = mock_nats\n    mocker.patch('naq.queue.get_nats_connection', return_value=mock_nc)\n    q = Queue(name=\"test\")\n    return q\n\n# Service-compatible pattern\n@pytest_asyncio.fixture\nasync def service_queue(service_compatible_queue):\n    return service_compatible_queue\n\n# Test using both patterns\nasync def test_queue_functionality(queue):\n    # Legacy test pattern\n    pass\n\nasync def test_service_queue_functionality(service_queue):\n    # New service pattern\n    pass"
  },
  {
    "objectID": "testing.html#troubleshooting",
    "href": "testing.html#troubleshooting",
    "title": "Testing Guide",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nCommon Issues\n\nService initialization errors\n\nCheck configuration validity\nVerify mock setup for dependencies\nEnsure proper cleanup in tests\n\nImport errors in tests\n\nVerify service layer imports are available\nCheck circular import issues\nUpdate fixture dependencies\n\nPerformance test failures\n\nCheck mock configuration\nVerify reasonable timeouts\nConsider test environment factors\n\n\n\n\nDebug Strategies\n\nUse verbose pytest output (-v flag)\nAdd debug logging to tests\nCheck fixture setup and teardown\nVerify mock configurations are correct"
  },
  {
    "objectID": "testing.html#future-considerations",
    "href": "testing.html#future-considerations",
    "title": "Testing Guide",
    "section": "Future Considerations",
    "text": "Future Considerations\nAs the service layer evolves, consider:\n\nAdding more service-specific tests\nExpanding performance test coverage\nCreating integration test scenarios\nDeveloping testing utilities for common patterns\n\nThe testing infrastructure is designed to be extensible and maintainable as NAQ continues to grow and evolve."
  },
  {
    "objectID": "examples.html",
    "href": "examples.html",
    "title": "Usage Examples",
    "section": "",
    "text": "This page provides practical examples for some of naq‚Äôs key features."
  },
  {
    "objectID": "examples.html#example-1-scheduled-and-recurring-jobs",
    "href": "examples.html#example-1-scheduled-and-recurring-jobs",
    "title": "Usage Examples",
    "section": "Example 1: Scheduled and Recurring Jobs",
    "text": "Example 1: Scheduled and Recurring Jobs\nnaq allows you to schedule jobs to run at a specific time in the future or on a recurring basis using cron expressions.\nTo run these examples, you need both a worker and the scheduler process running:\n# Terminal 1: Start the scheduler\nnaq scheduler\n\n# Terminal 2: Start a worker\nnaq worker scheduled_queue\n\nOne-Time Scheduled Job\nYou can enqueue a job to run after a specific delay or at a precise time.\n# schedule_task.py\nimport asyncio\nfrom datetime import datetime, timedelta\nfrom naq import enqueue_at, enqueue_in\n\nasync def send_reminder(user_id, message):\n    print(f\"Sending reminder to {user_id}: {message}\")\n    # Add logic to send an email or push notification\n    return f\"Reminder sent to {user_id}\"\n\nasync def main():\n    # Schedule a job to run in 5 minutes\n    run_in_5_min = await enqueue_in(\n        send_reminder,\n        delay=timedelta(minutes=5),\n        user_id=\"user123\",\n        message=\"Your meeting starts in 5 minutes.\",\n        queue_name=\"scheduled_queue\"\n    )\n    print(f\"Job {run_in_5_min.job_id} scheduled to run in 5 minutes.\")\n\n    # Schedule a job to run at a specific time (UTC)\n    run_at_time = datetime.utcnow() + timedelta(hours=1)\n    run_at = await enqueue_at(\n        send_reminder,\n        run_at=run_at_time,\n        user_id=\"user456\",\n        message=\"Don't forget your 1-hour follow-up.\",\n        queue_name=\"scheduled_queue\"\n    )\n    print(f\"Job {run_at.job_id} scheduled to run at {run_at_time.isoformat()}.\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\n\nRecurring Job (Cron)\nFor tasks that need to run on a regular schedule (e.g., nightly reports, weekly cleanups), you can use the schedule function with a cron string.\n# recurring_task.py\nimport asyncio\nfrom naq import schedule\n\nasync def generate_nightly_report():\n    print(\"Generating the nightly sales report...\")\n    # Logic to aggregate data and create a report\n    print(\"Nightly report complete.\")\n    return \"Report generated successfully.\"\n\nasync def main():\n    # Schedule the report to run every day at 2:00 AM UTC\n    cron_schedule = await schedule(\n        generate_nightly_report,\n        cron=\"0 2 * * *\",  # Standard cron format\n        schedule_id=\"nightly-sales-report\",\n        queue_name=\"scheduled_queue\"\n    )\n    print(f\"Cron job '{cron_schedule.schedule_id}' is now active.\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
  },
  {
    "objectID": "examples.html#example-2-automatic-job-retries",
    "href": "examples.html#example-2-automatic-job-retries",
    "title": "Usage Examples",
    "section": "Example 2: Automatic Job Retries",
    "text": "Example 2: Automatic Job Retries\nnaq can automatically retry failed jobs with configurable strategies. This is useful for tasks that might fail due to transient issues, like network hiccups.\n# retry_task.py\nimport asyncio\nimport random\nfrom naq import enqueue\n\nasync def flaky_api_call(request_id):\n    \"\"\"\n    This function simulates an API call that sometimes fails.\n    \"\"\"\n    print(f\"Attempting to call API for request {request_id}...\")\n    if random.random() &gt; 0.5:\n        print(\"API call successful!\")\n        return \"Success\"\n    else:\n        print(\"API call failed. Will retry...\")\n        raise ConnectionError(\"Could not connect to the API\")\n\nasync def main():\n    # Enqueue the job with a retry policy\n    job = await enqueue(\n        flaky_api_call,\n        request_id=\"abc-123\",\n        queue_name=\"default\",\n        max_retries=3,          # Attempt the job up to 3 more times\n        retry_delay=5,          # Wait 5 seconds between retries\n        retry_strategy=\"linear\" # Use a fixed delay\n    )\n    print(f\"Enqueued job {job.job_id} with 3 linear retries.\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\n\n\n\n\n\nNote\n\n\n\nRetry Strategies\nnaq supports both linear (fixed delay) and exponential backoff strategies for retries. You can also provide a list of integers to retry_delay for a custom delay sequence."
  },
  {
    "objectID": "examples.html#example-3-job-dependencies",
    "href": "examples.html#example-3-job-dependencies",
    "title": "Usage Examples",
    "section": "Example 3: Job Dependencies",
    "text": "Example 3: Job Dependencies\nYou can create workflows by making jobs dependent on the successful completion of others. The dependent job will only run after its dependencies have finished.\n# dependency_workflow.py\nimport asyncio\nfrom naq import enqueue\n\nasync def download_data(source_url):\n    print(f\"Downloading data from {source_url}...\")\n    await asyncio.sleep(2)  # Simulate download\n    file_path = f\"/tmp/{source_url.split('/')[-1]}.csv\"\n    print(f\"Data downloaded to {file_path}\")\n    return file_path\n\nasync def process_data(file_path):\n    print(f\"Processing data from {file_path}...\")\n    await asyncio.sleep(3)  # Simulate processing\n    result_path = f\"{file_path}.processed\"\n    print(f\"Data processed and saved to {result_path}\")\n    return result_path\n\nasync def upload_results(result_path):\n    print(f\"Uploading {result_path} to cloud storage...\")\n    await asyncio.sleep(1)  # Simulate upload\n    print(\"Upload complete.\")\n    return \"Workflow finished successfully.\"\n\nasync def main():\n    # Step 1: Download data\n    download_job = await enqueue(download_data, source_url=\"http://example.com/data\")\n\n    # Step 2: Process data (depends on download)\n    process_job = await enqueue(\n        process_data,\n        file_path=download_job, # Pass the result of the dependency\n        depends_on=[download_job]\n    )\n\n    # Step 3: Upload results (depends on processing)\n    upload_job = await enqueue(\n        upload_results,\n        result_path=process_job,\n        depends_on=[process_job]\n    )\n\n    print(f\"Workflow started. Final job: {upload_job.job_id}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
  },
  {
    "objectID": "examples.html#example-4-using-multiple-queues",
    "href": "examples.html#example-4-using-multiple-queues",
    "title": "Usage Examples",
    "section": "Example 4: Using Multiple Queues",
    "text": "Example 4: Using Multiple Queues\nYou can use different queues to prioritize jobs or to dedicate workers to specific types of tasks.\nStart workers for each queue in separate terminals:\n# Terminal 1: High-priority worker\nnaq worker notifications --log-level info\n\n# Terminal 2: Low-priority worker\nnaq worker data_processing --log-level info\nNow, you can enqueue jobs to the appropriate queues.\n# multi_queue_example.py\nimport asyncio\nfrom naq import enqueue\n\nasync def send_email(address, subject, body):\n    print(f\"Sending high-priority email to {address}...\")\n    await asyncio.sleep(0.5)\n    return \"Email sent.\"\n\nasync def transcode_video(video_id):\n    print(f\"Starting low-priority video transcoding for {video_id}...\")\n    await asyncio.sleep(10) # Simulate long-running task\n    return \"Video transcoded.\"\n\nasync def main():\n    # Enqueue a high-priority job\n    email_job = await enqueue(\n        send_email,\n        address=\"user@example.com\",\n        subject=\"Your order\",\n        body=\"...\",\n        queue_name=\"notifications\" # Target the 'notifications' queue\n    )\n    print(f\"Enqueued notification job {email_job.job_id}\")\n\n    # Enqueue a low-priority job\n    video_job = await enqueue(\n        transcode_video,\n        video_id=12345,\n        queue_name=\"data_processing\" # Target the 'data_processing' queue\n    )\n    print(f\"Enqueued data processing job {video_job.job_id}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
  },
  {
    "objectID": "examples.html#example-3-event-monitoring-and-analytics",
    "href": "examples.html#example-3-event-monitoring-and-analytics",
    "title": "Usage Examples",
    "section": "Example 3: Event Monitoring and Analytics",
    "text": "Example 3: Event Monitoring and Analytics\nNAQ‚Äôs event-driven architecture provides comprehensive monitoring and analytics capabilities. This example shows how to leverage events for system observability and custom business logic.\n\nReal-time Event Monitoring\nMonitor job events programmatically for custom processing:\n# event_monitor.py\nimport asyncio\nfrom naq.events import AsyncJobEventProcessor, JobEventType\n\nasync def monitor_jobs():\n    \"\"\"Monitor job events and implement custom logic.\"\"\"\n    processor = AsyncJobEventProcessor()\n    \n    # Global handler for all events\n    def log_all_events(event):\n        print(f\"[{event.timestamp}] {event.event_type}: {event.job_id}\")\n    \n    # Specific handler for failed jobs\n    def handle_failures(event):\n        print(f\"‚ùå Job {event.job_id} failed: {event.error_message}\")\n        # Send alert, create ticket, etc.\n        \n    # Performance monitoring handler  \n    def track_performance(event):\n        if event.duration_ms:\n            if event.duration_ms &gt; 10000:  # &gt; 10 seconds\n                print(f\"‚ö†Ô∏è  Slow job detected: {event.job_id} took {event.duration_ms}ms\")\n    \n    # Register event handlers\n    processor.add_global_handler(log_all_events)\n    processor.add_handler(JobEventType.FAILED, handle_failures)\n    processor.add_handler(JobEventType.COMPLETED, track_performance)\n    \n    # Start monitoring\n    await processor.start()\n    \n    try:\n        print(\"üîç Monitoring events... (Press Ctrl+C to stop)\")\n        # Stream events with filtering\n        async for event in processor.stream_job_events():\n            # Events are processed by registered handlers\n            pass\n    except KeyboardInterrupt:\n        print(\"\\nüëã Stopping event monitor\")\n    finally:\n        await processor.stop()\n\nif __name__ == \"__main__\":\n    asyncio.run(monitor_jobs())\n\n\nWorker Status Monitoring\nMonitor worker health and performance:\n# worker_monitor.py\nimport asyncio\nfrom naq.events import AsyncJobEventProcessor\nfrom collections import defaultdict\nfrom datetime import datetime\n\nclass WorkerMonitor:\n    def __init__(self):\n        self.worker_stats = defaultdict(lambda: {\n            'jobs_processed': 0,\n            'last_heartbeat': None,\n            'status': 'unknown',\n            'errors': 0\n        })\n    \n    def handle_worker_event(self, event):\n        \"\"\"Handle worker-specific events.\"\"\"\n        if not (hasattr(event, 'details') and event.details and \n               event.details.get('event_category') == 'worker'):\n            return\n            \n        worker_id = event.worker_id\n        stats = self.worker_stats[worker_id]\n        \n        if 'worker_started' in event.event_type.value:\n            stats['status'] = 'active'\n            print(f\"üü¢ Worker {worker_id} started\")\n            \n        elif 'worker_stopped' in event.event_type.value:\n            stats['status'] = 'stopped'\n            print(f\"üî¥ Worker {worker_id} stopped\")\n            \n        elif 'worker_heartbeat' in event.event_type.value:\n            stats['last_heartbeat'] = datetime.now()\n            if event.details.get('active_jobs') is not None:\n                active = event.details['active_jobs']\n                limit = event.details.get('concurrency_limit', 0)\n                print(f\"üíì {worker_id}: {active}/{limit} jobs active\")\n                \n        elif 'worker_error' in event.event_type.value:\n            stats['errors'] += 1\n            print(f\"‚ö†Ô∏è  Worker {worker_id} error: {event.error_message}\")\n    \n    def handle_job_event(self, event):\n        \"\"\"Handle job events for worker tracking.\"\"\"\n        if event.event_type == JobEventType.COMPLETED and event.worker_id:\n            self.worker_stats[event.worker_id]['jobs_processed'] += 1\n    \n    def print_stats(self):\n        \"\"\"Print current worker statistics.\"\"\"\n        print(\"\\nüìä Worker Statistics:\")\n        print(\"-\" * 60)\n        for worker_id, stats in self.worker_stats.items():\n            print(f\"Worker: {worker_id}\")\n            print(f\"  Status: {stats['status']}\")\n            print(f\"  Jobs Processed: {stats['jobs_processed']}\")\n            print(f\"  Errors: {stats['errors']}\")\n            if stats['last_heartbeat']:\n                print(f\"  Last Heartbeat: {stats['last_heartbeat']}\")\n            print()\n\nasync def monitor_workers():\n    \"\"\"Monitor worker status and health.\"\"\"\n    monitor = WorkerMonitor()\n    processor = AsyncJobEventProcessor()\n    \n    # Register handlers\n    processor.add_global_handler(monitor.handle_worker_event)\n    processor.add_global_handler(monitor.handle_job_event)\n    \n    await processor.start()\n    \n    try:\n        print(\"üë∑ Monitoring workers... (Press Ctrl+C to stop)\")\n        \n        # Print stats every 30 seconds\n        import time\n        last_stats = time.time()\n        \n        async for event in processor.stream_job_events():\n            # Check if it's time to print stats\n            if time.time() - last_stats &gt; 30:\n                monitor.print_stats()\n                last_stats = time.time()\n                \n    except KeyboardInterrupt:\n        print(\"\\nüëã Stopping worker monitor\")\n        monitor.print_stats()\n    finally:\n        await processor.stop()\n\nif __name__ == \"__main__\":\n    asyncio.run(monitor_workers())\n\n\nCustom Event-Driven Application\nBuild applications that react to job lifecycle events:\n# notification_system.py\nimport asyncio\nfrom naq.events import AsyncJobEventProcessor, JobEventType\nfrom naq import enqueue\n\nclass NotificationSystem:\n    \"\"\"Example system that reacts to job events.\"\"\"\n    \n    def __init__(self):\n        self.failed_jobs = []\n        self.slow_jobs = []\n    \n    async def handle_job_completed(self, event):\n        \"\"\"Handle successful job completion.\"\"\"\n        if event.duration_ms and event.duration_ms &gt; 5000:\n            self.slow_jobs.append({\n                'job_id': event.job_id,\n                'duration': event.duration_ms,\n                'queue': event.queue_name\n            })\n            \n            # Maybe enqueue a follow-up optimization job\n            await enqueue(\n                self.analyze_slow_job,\n                job_id=event.job_id,\n                duration=event.duration_ms,\n                queue_name=\"analytics\"\n            )\n    \n    async def handle_job_failed(self, event):\n        \"\"\"Handle job failures.\"\"\"\n        self.failed_jobs.append({\n            'job_id': event.job_id,\n            'error': event.error_message,\n            'worker': event.worker_id,\n            'timestamp': event.timestamp\n        })\n        \n        # Enqueue failure analysis\n        await enqueue(\n            self.analyze_failure,\n            job_id=event.job_id,\n            error_message=event.error_message,\n            queue_name=\"support\"\n        )\n        \n        print(f\"üìß Failure notification sent for job {event.job_id}\")\n    \n    async def analyze_slow_job(self, job_id, duration):\n        \"\"\"Analyze why a job was slow.\"\"\"\n        print(f\"üîç Analyzing slow job {job_id} (took {duration}ms)\")\n        # Implement analysis logic\n        return {\"analysis\": \"completed\", \"recommendations\": [\"optimize_query\"]}\n    \n    async def analyze_failure(self, job_id, error_message):\n        \"\"\"Analyze job failure.\"\"\"\n        print(f\"üîç Analyzing failed job {job_id}: {error_message}\")\n        # Implement failure analysis logic\n        return {\"analysis\": \"completed\", \"likely_cause\": \"timeout\"}\n    \n    async def get_health_report(self):\n        \"\"\"Generate system health report.\"\"\"\n        return {\n            \"failed_jobs_count\": len(self.failed_jobs),\n            \"slow_jobs_count\": len(self.slow_jobs),\n            \"recent_failures\": self.failed_jobs[-5:],\n            \"recent_slow_jobs\": self.slow_jobs[-5:]\n        }\n\nasync def run_notification_system():\n    \"\"\"Run the notification system.\"\"\"\n    notification_system = NotificationSystem()\n    processor = AsyncJobEventProcessor()\n    \n    # Register event handlers\n    processor.add_handler(JobEventType.COMPLETED, notification_system.handle_job_completed)\n    processor.add_handler(JobEventType.FAILED, notification_system.handle_job_failed)\n    \n    await processor.start()\n    \n    try:\n        print(\"üì¢ Notification system active... (Press Ctrl+C to stop)\")\n        async for event in processor.stream_job_events():\n            pass\n    except KeyboardInterrupt:\n        print(\"\\nüëã Stopping notification system\")\n        # Print final health report\n        health_report = await notification_system.get_health_report()\n        print(f\"üìä Final Health Report: {health_report}\")\n    finally:\n        await processor.stop()\n\nif __name__ == \"__main__\":\n    asyncio.run(run_notification_system())\n\n\nSchedule Management with Event Tracking\nMonitor and manage scheduled jobs:\n# schedule_manager.py\nimport asyncio\nfrom datetime import datetime, timedelta\nfrom naq import Queue\nfrom naq.events import AsyncJobEventProcessor, JobEventType\n\nasync def scheduled_task(task_name, run_count):\n    \"\"\"Example scheduled task.\"\"\"\n    print(f\"üöÄ Running {task_name} (execution #{run_count})\")\n    await asyncio.sleep(2)\n    return f\"Task {task_name} completed\"\n\nasync def demo_schedule_management():\n    \"\"\"Demonstrate schedule management with event tracking.\"\"\"\n    queue = Queue(\"scheduled_demo\")\n    processor = AsyncJobEventProcessor()\n    \n    # Track schedule events\n    def track_schedule_events(event):\n        if 'schedule' in event.event_type.value:\n            print(f\"üìÖ {event.event_type}: {event.job_id} - {event.message}\")\n    \n    processor.add_global_handler(track_schedule_events)\n    await processor.start()\n    \n    try:\n        # Schedule a recurring job (every 30 seconds, 3 times)\n        scheduled_job = await queue.schedule(\n            scheduled_task,\n            task_name=\"data_sync\",\n            run_count=1,\n            interval=timedelta(seconds=30),\n            repeat=3\n        )\n        \n        print(f\"üìã Scheduled job {scheduled_job.job_id}\")\n        \n        # Let it run for a bit\n        await asyncio.sleep(45)\n        \n        # Pause the scheduled job\n        print(\"‚è∏Ô∏è  Pausing scheduled job...\")\n        await queue.pause_scheduled_job(scheduled_job.job_id)\n        \n        await asyncio.sleep(10)\n        \n        # Resume it\n        print(\"‚ñ∂Ô∏è  Resuming scheduled job...\")\n        await queue.resume_scheduled_job(scheduled_job.job_id)\n        \n        await asyncio.sleep(45)\n        \n        # Finally cancel it\n        print(\"‚ùå Cancelling scheduled job...\")\n        await queue.cancel_scheduled_job(scheduled_job.job_id)\n        \n    finally:\n        await processor.stop()\n        await queue.close()\n\nif __name__ == \"__main__\":\n    asyncio.run(demo_schedule_management())\nThese examples show how NAQ‚Äôs event system enables:\n\nCustom Monitoring: Build tailored monitoring solutions\nReactive Applications: Create systems that respond to job events\n\nPerformance Analytics: Track and analyze system performance\nAutomated Management: Implement automated responses to events\nBusiness Intelligence: Extract insights from job processing patterns"
  },
  {
    "objectID": "event-logging.html",
    "href": "event-logging.html",
    "title": "Event-Driven State Management",
    "section": "",
    "text": "NAQ provides comprehensive event-driven state management using NATS JetStream for complete observability, monitoring, and debugging of your job queues."
  },
  {
    "objectID": "event-logging.html#overview",
    "href": "event-logging.html#overview",
    "title": "Event-Driven State Management",
    "section": "Overview",
    "text": "Overview\nThe event logging system captures all state transitions and lifecycle events across:\n\nJob Lifecycle: Enqueue, start, completion, failures, retries\nSchedule Management: Job scheduling, triggers, pauses, resumes, modifications\n\nWorker Status: Worker start/stop, status changes, heartbeats, errors\n\nAll events are stored in a NATS JetStream event stream providing: - Complete Audit Trail: Immutable record of all system activity - Real-time Monitoring: Live event streaming for system observability - Historical Analysis: Query past events for debugging and analytics - High Performance: Non-blocking async event logging with minimal latency impact"
  },
  {
    "objectID": "event-logging.html#architecture",
    "href": "event-logging.html#architecture",
    "title": "Event-Driven State Management",
    "section": "Architecture",
    "text": "Architecture\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ     Worker      ‚îÇ    ‚îÇ     Queue       ‚îÇ    ‚îÇ   Scheduler     ‚îÇ\n‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ    ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ    ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n‚îÇ  ‚îÇJob Events ‚îÇ  ‚îÇ    ‚îÇ  ‚îÇJob Events ‚îÇ  ‚îÇ    ‚îÇ  ‚îÇJob Events ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇWorker     ‚îÇ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÇSchedule   ‚îÇ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÇSchedule   ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇEvents     ‚îÇ  ‚îÇ    ‚îÇ  ‚îÇEvents     ‚îÇ  ‚îÇ    ‚îÇ  ‚îÇEvents     ‚îÇ  ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n         ‚îÇ                       ‚îÇ                       ‚îÇ\n         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n                                 ‚ñº\n                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n                    ‚îÇ   NATS JetStream        ‚îÇ\n                    ‚îÇ   Event Stream          ‚îÇ\n                    ‚îÇ   (NAQ_JOB_EVENTS)      ‚îÇ\n                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n                                 ‚îÇ\n                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n                    ‚îÇ   Event Processors      ‚îÇ\n                    ‚îÇ   - Real-time Monitor   ‚îÇ\n                    ‚îÇ   - Analytics          ‚îÇ\n                    ‚îÇ   - Alerting           ‚îÇ\n                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò"
  },
  {
    "objectID": "event-logging.html#why-event-driven-state-management",
    "href": "event-logging.html#why-event-driven-state-management",
    "title": "Event-Driven State Management",
    "section": "Why Event-Driven State Management?",
    "text": "Why Event-Driven State Management?\nEvent-driven state management transforms NAQ from a simple task queue into a complete observability platform:\n\nReal-time Monitoring: See job progress as it happens\nDebugging: Track job failures with detailed error information\n\nPerformance Analysis: Monitor job durations and throughput\nReactive Systems: Build event-driven applications that respond to job lifecycle changes\nAudit Trail: Complete history of all job operations\nAlerting: React to job failures or performance issues instantly"
  },
  {
    "objectID": "event-logging.html#quick-start-monitoring-jobs",
    "href": "event-logging.html#quick-start-monitoring-jobs",
    "title": "Event-Driven State Management",
    "section": "Quick Start: Monitoring Jobs",
    "text": "Quick Start: Monitoring Jobs\nThe fastest way to see NAQ‚Äôs event logging in action is using the built-in monitoring command:\n# Monitor all job events in real-time\nnaq events\n\n# Monitor events for a specific job\nnaq events --job-id abc123def456\n\n# Monitor only failed jobs\nnaq events --event-type failed\n\n# Output in JSON format for processing\nnaq events --format json\n\n# Monitor worker status and activity\nnaq events workers --refresh 5\n\n# Get event statistics and analytics\nnaq events stats --hours 24 --by-queue\nThis command connects to your NATS event stream and displays job lifecycle events as they occur, giving you instant visibility into your job processing."
  },
  {
    "objectID": "event-logging.html#job-event-types",
    "href": "event-logging.html#job-event-types",
    "title": "Event-Driven State Management",
    "section": "Job Event Types",
    "text": "Job Event Types\nNAQ captures every significant moment in a job‚Äôs lifecycle:\n\n\n\n\n\n\n\n\nEvent Type\nDescription\nWhen It Occurs\n\n\n\n\nENQUEUED\nJob added to queue\nWhen enqueue(), enqueue_at(), or enqueue_in() is called\n\n\nSTARTED\nJob execution begins\nWhen a worker picks up the job and starts processing\n\n\nCOMPLETED\nJob finished successfully\nWhen job function returns without errors\n\n\nFAILED\nJob execution failed\nWhen job function raises an exception\n\n\nRETRY_SCHEDULED\nJob scheduled for retry\nWhen a failed job will be retried\n\n\nSCHEDULED\nJob scheduled for future\nWhen using schedule() with cron or interval\n\n\nSCHEDULE_TRIGGERED\nScheduled job enqueued\nWhen scheduler moves a scheduled job to active queue\n\n\nSCHEDULER_ERROR\nScheduler error occurred\nWhen scheduler encounters errors during operation\n\n\n\nEach event includes rich metadata like timestamps, worker IDs, queue names, error details, and execution duration."
  },
  {
    "objectID": "event-logging.html#cli-monitoring",
    "href": "event-logging.html#cli-monitoring",
    "title": "Event-Driven State Management",
    "section": "CLI Monitoring",
    "text": "CLI Monitoring\nThe naq events command provides powerful filtering and formatting options:\n\nBasic Usage\n# Monitor all events\nnaq events\n\n# Monitor specific queue\nnaq events --queue high-priority\n\n# Monitor specific worker\nnaq events --worker worker-1\n\n# Combine filters\nnaq events --queue urgent --event-type failed\n\n\nOutput Formats\n# Table format (default) - human readable\nnaq events --format table\n\n# JSON format - machine readable\nnaq events --format json\n\n# Raw format - compact logging\nnaq events --format raw\n\n\nExample Output\nTime      Job ID       Event      Queue        Worker       Message\n14:30:15  abc123...    enqueued   default      -            Job abc123 enqueued to default\n14:30:16  abc123...    started    default      worker-1     Job abc123 started by worker worker-1\n14:30:19  abc123...    completed  default      worker-1     Job abc123 completed successfully in 2847ms"
  },
  {
    "objectID": "event-logging.html#programmatic-event-processing",
    "href": "event-logging.html#programmatic-event-processing",
    "title": "Event-Driven State Management",
    "section": "Programmatic Event Processing",
    "text": "Programmatic Event Processing\nFor advanced use cases, you can programmatically process events in your applications:\n\nBasic Event Handler\nimport asyncio\nfrom naq.events import AsyncJobEventProcessor\n\nasync def main():\n    # Create event processor\n    processor = AsyncJobEventProcessor()\n    \n    # Register handler for failed jobs\n    def handle_job_failure(event):\n        print(f\"üö® Job {event.job_id} failed: {event.error_message}\")\n        # Send alert, log to monitoring system, etc.\n    \n    processor.on_job_failed(handle_job_failure)\n    \n    # Start processing events\n    await processor.start()\n    \n    # Keep running (in real app, this would be your main loop)\n    try:\n        await asyncio.sleep(3600)  # Run for 1 hour\n    finally:\n        await processor.stop()\n\nasyncio.run(main())\n\n\nAdvanced Event Processing\nfrom naq.events import AsyncJobEventProcessor, JobEventType\n\nasync def build_monitoring_system():\n    processor = AsyncJobEventProcessor()\n    \n    # Track job statistics\n    job_stats = {\"completed\": 0, \"failed\": 0, \"total_duration\": 0}\n    \n    def update_stats(event):\n        if event.event_type == JobEventType.COMPLETED:\n            job_stats[\"completed\"] += 1\n            if event.duration_ms:\n                job_stats[\"total_duration\"] += event.duration_ms\n        elif event.event_type == JobEventType.FAILED:\n            job_stats[\"failed\"] += 1\n    \n    # Global handler receives all events\n    processor.add_global_handler(update_stats)\n    \n    # Specific handlers for different event types\n    processor.on_job_failed(lambda e: send_alert(f\"Job failed: {e.job_id}\"))\n    processor.on_job_completed(lambda e: log_success(e.job_id, e.duration_ms))\n    \n    await processor.start()\n    return processor\n\ndef send_alert(message):\n    # Integration with monitoring systems\n    print(f\"ALERT: {message}\")\n\ndef log_success(job_id, duration_ms):\n    print(f\"‚úÖ Job {job_id} completed in {duration_ms}ms\")\n\n\nEvent-Driven Workflows\nBuild reactive systems that respond to job completions:\nfrom naq import enqueue\nfrom naq.events import AsyncJobEventProcessor\n\n# Define workflow tasks\nasync def process_image(image_path):\n    # Process the image\n    return f\"processed_{image_path}\"\n\nasync def generate_thumbnail(processed_path):\n    # Generate thumbnail from processed image\n    return f\"thumb_{processed_path}\"\n\nasync def send_notification(user_id, image_path):\n    # Notify user that processing is complete\n    print(f\"Image {image_path} ready for user {user_id}\")\n\nasync def image_workflow():\n    processor = AsyncJobEventProcessor()\n    \n    # When image processing completes, start thumbnail generation\n    async def on_image_processed(event):\n        if event.event_type == JobEventType.COMPLETED:\n            # Check if this was an image processing job\n            if \"process_image\" in str(event.message):\n                # Get the result and enqueue thumbnail job\n                result = await get_job_result(event.job_id)\n                await enqueue(generate_thumbnail, result)\n    \n    # When thumbnail generation completes, notify user\n    async def on_thumbnail_generated(event):\n        if event.event_type == JobEventType.COMPLETED:\n            if \"generate_thumbnail\" in str(event.message):\n                await enqueue(send_notification, \"user123\", event.job_id)\n    \n    processor.add_global_handler(on_image_processed)\n    processor.add_global_handler(on_thumbnail_generated)\n    \n    await processor.start()\n    return processor\n\nasync def get_job_result(job_id):\n    # Implementation to fetch job result\n    pass"
  },
  {
    "objectID": "event-logging.html#configuration",
    "href": "event-logging.html#configuration",
    "title": "Event-Driven State Management",
    "section": "Configuration",
    "text": "Configuration\nEvent logging can be configured via environment variables:\n\nCore Settings\n\n\n\n\n\n\n\n\nVariable\nDefault\nDescription\n\n\n\n\nNAQ_EVENTS_ENABLED\ntrue\nEnable/disable event logging system\n\n\nNAQ_EVENT_STORAGE_URL\nSame as NAQ_NATS_URL\nNATS URL for event storage\n\n\nNAQ_EVENT_STREAM_NAME\nNAQ_JOB_EVENTS\nJetStream stream name for events\n\n\nNAQ_EVENT_SUBJECT_PREFIX\nnaq.jobs.events\nBase subject for event routing\n\n\n\n\n\nPerformance Tuning\n\n\n\n\n\n\n\n\nVariable\nDefault\nDescription\n\n\n\n\nNAQ_EVENT_BATCH_SIZE\n100\nNumber of events to buffer before flushing\n\n\nNAQ_EVENT_FLUSH_INTERVAL\n5.0\nSeconds between automatic flushes\n\n\nNAQ_EVENT_MAX_BUFFER_SIZE\n10000\nMaximum events to buffer (prevents memory issues)\n\n\nNAQ_EVENT_MAX_CONCURRENT_HANDLERS\n10\nMaximum concurrent event handlers\n\n\nNAQ_EVENT_BUFFER_SIZE\n1000\nEvent buffer size for batching\n\n\n\n\n\nEvent Filtering\n\n\n\n\n\n\n\n\nVariable\nDefault\nDescription\n\n\n\n\nNAQ_EVENT_EXCLUDE_HEARTBEATS\ntrue\nExclude worker heartbeat events from storage\n\n\nNAQ_EVENT_MIN_JOB_DURATION\n0\nMinimum job duration (ms) to log completion events\n\n\n\n\n\nExample Configuration\n# Enable event logging with custom settings\nexport NAQ_EVENTS_ENABLED=true\nexport NAQ_EVENT_STREAM_NAME=MY_JOB_EVENTS\nexport NAQ_EVENT_LOGGER_BATCH_SIZE=50\nexport NAQ_EVENT_LOGGER_FLUSH_INTERVAL=2.0\n\n# Run your application\npython my_app.py"
  },
  {
    "objectID": "event-logging.html#architecture-performance",
    "href": "event-logging.html#architecture-performance",
    "title": "Event-Driven State Management",
    "section": "Architecture & Performance",
    "text": "Architecture & Performance\n\nNATS JetStream Integration\nNAQ‚Äôs event logging is built on NATS JetStream, providing:\n\nDurable Storage: Events survive server restarts\nOrdered Delivery: Events maintain chronological order\nHigh Throughput: Optimized for high-volume event streams\nSubject Filtering: Efficient filtering by job, queue, or worker\nClustering: Horizontal scaling and high availability\n\n\n\nEvent Storage Structure\nEvents are stored using a hierarchical NATS subject structure:\nnaq.jobs.events.{job_id}.{context}.{event_type}\nExamples: - naq.jobs.events.abc123.worker.worker-1.started - naq.jobs.events.def456.queue.high-priority.enqueued - naq.jobs.events.ghi789.system.completed\nThis structure enables efficient filtering and routing of events.\n\n\nPerformance Characteristics\n\nBuffered Logging: Events are batched for efficiency\nNon-blocking: Event logging never blocks job execution\nRetry Logic: Failed event writes are automatically retried\nMemory Management: Automatic buffer limits prevent memory issues\nBackground Processing: All event I/O happens in background tasks\n\n\n\n\n\n\n\nTip\n\n\n\nPerformance Tip\nFor high-throughput applications, adjust batch size and flush interval:\n# Process more events per batch, flush less frequently\nexport NAQ_EVENT_LOGGER_BATCH_SIZE=500\nexport NAQ_EVENT_LOGGER_FLUSH_INTERVAL=10.0\nThis reduces overhead but increases memory usage and potential data loss window.\n\n\n\n\nAdvanced Configuration\nFor high-performance applications, configure event processing limits:\n# Control event handler concurrency\nexport NAQ_EVENT_MAX_CONCURRENT_HANDLERS=20\n\n# Adjust event buffer sizes\nexport NAQ_EVENT_BUFFER_SIZE=2000\n\n# Filter out high-frequency events\nexport NAQ_EVENT_EXCLUDE_HEARTBEATS=true\nexport NAQ_EVENT_MIN_JOB_DURATION=100  # Only log jobs &gt; 100ms\n\n\nConfiguration in Code\nYou can also configure events programmatically:\nfrom naq.config import NAQConfig, EventsConfig\n\nconfig = NAQConfig(\n    events=EventsConfig(\n        enabled=True,\n        batch_size=200,\n        flush_interval=2.0,\n        max_buffer_size=20000,\n        filters=EventFiltersConfig(\n            exclude_heartbeats=True,\n            min_job_duration=50  # ms\n        )\n    )\n)"
  },
  {
    "objectID": "event-logging.html#use-cases",
    "href": "event-logging.html#use-cases",
    "title": "Event-Driven State Management",
    "section": "Use Cases",
    "text": "Use Cases\n\nProduction Monitoring\n# Monitor production job health\nfrom naq.events import AsyncJobEventProcessor\n\nasync def production_monitor():\n    processor = AsyncJobEventProcessor()\n    \n    # Track failure rate\n    failure_count = 0\n    total_count = 0\n    \n    def track_jobs(event):\n        nonlocal failure_count, total_count\n        if event.event_type in [JobEventType.COMPLETED, JobEventType.FAILED]:\n            total_count += 1\n            if event.event_type == JobEventType.FAILED:\n                failure_count += 1\n            \n            # Alert if failure rate exceeds 10%\n            failure_rate = failure_count / total_count\n            if failure_rate &gt; 0.1 and total_count &gt; 10:\n                send_alert(f\"High failure rate: {failure_rate:.1%}\")\n    \n    processor.add_global_handler(track_jobs)\n    await processor.start()\n\n\nPerformance Analysis\n# Analyze job performance patterns\nimport statistics\nfrom collections import defaultdict\n\nasync def performance_analyzer():\n    processor = AsyncJobEventProcessor()\n    durations = defaultdict(list)  # queue_name -&gt; [durations]\n    \n    def analyze_performance(event):\n        if event.event_type == JobEventType.COMPLETED and event.duration_ms:\n            durations[event.queue_name].append(event.duration_ms)\n            \n            # Log statistics every 100 jobs per queue\n            if len(durations[event.queue_name]) % 100 == 0:\n                queue_durations = durations[event.queue_name]\n                avg_duration = statistics.mean(queue_durations)\n                p95_duration = statistics.quantiles(queue_durations, n=20)[18]  # 95th percentile\n                \n                print(f\"Queue {event.queue_name}: avg={avg_duration:.0f}ms, p95={p95_duration:.0f}ms\")\n    \n    processor.add_global_handler(analyze_performance)\n    await processor.start()\n\n\nDebugging Failed Jobs\n# Detailed failure analysis\nasync def debug_failures():\n    processor = AsyncJobEventProcessor()\n    \n    def debug_failure(event):\n        print(f\"\"\"\n        üö® Job Failure Debug Report\n        ===========================\n        Job ID: {event.job_id}\n        Queue: {event.queue_name}\n        Worker: {event.worker_id}\n        Error Type: {event.error_type}\n        Error Message: {event.error_message}\n        Duration: {event.duration_ms}ms\n        Timestamp: {event.timestamp}\n        \"\"\")\n    \n    processor.on_job_failed(debug_failure)\n    await processor.start()\n\n\n\n\n\n\nNote\n\n\n\nIntegration Note\nThe event logging system integrates seamlessly with existing NAQ functionality. When enabled, it automatically captures events from all job operations without requiring code changes to your existing tasks or workers."
  },
  {
    "objectID": "event-logging.html#whats-next",
    "href": "event-logging.html#whats-next",
    "title": "Event-Driven State Management",
    "section": "What‚Äôs Next?",
    "text": "What‚Äôs Next?\n\nExplore the Event Logging API Reference for detailed technical documentation\nLearn about advanced configuration options\nCheck out more event-driven examples\nUnderstand the architecture behind the event system"
  },
  {
    "objectID": "configuration.html",
    "href": "configuration.html",
    "title": "Configuration",
    "section": "",
    "text": "NAQ provides a comprehensive YAML-based configuration system that supports hierarchical configuration loading, validation, and environment-specific overrides while maintaining full backward compatibility with environment variables."
  },
  {
    "objectID": "configuration.html#quick-start",
    "href": "configuration.html#quick-start",
    "title": "Configuration",
    "section": "Quick Start",
    "text": "Quick Start\n\nCreating Your First Configuration\nUse the CLI to generate a configuration file for your environment:\n# Create development configuration\nnaq system config-init --environment development\n\n# Create production configuration  \nnaq system config-init --environment production --output ./prod-config.yaml\nThis creates a naq.yaml file in your current directory with sensible defaults for your chosen environment.\n\n\nBasic Configuration Example\n# naq.yaml\nnats:\n  servers: [\"nats://localhost:4222\"]\n  client_name: my-app\n  \nworkers:\n  concurrency: 10\n  heartbeat_interval: 15\n  \nevents:\n  enabled: true\n  batch_size: 100\n  \nlogging:\n  level: INFO\n  format: text"
  },
  {
    "objectID": "configuration.html#configuration-discovery",
    "href": "configuration.html#configuration-discovery",
    "title": "Configuration",
    "section": "Configuration Discovery",
    "text": "Configuration Discovery\nNAQ loads configuration from multiple sources with the following priority (highest to lowest):\n\nExplicit config file (via --config flag or config_path parameter)\nCurrent directory: ./naq.yaml or ./naq.yml\nUser config directory: ~/.naq/config.yaml\nSystem config directory: /etc/naq/config.yaml\nEnvironment variables: NAQ_* variables\nBuilt-in defaults\n\n\nConfiguration File Locations\n# Check which configuration files are being used\nnaq system config --sources\n\n# Validate current configuration\nnaq system config --validate\n\n# View current effective configuration\nnaq system config --show"
  },
  {
    "objectID": "configuration.html#configuration-structure",
    "href": "configuration.html#configuration-structure",
    "title": "Configuration",
    "section": "Configuration Structure",
    "text": "Configuration Structure\n\nComplete Configuration Schema\n# Complete NAQ configuration with all available options\nnats:\n  servers:\n    - \"nats://primary:4222\"\n    - \"nats://backup:4222\"\n  client_name: \"my-app\"\n  max_reconnect_attempts: 5\n  reconnect_time_wait: 2.0\n  connection_timeout: 10.0\n  drain_timeout: 30.0\n  auth:\n    username: \"${NATS_USER}\"\n    password: \"${NATS_PASS}\"\n  tls:\n    enabled: false\n    cert_file: \"/path/to/cert.pem\"\n    key_file: \"/path/to/key.pem\"\n    ca_file: \"/path/to/ca.pem\"\n\nqueues:\n  default: \"main_queue\"\n  prefix: \"myapp\"\n  configs:\n    urgent:\n      ack_wait: 30\n      max_deliver: 5\n    background:\n      ack_wait: 300\n      max_deliver: 3\n\nworkers:\n  concurrency: 10\n  heartbeat_interval: 15\n  ttl: 60\n  max_job_duration: 3600\n  shutdown_timeout: 30\n  pools:\n    priority:\n      concurrency: 5\n      queues: [\"urgent\", \"critical\"]\n    background:\n      concurrency: 3\n      queues: [\"background\", \"cleanup\"]\n\nscheduler:\n  enabled: true\n  lock_ttl: 30\n  lock_renew_interval: 15\n  max_failures: 5\n  scan_interval: 1.0\n\nevents:\n  enabled: true\n  batch_size: 100\n  flush_interval: 5.0\n  max_buffer_size: 10000\n  storage_url: \"nats://events:4222\"\n  subject_prefix: \"myapp.jobs.events\"\n  stream:\n    name: \"MYAPP_JOB_EVENTS\"\n    max_age: \"7d\"\n    max_bytes: \"1GB\"\n    replicas: 1\n  filters:\n    exclude_heartbeats: true\n    min_job_duration: 100\n\nresults:\n  ttl: 604800  # 7 days\n  cleanup_interval: 3600\n  kv_bucket: \"myapp_results\"\n\nserialization:\n  job_serializer: \"pickle\"\n  compression: false\n  json:\n    encoder: \"json.JSONEncoder\"\n    decoder: \"json.JSONDecoder\"\n\nlogging:\n  level: \"INFO\"\n  format: \"json\"\n  file: \"/var/log/naq/naq.log\"\n  max_size: \"100MB\"\n  backup_count: 5\n  to_file_enabled: true\n\n\nEnvironment Variable Interpolation\nConfiguration files support environment variable interpolation using ${VARIABLE:default} syntax:\nnats:\n  servers: \n    - \"${NAQ_NATS_URL:nats://localhost:4222}\"\n    - \"${NATS_BACKUP_URL}\"  # Required, no default\n  auth:\n    username: \"${NATS_USER}\"\n    password: \"${NATS_PASS}\"\n\nworkers:\n  concurrency: \"${NAQ_CONCURRENCY:10}\"\n  \nlogging:\n  level: \"${LOG_LEVEL:INFO}\""
  },
  {
    "objectID": "configuration.html#environment-specific-configuration",
    "href": "configuration.html#environment-specific-configuration",
    "title": "Configuration",
    "section": "Environment-Specific Configuration",
    "text": "Environment-Specific Configuration\nDefine environment-specific overrides within your configuration file:\n# Base configuration\nnats:\n  servers: [\"nats://localhost:4222\"]\nworkers:\n  concurrency: 10\nlogging:\n  level: \"INFO\"\n\n# Environment overrides\nenvironments:\n  development:\n    name: development\n    overrides:\n      logging.level: \"DEBUG\"\n      workers.concurrency: 2\n      events.batch_size: 10\n      \n  production:\n    name: production\n    overrides:\n      logging.level: \"INFO\"\n      logging.format: \"json\"\n      workers.concurrency: 50\n      events.batch_size: 500\n      \n  testing:\n    name: testing\n    overrides:\n      logging.level: \"WARNING\"\n      events.enabled: false\n      scheduler.enabled: false\n\nUsing Environment Overrides\nSet the NAQ_ENVIRONMENT variable to apply specific overrides:\n# Apply development overrides\nexport NAQ_ENVIRONMENT=development\nnaq worker start\n\n# Apply production overrides  \nexport NAQ_ENVIRONMENT=production\nnaq worker start"
  },
  {
    "objectID": "configuration.html#configuration-templates",
    "href": "configuration.html#configuration-templates",
    "title": "Configuration",
    "section": "Configuration Templates",
    "text": "Configuration Templates\n\nDevelopment Environment\n# development.yaml - optimized for local development\nnats:\n  servers: [\"nats://localhost:4222\"]\n  client_name: \"dev-client\"\n\nworkers:\n  concurrency: 2  # Low concurrency for debugging\n  heartbeat_interval: 15\n\nevents:\n  enabled: true\n  batch_size: 10  # Small batches for quick feedback\n  flush_interval: 2.0\n  filters:\n    exclude_heartbeats: false  # Include all events for debugging\n\nlogging:\n  level: \"DEBUG\"\n  format: \"text\"  # Human-readable logs\n  file: null  # Log to stdout\n\nscheduler:\n  enabled: true\n  scan_interval: 1.0  # Fast scanning for development\n\n\nProduction Environment\n# production.yaml - optimized for performance and reliability\nnats:\n  servers:\n    - \"nats://nats-1:4222\" \n    - \"nats://nats-2:4222\"\n    - \"nats://nats-3:4222\"\n  client_name: \"prod-client\"\n  max_reconnect_attempts: 10\n  auth:\n    username: \"${NATS_USER}\"\n    password: \"${NATS_PASS}\"\n\nworkers:\n  concurrency: 50\n  heartbeat_interval: 30\n  max_job_duration: 7200\n\nevents:\n  enabled: true\n  batch_size: 500  # Large batches for efficiency\n  flush_interval: 1.0\n  max_buffer_size: 50000\n  stream:\n    name: \"PROD_JOB_EVENTS\"\n    max_age: \"30d\"\n    max_bytes: \"10GB\"\n    replicas: 3\n\nlogging:\n  level: \"INFO\"\n  format: \"json\"  # Structured logs for monitoring\n  file: \"/var/log/naq/naq.log\"\n  to_file_enabled: true\n\nscheduler:\n  enabled: true\n  lock_ttl: 60\n  scan_interval: 5.0\n\n\nTesting Environment\n# testing.yaml - minimal configuration for tests\nnats:\n  servers: [\"nats://localhost:4222\"]\n\nworkers:\n  concurrency: 1  # Single worker for predictable tests\n  heartbeat_interval: 5\n\nscheduler:\n  enabled: false  # Disable for most tests\n\nevents:\n  enabled: false  # Reduce noise in test logs\n\nlogging:\n  level: \"WARNING\"\n  format: \"text\"\n\nresults:\n  ttl: 60  # Short TTL for test cleanup"
  },
  {
    "objectID": "configuration.html#cli-configuration-management",
    "href": "configuration.html#cli-configuration-management",
    "title": "Configuration",
    "section": "CLI Configuration Management",
    "text": "CLI Configuration Management\n\nConfiguration Commands\n# Show current configuration\nnaq system config --show\n\n# Show configuration in JSON format\nnaq system config --show --format json\n\n# Show configuration sources\nnaq system config --sources\n\n# Validate configuration\nnaq system config --validate\n\n# Validate specific file\nnaq system config --validate --config ./my-config.yaml\n\n# Create new configuration\nnaq system config-init --environment production\n\n# Create configuration with custom output path\nnaq system config-init --environment development --output ./dev-config.yaml\n\n# Overwrite existing configuration\nnaq system config-init --force\n\n\nIntegration with Workers and Services\n# Start worker with specific configuration\nnaq worker start --config ./production.yaml\n\n# Start scheduler with configuration\nnaq scheduler start --config ./production.yaml\n\n# Use configuration in application\npython -c \"\nfrom naq.config import load_config\nconfig = load_config('./my-config.yaml')\nprint(f'NATS servers: {config.nats.servers}')\n\""
  },
  {
    "objectID": "configuration.html#configuration-validation",
    "href": "configuration.html#configuration-validation",
    "title": "Configuration",
    "section": "Configuration Validation",
    "text": "Configuration Validation\nNAQ provides comprehensive configuration validation using JSON Schema and custom business rules.\n\nSchema Validation\nAll configuration is automatically validated against a comprehensive JSON schema that checks:\n\nData types: Ensures values are the correct type (string, number, boolean)\nValue ranges: Validates numeric ranges and string patterns\nRequired fields: Ensures essential configuration is present\nFormat validation: Validates URLs, file paths, and other formatted strings\n\n\n\nBusiness Rules Validation\nAdditional validation includes:\n\nNATS server URL format validation\nFile existence checks for TLS certificates\nLogical consistency (e.g., lock renewal interval &lt; lock TTL)\nResource limits (reasonable concurrency values, timeout relationships)\n\n\n\nValidation Examples\n# Validate current configuration\nnaq system config --validate\n\n# Example validation output for invalid config\nConfiguration validation failed:\n- NATS servers list cannot be empty\n- Worker concurrency must be at least 1\n- Events flush interval must be at least 0.1 seconds\n- NATS TLS cert file not found: /missing/cert.pem"
  },
  {
    "objectID": "configuration.html#type-safe-configuration-access",
    "href": "configuration.html#type-safe-configuration-access",
    "title": "Configuration",
    "section": "Type-Safe Configuration Access",
    "text": "Type-Safe Configuration Access\nWhen using NAQ in Python applications, you get full type safety and IDE support:\nfrom naq.config import load_config\n\n# Load and validate configuration\nconfig = load_config(\"./my-config.yaml\")\n\n# Type-safe access with IDE autocompletion\nnats_url = config.nats.servers[0]  # str\nworker_count = config.workers.concurrency  # int\nevents_enabled = config.events.enabled  # bool\n\n# Nested access using dot notation\nprimary_server = config.get_nested(\"nats.servers[0]\")\nbatch_size = config.get_nested(\"events.batch_size\", default=100)\n\n# Configuration serialization\nconfig_dict = config.to_dict()"
  },
  {
    "objectID": "configuration.html#service-layer-integration",
    "href": "configuration.html#service-layer-integration",
    "title": "Configuration",
    "section": "Service Layer Integration",
    "text": "Service Layer Integration\nThe configuration system integrates seamlessly with NAQ‚Äôs service layer:\nfrom naq.services import create_service_manager_from_config\n\n# Create service manager with typed configuration\nservice_manager = create_service_manager_from_config(\"./config.yaml\")\n\n# Services automatically receive typed configuration\nasync with service_manager as services:\n    # All services have access to type-safe configuration\n    connection_service = await services.get_service(ConnectionService)\n    # Connection service uses config.nats for NATS settings"
  },
  {
    "objectID": "configuration.html#backward-compatibility",
    "href": "configuration.html#backward-compatibility",
    "title": "Configuration",
    "section": "Backward Compatibility",
    "text": "Backward Compatibility\nThe new YAML configuration system is fully backward compatible with environment variables. All existing NAQ_* environment variables continue to work and will override corresponding YAML values.\n\nEnvironment Variable Reference\n\n\n\n\n\n\n\n\nEnvironment Variable\nYAML Path\nDescription\n\n\n\n\nNAQ_NATS_URL\nnats.servers[0]\nPrimary NATS server URL\n\n\nNAQ_DEFAULT_QUEUE\nqueues.default\nDefault queue name\n\n\nNAQ_WORKER_CONCURRENCY\nworkers.concurrency\nWorker concurrency level\n\n\nNAQ_LOG_LEVEL\nlogging.level\nLogging level\n\n\nNAQ_EVENTS_ENABLED\nevents.enabled\nEnable/disable event logging\n\n\nNAQ_SCHEDULER_ENABLED\nscheduler.enabled\nEnable/disable scheduler\n\n\n\nView complete mapping table ‚Üí\n\n\nMigration from Environment Variables\n\nGenerate current configuration:\n# This includes current environment variable values\nnaq system config-init --environment development\nMove environment-specific values:\n# Replace hardcoded values with environment interpolation\nnats:\n  servers: [\"${NAQ_NATS_URL:nats://localhost:4222}\"]\nGradually migrate:\n\nStart with YAML file containing environment interpolation\nGradually move values from environment to YAML\nUse environment overrides for deployment differences"
  },
  {
    "objectID": "configuration.html#docker-and-container-deployments",
    "href": "configuration.html#docker-and-container-deployments",
    "title": "Configuration",
    "section": "Docker and Container Deployments",
    "text": "Docker and Container Deployments\n\nContainer-Optimized Configuration\n# docker-config.yaml - optimized for containers\nnats:\n  servers: [\"${NATS_URL:nats://nats:4222}\"]\n  client_name: \"${NAQ_CLIENT_NAME:naq-worker}\"\n  auth:\n    username: \"${NATS_USER}\"\n    password: \"${NATS_PASS}\"\n\nworkers:\n  concurrency: \"${NAQ_CONCURRENCY:10}\"\n  \nlogging:\n  level: \"${NAQ_LOG_LEVEL:INFO}\"\n  format: \"json\"  # Structured logs for container log aggregation\n  file: null      # Always log to stdout in containers\n\nevents:\n  enabled: \"${NAQ_EVENTS_ENABLED:true}\"\n  storage_url: \"${NAQ_EVENT_STORAGE_URL}\"\n\n\nDocker Usage\n# Dockerfile\nFROM python:3.12\nCOPY docker-config.yaml /app/naq.yaml\nENV NAQ_ENVIRONMENT=production\nCMD [\"naq\", \"worker\", \"start\"]\n# Docker run with configuration\ndocker run -v ./config.yaml:/app/naq.yaml \\\n  -e NAQ_ENVIRONMENT=production \\\n  -e NATS_URL=nats://nats-server:4222 \\\n  my-naq-app"
  },
  {
    "objectID": "configuration.html#advanced-configuration-patterns",
    "href": "configuration.html#advanced-configuration-patterns",
    "title": "Configuration",
    "section": "Advanced Configuration Patterns",
    "text": "Advanced Configuration Patterns\n\nDynamic Configuration Updates\nfrom naq.config import reload_config, get_config\n\n# Current configuration\nconfig = get_config()\nprint(f\"Current concurrency: {config.workers.concurrency}\")\n\n# Reload configuration (picks up file changes)\nconfig = reload_config()\nprint(f\"New concurrency: {config.workers.concurrency}\")\n\n\nConfiguration Monitoring\nimport asyncio\nfrom watchdog.observers import Observer\nfrom watchdog.events import FileSystemEventHandler\nfrom naq.config import reload_config\n\nclass ConfigReloadHandler(FileSystemEventHandler):\n    def on_modified(self, event):\n        if event.src_path.endswith('naq.yaml'):\n            print(\"Configuration file changed, reloading...\")\n            try:\n                reload_config()\n                print(\"Configuration reloaded successfully\")\n            except Exception as e:\n                print(f\"Failed to reload configuration: {e}\")\n\n# Monitor configuration file for changes\nobserver = Observer()\nobserver.schedule(ConfigReloadHandler(), path=\".\", recursive=False)\nobserver.start()\n\n\nConfiguration Templating\nFor complex deployments, you can use templating tools with NAQ configuration:\n# config-template.yaml (using Jinja2)\nnats:\n  servers:\n    {% for server in nats_servers %}\n    - \"{{ server }}\"\n    {% endfor %}\n  \nworkers:\n  concurrency: {{ worker_concurrency | default(10) }}\n  \n{% if environment == 'production' %}\nlogging:\n  level: \"INFO\"\n  format: \"json\"\n{% else %}\nlogging:\n  level: \"DEBUG\"\n  format: \"text\"\n{% endif %}"
  },
  {
    "objectID": "configuration.html#troubleshooting",
    "href": "configuration.html#troubleshooting",
    "title": "Configuration",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nCommon Configuration Issues\nConfiguration not found:\n# Check search paths\nnaq system config --sources\n\n# Verify file exists\nls -la naq.yaml ~/.naq/config.yaml /etc/naq/config.yaml\nEnvironment variables not working:\n# Check current environment\nenv | grep NAQ_\n\n# Test specific variable\nexport NAQ_LOG_LEVEL=DEBUG\nnaq system config --show | grep level\nValidation errors:\n# Get detailed validation errors\nnaq system config --validate\n\n# Check specific configuration file\nnaq system config --validate --config ./my-config.yaml\nYAML syntax errors:\n# Use YAML validator\npython -c \"\nimport yaml\nwith open('naq.yaml') as f:\n    try:\n        yaml.safe_load(f)\n        print('YAML syntax is valid')\n    except yaml.YAMLError as e:\n        print(f'YAML syntax error: {e}')\n\"\n\n\nDebug Mode\nEnable debug logging to see configuration loading:\nexport NAQ_LOG_LEVEL=DEBUG\nnaq system config --show\nThis will show detailed information about: - Configuration file discovery - Environment variable processing\n- Value merging and overrides - Validation results"
  },
  {
    "objectID": "configuration.html#best-practices",
    "href": "configuration.html#best-practices",
    "title": "Configuration",
    "section": "Best Practices",
    "text": "Best Practices\n\nDevelopment\n\nUse development environment with debug logging\nKeep configuration files in version control\nUse environment variable interpolation for secrets\nValidate configuration in CI/CD pipelines\n\n\n\nProduction\n\nUse production environment with structured logging\nStore sensitive values in environment variables or secret management\nMonitor configuration changes\nUse separate configurations for different deployment stages\nEnable comprehensive event logging for observability\n\n\n\nSecurity\n\nNever commit secrets to configuration files\nUse environment variable interpolation for sensitive values\nValidate configuration on startup\nRestrict file permissions on configuration files\nUse TLS for NATS connections in production\n\n\n\nPerformance\n\nUse larger batch sizes and longer intervals for high-throughput scenarios\nConfigure appropriate worker concurrency based on workload\nTune event logging parameters for your monitoring needs\nUse connection pooling through the service layer"
  },
  {
    "objectID": "contributing.html",
    "href": "contributing.html",
    "title": "Contributing",
    "section": "",
    "text": "We welcome contributions to naq! Whether you‚Äôre fixing a bug, adding a new feature, or improving the documentation, your help is appreciated."
  },
  {
    "objectID": "contributing.html#getting-started",
    "href": "contributing.html#getting-started",
    "title": "Contributing",
    "section": "Getting Started",
    "text": "Getting Started\nIf you‚Äôre new to the project, a good place to start is by looking at the open issues on GitHub.\n\nDevelopment Setup\nTo get your development environment set up, follow these steps:\n\nFork and Clone the Repository\nStart by forking the main repository on GitHub, and then clone your fork locally:\ngit clone https://github.com/YOUR_USERNAME/naq.git\ncd naq\nInstall Dependencies\nWe recommend using a virtual environment. naq uses uv for dependency management.\n# Create a virtual environment\npython -m venv .venv\nsource .venv/bin/activate\n\n# Install dependencies, including development tools\nuv pip install -e \".[dev]\"\nRun the NATS Server\nThe test suite and examples require a running NATS server. You can use the provided Docker Compose file:\ncd docker\ndocker-compose up -d\nRun the Tests\nTo make sure everything is set up correctly, run the test suite:\npytest\n\n# Or run specific test categories\npytest tests/test_services/    # Service layer tests\npytest tests/test_config/      # Configuration tests\npytest tests/test_compatibility/  # Backward compatibility tests\nSee the Testing Guide for comprehensive testing documentation and patterns."
  },
  {
    "objectID": "contributing.html#making-changes",
    "href": "contributing.html#making-changes",
    "title": "Contributing",
    "section": "Making Changes",
    "text": "Making Changes\n\nCreate a New Branch\nCreate a new branch for your changes:\ngit checkout -b feature/my-new-feature\nWrite Your Code\nMake your changes to the codebase. If you‚Äôre adding a new feature, please include tests.\nFormat Your Code\nBefore committing, make sure your code is formatted correctly:\nruff format .\nruff check --fix .\nCommit and Push\nCommit your changes with a clear and descriptive message, and push them to your fork:\ngit commit -m \"feat: Add my new feature\"\ngit push origin feature/my-new-feature\nCreate a Pull Request\nOpen a pull request from your fork to the main branch of the naq repository. Provide a clear description of your changes and reference any related issues."
  },
  {
    "objectID": "contributing.html#code-of-conduct",
    "href": "contributing.html#code-of-conduct",
    "title": "Contributing",
    "section": "Code of Conduct",
    "text": "Code of Conduct\nPlease note that this project is released with a Contributor Code of Conduct. By participating in this project you agree to abide by its terms.\nThank you for contributing to naq!"
  },
  {
    "objectID": "import-migration.html",
    "href": "import-migration.html",
    "title": "Import Migration Guide",
    "section": "",
    "text": "NAQ 0.2.0 introduces a new modular structure while maintaining full backward compatibility. All existing imports continue to work without modification, and new modular imports are available for improved organization."
  },
  {
    "objectID": "import-migration.html#overview",
    "href": "import-migration.html#overview",
    "title": "Import Migration Guide",
    "section": "",
    "text": "NAQ 0.2.0 introduces a new modular structure while maintaining full backward compatibility. All existing imports continue to work without modification, and new modular imports are available for improved organization."
  },
  {
    "objectID": "import-migration.html#backward-compatibility",
    "href": "import-migration.html#backward-compatibility",
    "title": "Import Migration Guide",
    "section": "Backward Compatibility",
    "text": "Backward Compatibility\nAll existing imports continue to work unchanged:\n# Main functionality - STILL WORKS\nfrom naq import Queue, Worker, Job, enqueue, enqueue_sync\n\n# Models and types - STILL WORKS  \nfrom naq.models import JOB_STATUS, JobEvent, JobResult\n\n# Event logging - STILL WORKS\nfrom naq.events import AsyncJobEventLogger\n\n# Configuration - STILL WORKS\nfrom naq import get_config, DEFAULT_NATS_URL"
  },
  {
    "objectID": "import-migration.html#new-modular-structure-optional",
    "href": "import-migration.html#new-modular-structure-optional",
    "title": "Import Migration Guide",
    "section": "New Modular Structure (Optional)",
    "text": "New Modular Structure (Optional)\nFor new code, you can use the new modular structure for better organization:\n\nModels by Category\n# Job models\nfrom naq.models.jobs import Job, JobResult\n\n# Status and event enums\nfrom naq.models.enums import JOB_STATUS, JobEventType, WorkerEventType\n\n# Event models\nfrom naq.models.events import JobEvent, WorkerEvent\n\n# Schedule models\nfrom naq.models.schedules import Schedule\n\n\nServices (New)\n# Service layer for advanced usage\nfrom naq.services import JobService, EventService, ServiceManager\n\n# Create service manager\nfrom naq.services import create_service_manager_from_config\n\n\nUtilities (New)\n# Decorators\nfrom naq.utils.decorators import retry, timing, log_errors\n\n# Context managers\nfrom naq.utils.context_managers import managed_resource, timeout_context\n\n# Async helpers\nfrom naq.utils.async_helpers import gather_with_concurrency, retry_async\n\n# Error handling\nfrom naq.utils.error_handling import ErrorHandler, wrap_naq_exception\n\n# Structured logging\nfrom naq.utils.logging import StructuredLogger, setup_structured_logging\n\n\nConfiguration (Enhanced)\n# Enhanced configuration system\nfrom naq.config import load_config, get_config, reload_config\n\n# Configuration validation\nfrom naq.config import validate_config_file\n\n# Configuration types\nfrom naq.config.types import NAQConfig"
  },
  {
    "objectID": "import-migration.html#import-equivalence",
    "href": "import-migration.html#import-equivalence",
    "title": "Import Migration Guide",
    "section": "Import Equivalence",
    "text": "Import Equivalence\nOld and new imports refer to the same objects:\n# These are identical\nfrom naq import Job as LegacyJob\nfrom naq.models.jobs import Job as NewJob\nassert LegacyJob is NewJob  # True\n\n# These are identical\nfrom naq.models import JOB_STATUS as LegacyStatus\nfrom naq.models.enums import JOB_STATUS as NewStatus\nassert LegacyStatus is NewStatus  # True"
  },
  {
    "objectID": "import-migration.html#migration-recommendations",
    "href": "import-migration.html#migration-recommendations",
    "title": "Import Migration Guide",
    "section": "Migration Recommendations",
    "text": "Migration Recommendations\n\nFor Existing Projects\n\nNo changes needed - all existing code continues to work\nConsider migrating imports gradually over time\nUse new modular imports for new features\n\n\n\nFor New Projects\n\nUse new modular imports for better organization\nTake advantage of new service layer for complex applications\nUse enhanced configuration system\n\n\n\nFor Large Codebases\n\nMigrate imports gradually, module by module\nUse both old and new imports during transition\nTest thoroughly after each migration step"
  },
  {
    "objectID": "import-migration.html#quick-reference",
    "href": "import-migration.html#quick-reference",
    "title": "Import Migration Guide",
    "section": "Quick Reference",
    "text": "Quick Reference\n\n\n\n\n\n\n\n\nComponent\nLegacy Import\nNew Import\n\n\n\n\nJob\nfrom naq import Job\nfrom naq.models.jobs import Job\n\n\nJOB_STATUS\nfrom naq.models import JOB_STATUS\nfrom naq.models.enums import JOB_STATUS\n\n\nQueue\nfrom naq import Queue\nfrom naq.queue.core import Queue\n\n\nWorker\nfrom naq import Worker\nfrom naq.worker.core import Worker\n\n\nenqueue\nfrom naq import enqueue\nfrom naq.queue.async_api import enqueue\n\n\nJobEvent\nfrom naq.models import JobEvent\nfrom naq.models.events import JobEvent\n\n\nAsyncJobEventLogger\nfrom naq.events import AsyncJobEventLogger\nfrom naq.events.logger import AsyncJobEventLogger"
  },
  {
    "objectID": "import-migration.html#benefits-of-new-structure",
    "href": "import-migration.html#benefits-of-new-structure",
    "title": "Import Migration Guide",
    "section": "Benefits of New Structure",
    "text": "Benefits of New Structure\n\nBetter Organization\n\nRelated functionality grouped into focused modules\nClear separation between models, services, and utilities\nEasier to understand and maintain\n\n\n\nEnhanced Functionality\n\nService layer for dependency injection and centralized operations\nComprehensive utility functions with clear categorization\nEnhanced configuration system with validation\n\n\n\nFuture-Proof\n\nModular structure supports future extensions\nClear API boundaries for plugin development\nBetter type hints and IDE support"
  },
  {
    "objectID": "import-migration.html#migration-examples",
    "href": "import-migration.html#migration-examples",
    "title": "Import Migration Guide",
    "section": "Migration Examples",
    "text": "Migration Examples\n\nBefore (Legacy)\nfrom naq import enqueue_sync, Worker, JOB_STATUS\nfrom naq.models import Job, JobEvent\nfrom naq.events import AsyncJobEventLogger\n\ndef process_job():\n    job = enqueue_sync(my_task, arg1, arg2)\n    worker = Worker(['default'])\n    logger = AsyncJobEventLogger()\n    return job\n\n\nAfter (New Modular - Optional)\nfrom naq.queue.sync_api import enqueue_sync\nfrom naq.worker.core import Worker\nfrom naq.models.enums import JOB_STATUS\nfrom naq.models.jobs import Job\nfrom naq.models.events import JobEvent\nfrom naq.events.logger import AsyncJobEventLogger\n\ndef process_job():\n    job = enqueue_sync(my_task, arg1, arg2)\n    worker = Worker(['default'])\n    logger = AsyncJobEventLogger()\n    return job\nBoth approaches work identically - choose based on your preferences and project needs."
  },
  {
    "objectID": "architecture.html",
    "href": "architecture.html",
    "title": "Architecture Overview",
    "section": "",
    "text": "naq is designed to be a simple yet powerful distributed task queue. Its architecture is fundamentally built around NATS and its persistence layer, JetStream, which serve as the central nervous system for communication between clients, workers, and the scheduler."
  },
  {
    "objectID": "architecture.html#core-components",
    "href": "architecture.html#core-components",
    "title": "Architecture Overview",
    "section": "Core Components",
    "text": "Core Components\nThe naq ecosystem consists of five main components:\n\nThe Client (Producer): Any application that enqueues jobs. This could be a web server, a script, or any other part of your system that needs to offload work.\nNATS Server (with JetStream): The message broker that provides persistence, message delivery, and storage for job results and worker metadata.\nThe Worker(s): The processes that subscribe to queues, execute jobs, and report back their results. You can run as many workers as you need, on as many machines as you want.\nThe Scheduler: A dedicated process that handles time-based events, such as scheduled jobs and recurring tasks.\nThe Service Layer: A centralized service architecture that provides connection management, configuration integration, and resource lifecycle management across all NAQ components."
  },
  {
    "objectID": "architecture.html#how-it-works-the-job-lifecycle",
    "href": "architecture.html#how-it-works-the-job-lifecycle",
    "title": "Architecture Overview",
    "section": "How It Works: The Job Lifecycle",
    "text": "How It Works: The Job Lifecycle\nHere is a high-level overview of what happens when a job is enqueued and processed:\n\n\n\n\n\ngraph TD\n    subgraph \"Your Application\"\n        Client[Client Application]\n        PythonCode[Python Functions]\n    end\n\n    subgraph \"NAQ Service Layer\"\n        ServiceManager[Service Manager]\n        ConnectionService[Connection Service]\n        JobService[Job Service]\n        StreamService[Stream Service]\n        KVService[KV Store Service]\n        EventService[Event Service]\n    end\n\n    subgraph \"NAQ Processes\"\n        Worker[Worker Process]\n        Scheduler[Scheduler Process]\n    end\n\n    subgraph \"NATS Server (JetStream)\"\n        QueueStream[Queue Stream]\n        ResultStore[Result KV Store]\n        ScheduledJobs[Scheduled Jobs KV]\n        EventStream[Event Stream]\n    end\n\n    subgraph \"Configuration\"\n        YAMLConfig[YAML Config]\n        EnvVars[Environment Variables]\n        TypedConfig[Typed Configuration]\n    end\n\n    %% Configuration Flow\n    YAMLConfig --&gt; TypedConfig\n    EnvVars --&gt; TypedConfig\n    TypedConfig --&gt; ServiceManager\n\n    %% Service Layer Flow\n    ServiceManager --&gt; ConnectionService\n    ServiceManager --&gt; JobService\n    ServiceManager --&gt; StreamService\n    ServiceManager --&gt; KVService\n    ServiceManager --&gt; EventService\n\n    %% Client Flow\n    Client -- \"1. Enqueue Job\" --&gt; JobService\n    JobService --&gt; StreamService\n    StreamService --&gt; QueueStream\n\n    %% Worker Flow\n    Worker --&gt; JobService\n    JobService -- \"3. Fetch Job\" --&gt; StreamService\n    StreamService --&gt; QueueStream\n    Worker -- \"4. Execute Function\" --&gt; PythonCode\n    PythonCode -- \"5. Return Result\" --&gt; Worker\n    Worker --&gt; JobService\n    JobService --&gt; KVService\n    KVService -- \"6. Store Result\" --&gt; ResultStore\n\n    %% Scheduler Flow\n    Scheduler --&gt; JobService\n    JobService -- \"7. Check Due Jobs\" --&gt; KVService\n    KVService --&gt; ScheduledJobs\n    Scheduler -- \"8. Enqueue Due Job\" --&gt; JobService\n\n    %% Event Flow\n    Worker -.-&gt; EventService\n    Scheduler -.-&gt; EventService\n    EventService -.-&gt; EventStream\n\n    %% Service to NATS Connection\n    ConnectionService --&gt; QueueStream\n    ConnectionService --&gt; ResultStore\n    ConnectionService --&gt; ScheduledJobs\n    ConnectionService --&gt; EventStream\n\n\n\n\n\n\n\nEnqueueing: The client calls an enqueue function (e.g., enqueue_sync). The function, its arguments, and other metadata are serialized into a job payload. This payload is then published as a message to a NATS subject that corresponds to the target queue.\nPersistence: NATS JetStream receives this message and persists it in a Stream. Each queue in naq maps directly to a JetStream Stream. This ensures that even if no workers are online, the job is safely stored and will be processed later.\nFetching: A naq worker process is constantly listening on the queue‚Äôs Stream. When a new job is available, the worker consumes the message, acknowledging it to NATS so that it isn‚Äôt delivered to another worker.\nExecution: The worker deserializes the job payload and executes the specified Python function with the provided arguments.\nResult Handling: Once the function completes, its return value is serialized. The worker then stores this result in a NATS Key-Value (KV) Store, using the unique job ID as the key. This result has a configurable Time-To-Live (TTL), after which it is automatically purged by NATS.\nScheduled Jobs: The naq scheduler process periodically scans a dedicated KV store for jobs that are due to run. When it finds one, it enqueues it into the appropriate queue, and the job follows the normal lifecycle from there."
  },
  {
    "objectID": "architecture.html#why-nats",
    "href": "architecture.html#why-nats",
    "title": "Architecture Overview",
    "section": "Why NATS?",
    "text": "Why NATS?\nUsing NATS and JetStream as the foundation provides several key advantages:\n\nDecoupling: Clients, workers, and the scheduler are completely decoupled. They only need to know how to talk to NATS, not to each other directly.\nScalability: You can add more workers at any time to increase your processing capacity. NATS handles the load balancing of jobs to available workers automatically.\nResilience: If a worker crashes, JetStream ensures that the job it was processing will be re-delivered to another worker after a timeout. If the entire naq system goes down, the jobs are safe in the NATS stream, ready to be processed when the system comes back online.\nSimplicity: By offloading the complexities of persistence, delivery guarantees, and storage to NATS, the naq codebase can remain focused on the core logic of job execution and scheduling."
  },
  {
    "objectID": "architecture.html#utility-patterns-and-common-code",
    "href": "architecture.html#utility-patterns-and-common-code",
    "title": "Architecture Overview",
    "section": "Utility Patterns and Common Code",
    "text": "Utility Patterns and Common Code\nNAQ implements a comprehensive set of utility patterns to reduce code duplication and improve maintainability across the codebase. These utilities provide standardized approaches to common operations.\n\nCore Utility Modules\nError Handling (utils.error_handling) - Centralized error handling with configurable strategies - Exception wrapping and context management - Error metrics collection and reporting - Automatic error classification and routing\nRetry Patterns (utils.decorators) - Configurable retry decorators with exponential backoff - Support for different backoff strategies (linear, exponential, jitter) - Automatic retry on specific exception types - Performance timing integration\nStructured Logging (utils.logging) - Context-aware structured logging - Operation timing and performance tracking - JSON formatting for log aggregation - Thread-local context management\nAsync Utilities (utils.async_helpers) - Concurrency control with semaphores - Async/sync function conversion utilities - Rate limiting and throttling - Background task management\nSerialization (utils.serialization) - Safe serialization with fallback mechanisms - Metadata-aware serialization - Compression support - Type validation on deserialization\nContext Managers (utils.context_managers) - Resource lifecycle management - Timeout handling - Circuit breaker patterns - Operation error contexts\n\n\nUsage Example\nfrom naq.utils import (\n    retry, timing, log_errors,\n    async_error_handler_context,\n    StructuredLogger,\n    performance_context\n)\n\nclass JobProcessor:\n    def __init__(self):\n        self.logger = StructuredLogger(\"job_processor\")\n        self.error_handler = get_global_error_handler()\n    \n    @retry(max_attempts=3, backoff=\"exponential\")\n    @timing(threshold_ms=1000)\n    @log_errors(reraise=True)\n    async def process_job(self, job_id: str):\n        \"\"\"Process job with retry, timing, and error logging.\"\"\"\n        async with async_error_handler_context(\n            self.error_handler, f\"process_job_{job_id}\"\n        ):\n            async with performance_context(\"job_processing\", self.logger):\n                # Job processing logic here\n                result = await self._execute_job_logic(job_id)\n                return result"
  },
  {
    "objectID": "architecture.html#service-layer-architecture",
    "href": "architecture.html#service-layer-architecture",
    "title": "Architecture Overview",
    "section": "Service Layer Architecture",
    "text": "Service Layer Architecture\nNAQ‚Äôs service layer provides a comprehensive foundation for building reliable, scalable job processing applications. The service architecture follows dependency injection patterns and provides centralized resource management.\n\nService Layer Benefits\nCentralized Connection Management - Single point of NATS connection management with connection pooling - Automatic failover to backup servers - Resource cleanup and lifecycle management - Connection state monitoring and health checks\nConfiguration Integration - Direct integration with YAML configuration system - Type-safe configuration access throughout the application - Environment-specific configuration overrides - Real-time configuration validation\nDependency Injection - Services automatically resolve their dependencies - Lifecycle management with proper initialization and cleanup ordering - Easy testing with service mocking capabilities - Consistent resource management patterns\nType Safety and IDE Support - Full type annotations with dataclass-based configuration - IDE autocompletion for configuration properties - Compile-time validation of configuration access patterns - Structured error handling with detailed validation messages\n\n\nService Architecture Diagram\n\n\n\n\n\ngraph TD\n    subgraph \"Configuration Layer\"\n        YAMLFiles[YAML Configuration Files]\n        EnvVars[Environment Variables]\n        ConfigLoader[Configuration Loader]\n        TypedConfig[Typed Configuration Objects]\n    end\n\n    subgraph \"Service Layer\"\n        ServiceManager[Service Manager&lt;br/&gt;Dependency Injection]\n        BaseService[Base Service&lt;br/&gt;Lifecycle Management]\n    end\n\n    subgraph \"Core Services\"\n        ConnectionService[Connection Service&lt;br/&gt;NATS Management]\n        StreamService[Stream Service&lt;br/&gt;JetStream Operations]\n        KVService[KV Store Service&lt;br/&gt;KeyValue Operations]\n        JobService[Job Service&lt;br/&gt;Job Orchestration]\n        EventService[Event Service&lt;br/&gt;Event Logging]\n        SchedulerService[Scheduler Service&lt;br/&gt;Time-based Jobs]\n    end\n\n    subgraph \"Application Layer\"\n        Workers[Worker Processes]\n        Clients[Client Applications]\n        Scheduler[Scheduler Process]\n        CustomServices[Custom Services]\n    end\n\n    %% Configuration flow\n    YAMLFiles --&gt; ConfigLoader\n    EnvVars --&gt; ConfigLoader\n    ConfigLoader --&gt; TypedConfig\n    \n    %% Service management\n    TypedConfig --&gt; ServiceManager\n    ServiceManager --&gt; BaseService\n    BaseService --&gt; ConnectionService\n    BaseService --&gt; StreamService\n    BaseService --&gt; KVService\n    BaseService --&gt; JobService\n    BaseService --&gt; EventService\n    BaseService --&gt; SchedulerService\n\n    %% Application usage\n    ServiceManager --&gt; Workers\n    ServiceManager --&gt; Clients\n    ServiceManager --&gt; Scheduler\n    ServiceManager --&gt; CustomServices\n\n    %% Service dependencies\n    StreamService -.-&gt; ConnectionService\n    KVService -.-&gt; ConnectionService\n    JobService -.-&gt; StreamService\n    JobService -.-&gt; KVService\n    EventService -.-&gt; StreamService\n    SchedulerService -.-&gt; KVService\n\n    classDef configLayer fill:#e3f2fd\n    classDef serviceLayer fill:#f3e5f5\n    classDef coreServices fill:#e8f5e8\n    classDef appLayer fill:#fff3e0\n\n    class YAMLFiles,EnvVars,ConfigLoader,TypedConfig configLayer\n    class ServiceManager,BaseService serviceLayer\n    class ConnectionService,StreamService,KVService,JobService,EventService,SchedulerService coreServices\n    class Workers,Clients,Scheduler,CustomServices appLayer\n\n\n\n\n\n\n\n\nService Lifecycle Management\nEach service follows a consistent lifecycle pattern:\n\nInitialization: Services are created with typed configuration\nDependency Resolution: ServiceManager resolves service dependencies\nResource Acquisition: Services acquire NATS connections, streams, etc.\nActive Operation: Services handle job processing, scheduling, events\nCleanup: Resources are properly cleaned up when services shut down\n\n# Service lifecycle example\nasync with create_service_manager_from_config(\"./config.yaml\") as services:\n    # Services are automatically initialized with dependencies\n    job_service = await services.get_service(JobService)\n    \n    # Use services with managed resources\n    result = await job_service.execute_job(my_job)\n    \n    # Cleanup is automatic when exiting context\n\n\nConfiguration-Driven Service Creation\nServices are created and configured based on your YAML configuration:\n# config.yaml\nnats:\n  servers: [\"nats://primary:4222\", \"nats://backup:4222\"]\n  max_reconnect_attempts: 5\n\nworkers:\n  concurrency: 20\n  pools:\n    high_priority:\n      concurrency: 10\n      queues: [\"urgent\"]\n\nevents:\n  enabled: true\n  batch_size: 200\n# Services automatically use configuration\nfrom naq.services import create_service_manager_from_config\n\nservice_manager = create_service_manager_from_config(\"./config.yaml\")\n\nasync with service_manager as services:\n    # ConnectionService uses nats.servers configuration\n    conn_service = await services.get_service(ConnectionService)\n    \n    # EventService uses events.enabled and events.batch_size\n    event_service = await services.get_service(EventService)\n    \n    # All services have type-safe access to their configuration\n    nats_servers = event_service.nats_config.servers\n    batch_size = event_service.events_config.batch_size"
  },
  {
    "objectID": "architecture.html#event-driven-state-management-architecture",
    "href": "architecture.html#event-driven-state-management-architecture",
    "title": "Architecture Overview",
    "section": "Event-Driven State Management Architecture",
    "text": "Event-Driven State Management Architecture\nNAQ implements comprehensive event-driven state management where every state transition is captured as a structured event and stored in NATS JetStream streams. This provides complete observability and enables powerful monitoring, debugging, and analytics capabilities.\n\nEvent Flow Architecture\n\n\n\n\n\ngraph TD\n    subgraph \"NAQ Components\"\n        Worker[Worker Process]\n        Queue[Queue Client]\n        Scheduler[Scheduler Process]\n    end\n\n    subgraph \"NATS JetStream - Job Processing\"\n        JobStream[Job Queue Stream&lt;br/&gt;naq_jobs]\n        ResultKV[Results KV Store&lt;br/&gt;naq_results]\n        ScheduleKV[Scheduled Jobs KV&lt;br/&gt;naq_scheduled_jobs]\n        WorkerKV[Worker Status KV&lt;br/&gt;naq_workers]\n    end\n\n    subgraph \"NATS JetStream - Event Logging\"\n        EventStream[Event Stream&lt;br/&gt;NAQ_JOB_EVENTS]\n        EventStorage[(Durable Event Storage)]\n    end\n\n    subgraph \"Event Consumers\"\n        Monitor[Real-time Monitor&lt;br/&gt;naq events]\n        Analytics[Analytics Engine&lt;br/&gt;naq event-stats]\n        History[Historical Queries&lt;br/&gt;naq event-history]\n        Custom[Custom Event Handlers]\n    end\n\n    %% Job Processing Flow\n    Queue --&gt; JobStream\n    JobStream --&gt; Worker\n    Worker --&gt; ResultKV\n    Queue --&gt; ScheduleKV\n    Scheduler --&gt; ScheduleKV\n    Scheduler --&gt; JobStream\n    Worker --&gt; WorkerKV\n\n    %% Event Logging Flow\n    Worker -.-&gt;|Job Events| EventStream\n    Queue -.-&gt;|Enqueue Events| EventStream\n    Scheduler -.-&gt;|Schedule Events| EventStream\n    Worker -.-&gt;|Worker Events| EventStream\n\n    %% Event Storage & Consumption\n    EventStream --&gt; EventStorage\n    EventStorage --&gt; Monitor\n    EventStorage --&gt; Analytics  \n    EventStorage --&gt; History\n    EventStorage --&gt; Custom\n\n    %% Styling\n    classDef naqComponent fill:#e1f5fe\n    classDef natsStorage fill:#f3e5f5\n    classDef eventSystem fill:#e8f5e8\n    classDef consumers fill:#fff3e0\n\n    class Worker,Queue,Scheduler naqComponent\n    class JobStream,ResultKV,ScheduleKV,WorkerKV natsStorage\n    class EventStream,EventStorage eventSystem\n    class Monitor,Analytics,History,Custom consumers\n\n\n\n\n\n\n\n\nState Management Layers\nNAQ‚Äôs state management operates on multiple layers:\n1. Operational State (KV Stores) - Current state of jobs, workers, and schedules - Fast lookups for status queries - Optimized for real-time operations\n2. Event Stream (Audit Trail) - Complete history of all state transitions - Immutable event log for debugging and compliance - Time-ordered event sequence for analysis\n3. Derived State (Analytics) - Aggregated metrics computed from event streams - System health indicators and performance stats - Trend analysis and capacity planning data\n\n\nEvent Types and Sources\n\n\n\n\n\ngraph LR\n    subgraph \"Event Sources\"\n        W[Worker]\n        Q[Queue] \n        S[Scheduler]\n    end\n\n    subgraph \"Event Categories\"\n        JE[Job Lifecycle Events]\n        WE[Worker Status Events]\n        SE[Schedule Management Events]\n    end\n\n    subgraph \"Event Stream\"\n        ES[NAQ_JOB_EVENTS&lt;br/&gt;JetStream]\n    end\n\n    W --&gt; JE\n    W --&gt; WE\n    Q --&gt; JE\n    Q --&gt; SE\n    S --&gt; JE\n    S --&gt; SE\n\n    JE --&gt; ES\n    WE --&gt; ES\n    SE --&gt; ES\n\n\n\n\n\n\nJob Lifecycle Events: - ENQUEUED, STARTED, COMPLETED, FAILED, RETRY_SCHEDULED - SCHEDULED, SCHEDULE_TRIGGERED, CANCELLED\nWorker Status Events: - WORKER_STARTED, WORKER_STOPPED, WORKER_IDLE, WORKER_BUSY - WORKER_HEARTBEAT, WORKER_ERROR\nSchedule Management Events: - SCHEDULE_PAUSED, SCHEDULE_RESUMED, SCHEDULE_CANCELLED - SCHEDULE_MODIFIED\n\n\nBenefits of Event-Driven Architecture\nComplete Observability - Every system operation is logged and queryable - Real-time monitoring of all components - Historical analysis for debugging and optimization\nReactive Capabilities - Build applications that respond to job lifecycle events - Implement custom business logic triggered by state changes - Create monitoring and alerting systems\nDebugging & Analytics - Trace job execution across the entire system - Analyze performance patterns and bottlenecks - Monitor worker health and capacity utilization\nCompliance & Auditing - Immutable audit trail of all operations - Meet compliance requirements for job processing - Track system usage and resource consumption"
  },
  {
    "objectID": "api/events.html",
    "href": "api/events.html",
    "title": "events Module",
    "section": "",
    "text": "The naq.events module provides comprehensive job event logging and monitoring capabilities. This module enables real-time tracking of job lifecycle events, programmatic event processing, and integration with monitoring systems."
  },
  {
    "objectID": "api/events.html#module-overview",
    "href": "api/events.html#module-overview",
    "title": "events Module",
    "section": "Module Overview",
    "text": "Module Overview\nfrom naq import events\n\n# Core event types\nfrom naq.events import JobEvent, JobEventType, WorkerEvent, WorkerEventType\n\n# Event logging\nfrom naq.events import AsyncJobEventLogger, JobEventLogger\n\n# Event processing\nfrom naq.events import AsyncJobEventProcessor\n\n# Storage backends\nfrom naq.events import NATSJobEventStorage, BaseEventStorage"
  },
  {
    "objectID": "api/events.html#core-data-types",
    "href": "api/events.html#core-data-types",
    "title": "events Module",
    "section": "Core Data Types",
    "text": "Core Data Types\n\nJobEventType\nEnumeration of all possible job lifecycle events.\nclass JobEventType(str, Enum):\n    # Core job events\n    ENQUEUED = \"enqueued\"           # Job added to queue\n    STARTED = \"started\"             # Job processing began\n    COMPLETED = \"completed\"         # Job completed successfully\n    FAILED = \"failed\"               # Job failed with error\n    RETRY_SCHEDULED = \"retry_scheduled\"  # Job retry scheduled\n    CANCELLED = \"cancelled\"         # Job cancelled\n    PAUSED = \"paused\"              # Job paused\n    RESUMED = \"resumed\"            # Job resumed\n    \n    # Schedule events\n    SCHEDULED = \"scheduled\"         # Job scheduled for future execution\n    SCHEDULE_TRIGGERED = \"schedule_triggered\"  # Scheduled job enqueued\n    \n    # Schedule management events (NEW)\n    SCHEDULE_PAUSED = \"schedule_paused\"      # Scheduled job paused\n    SCHEDULE_RESUMED = \"schedule_resumed\"    # Scheduled job resumed\n    SCHEDULE_CANCELLED = \"schedule_cancelled\" # Scheduled job cancelled\n    SCHEDULE_MODIFIED = \"schedule_modified\"   # Scheduled job parameters modified\n\n\nWorkerEventType (NEW)\nEnumeration of worker lifecycle events.\nclass WorkerEventType(str, Enum):\n    WORKER_STARTED = \"worker_started\"     # Worker process started\n    WORKER_STOPPED = \"worker_stopped\"     # Worker process stopped\n    WORKER_IDLE = \"worker_idle\"          # Worker became idle\n    WORKER_BUSY = \"worker_busy\"          # Worker became busy\n    WORKER_HEARTBEAT = \"worker_heartbeat\" # Worker heartbeat (periodic)\n    WORKER_ERROR = \"worker_error\"        # Worker encountered error\nfrom naq.events import JobEventType\n\nclass JobEventType(str, Enum):\n    ENQUEUED = \"enqueued\"\n    STARTED = \"started\"\n    COMPLETED = \"completed\"\n    FAILED = \"failed\"\n    RETRY_SCHEDULED = \"retry_scheduled\"\n    SCHEDULED = \"scheduled\"\n    SCHEDULE_TRIGGERED = \"schedule_triggered\"\nValues:\n\nENQUEUED: Job added to queue\nSTARTED: Job execution began\nCOMPLETED: Job finished successfully\nFAILED: Job execution failed\nRETRY_SCHEDULED: Failed job scheduled for retry\nSCHEDULED: Job scheduled for future execution\nSCHEDULE_TRIGGERED: Scheduled job moved to active queue\n\n\n\nJobEvent\nRepresents a single job lifecycle event with complete metadata.\nfrom naq.events import JobEvent\n\nclass JobEvent(msgspec.Struct):\n    job_id: str\n    event_type: JobEventType\n    timestamp: float\n    worker_id: Optional[str] = None\n    queue_name: Optional[str] = None\n    message: Optional[str] = None\n    details: Optional[Dict[str, Any]] = None\n    error_type: Optional[str] = None\n    error_message: Optional[str] = None\n    duration_ms: Optional[int] = None\n    nats_subject: Optional[str] = None\n    nats_sequence: Optional[int] = None\nKey Attributes:\n\njob_id: Unique identifier for the job\nevent_type: Type of event (see JobEventType)\ntimestamp: Unix timestamp when event occurred\nworker_id: ID of worker processing the job (if applicable)\nqueue_name: Name of the queue containing the job\nmessage: Human-readable event description\nduration_ms: Job execution time in milliseconds (for completion/failure events)\nerror_type: Exception class name (for failure events)\nerror_message: Error description (for failure events)\n\nClass Methods:\n\nJobEvent.enqueued()\n@classmethod\ndef enqueued(\n    cls,\n    job_id: str,\n    queue_name: str,\n    message: Optional[str] = None,\n    **kwargs\n) -&gt; \"JobEvent\"\nCreate an ENQUEUED event.\nExample:\nevent = JobEvent.enqueued(\"job-123\", \"high-priority\")\n\n\nJobEvent.started()\n@classmethod\ndef started(\n    cls,\n    job_id: str,\n    worker_id: str,\n    queue_name: str,\n    message: Optional[str] = None,\n    **kwargs\n) -&gt; \"JobEvent\"\nCreate a STARTED event.\n\n\nJobEvent.completed()\n@classmethod\ndef completed(\n    cls,\n    job_id: str,\n    worker_id: str,\n    queue_name: str,\n    duration_ms: int,\n    message: Optional[str] = None,\n    **kwargs\n) -&gt; \"JobEvent\"\nCreate a COMPLETED event.\n\n\nJobEvent.failed()\n@classmethod\ndef failed(\n    cls,\n    job_id: str,\n    worker_id: str,\n    queue_name: str,\n    error_type: str,\n    error_message: str,\n    duration_ms: int,\n    message: Optional[str] = None,\n    **kwargs\n) -&gt; \"JobEvent\"\nCreate a FAILED event.\nInstance Methods:\n\n\nto_dict()\ndef to_dict(self) -&gt; Dict[str, Any]\nConvert event to dictionary representation.\nExample:\nevent = JobEvent.enqueued(\"job-123\", \"default\")\ndata = event.to_dict()\nprint(data[\"event_type\"])  # \"enqueued\""
  },
  {
    "objectID": "api/events.html#event-logging",
    "href": "api/events.html#event-logging",
    "title": "events Module",
    "section": "Event Logging",
    "text": "Event Logging\n\nAsyncJobEventLogger\nHigh-performance, non-blocking event logger with automatic batching and background flushing.\nfrom naq.events import AsyncJobEventLogger\n\nclass AsyncJobEventLogger:\n    def __init__(\n        self,\n        storage: Optional[BaseEventStorage] = None,\n        batch_size: int = 100,\n        flush_interval: float = 5.0,\n        max_buffer_size: int = 10000,\n        nats_url: str = DEFAULT_NATS_URL\n    )\nParameters:\n\nstorage: Storage backend (defaults to NATSJobEventStorage)\nbatch_size: Events to buffer before automatic flush\nflush_interval: Seconds between automatic flushes\nmax_buffer_size: Maximum events to buffer (prevents memory issues)\nnats_url: NATS server URL\n\nMethods:\n\nstart()\nasync def start(self) -&gt; None\nStart the logger and background flush loop.\n\n\nstop()\nasync def stop(self) -&gt; None\nStop the logger and flush remaining events.\n\n\nlog_event()\nasync def log_event(self, event: JobEvent) -&gt; None\nLog a job event (non-blocking).\nConvenience Methods:\n# Log specific event types\nasync def log_job_enqueued(self, job_id: str, queue_name: str, **kwargs) -&gt; None\nasync def log_job_started(self, job_id: str, worker_id: str, queue_name: str, **kwargs) -&gt; None\nasync def log_job_completed(self, job_id: str, worker_id: str, queue_name: str, duration_ms: int, **kwargs) -&gt; None\nasync def log_job_failed(self, job_id: str, worker_id: str, queue_name: str, error_type: str, error_message: str, duration_ms: int, **kwargs) -&gt; None\nasync def log_job_retry_scheduled(self, job_id: str, worker_id: str, queue_name: str, retry_count: int, retry_delay: float, **kwargs) -&gt; None\nasync def log_job_scheduled(self, job_id: str, queue_name: str, scheduled_timestamp: float, **kwargs) -&gt; None\nasync def log_schedule_triggered(self, job_id: str, queue_name: str, **kwargs) -&gt; None\nExample:\nimport asyncio\nfrom naq.events import AsyncJobEventLogger\n\nasync def main():\n    logger = AsyncJobEventLogger(batch_size=50, flush_interval=2.0)\n    \n    await logger.start()\n    \n    # Log events\n    await logger.log_job_enqueued(\"job-123\", \"high-priority\")\n    await logger.log_job_started(\"job-123\", \"worker-1\", \"high-priority\")\n    await logger.log_job_completed(\"job-123\", \"worker-1\", \"high-priority\", 2500)\n    \n    await logger.stop()\n\nasyncio.run(main())\n\n\n\nJobEventLogger\nSynchronous wrapper for AsyncJobEventLogger, following existing NAQ patterns.\nfrom naq.events import JobEventLogger\n\nclass JobEventLogger:\n    def __init__(\n        self,\n        storage: Optional[BaseEventStorage] = None,\n        batch_size: int = 100,\n        flush_interval: float = 5.0,\n        max_buffer_size: int = 10000,\n        nats_url: str = DEFAULT_NATS_URL\n    )\nMethods:\nAll methods mirror the async version but run synchronously:\ndef start(self) -&gt; None\ndef stop(self) -&gt; None\ndef log_event(self, event: JobEvent) -&gt; None\ndef log_job_enqueued(self, job_id: str, queue_name: str, **kwargs) -&gt; None\n# ... etc\nExample:\nfrom naq.events import JobEventLogger, JobEvent\n\nlogger = JobEventLogger()\nlogger.start()\n\nevent = JobEvent.enqueued(\"job-456\", \"default\")\nlogger.log_event(event)\n\nlogger.stop()"
  },
  {
    "objectID": "api/events.html#event-processing",
    "href": "api/events.html#event-processing",
    "title": "events Module",
    "section": "Event Processing",
    "text": "Event Processing\n\nAsyncJobEventProcessor\nReal-time event processor for building reactive systems and monitoring applications.\nfrom naq.events import AsyncJobEventProcessor\n\nclass AsyncJobEventProcessor:\n    def __init__(\n        self,\n        storage: Optional[BaseEventStorage] = None,\n        nats_url: str = DEFAULT_NATS_URL\n    )\nMethods:\n\nHandler Registration\ndef add_handler(self, event_type: JobEventType, handler: Callable) -&gt; None\ndef add_global_handler(self, handler: Callable) -&gt; None\ndef remove_handler(self, event_type: JobEventType, handler: Callable) -&gt; bool\ndef remove_global_handler(self, handler: Callable) -&gt; bool\nRegister handlers for specific event types or all events.\nExample:\nprocessor = AsyncJobEventProcessor()\n\ndef handle_failure(event):\n    print(f\"Job {event.job_id} failed: {event.error_message}\")\n\nprocessor.add_handler(JobEventType.FAILED, handle_failure)\n\n\nLifecycle Management\nasync def start(self) -&gt; None\nasync def stop(self) -&gt; None\n\n\nEvent Streaming\nasync def get_job_events(self, job_id: str) -&gt; List[JobEvent]\nasync def stream_job_events(\n    self,\n    job_id: Optional[str] = None,\n    event_type: Optional[JobEventType] = None,\n    queue_name: Optional[str] = None,\n    worker_id: Optional[str] = None\n)\nGet historical events or stream new events with filtering.\n\n\nConvenience Methods\ndef on_job_enqueued(self, handler: Callable) -&gt; None\ndef on_job_started(self, handler: Callable) -&gt; None\ndef on_job_completed(self, handler: Callable) -&gt; None\ndef on_job_failed(self, handler: Callable) -&gt; None\ndef on_job_retry_scheduled(self, handler: Callable) -&gt; None\ndef on_job_scheduled(self, handler: Callable) -&gt; None\ndef on_schedule_triggered(self, handler: Callable) -&gt; None\ndef on_all_events(self, handler: Callable) -&gt; None\nComplete Example:\nimport asyncio\nfrom naq.events import AsyncJobEventProcessor, JobEventType\n\nasync def monitoring_system():\n    processor = AsyncJobEventProcessor()\n    \n    # Statistics tracking\n    stats = {\"completed\": 0, \"failed\": 0}\n    \n    def update_stats(event):\n        if event.event_type == JobEventType.COMPLETED:\n            stats[\"completed\"] += 1\n        elif event.event_type == JobEventType.FAILED:\n            stats[\"failed\"] += 1\n        \n        # Print stats every 10 jobs\n        total = stats[\"completed\"] + stats[\"failed\"]\n        if total % 10 == 0:\n            print(f\"Stats: {stats['completed']} completed, {stats['failed']} failed\")\n    \n    # Specific failure handler\n    def handle_failure(event):\n        print(f\"üö® FAILURE: Job {event.job_id} - {event.error_message}\")\n    \n    # Register handlers\n    processor.add_global_handler(update_stats)\n    processor.on_job_failed(handle_failure)\n    \n    # Start processing\n    await processor.start()\n    \n    try:\n        # Keep running\n        await asyncio.sleep(3600)  # 1 hour\n    finally:\n        await processor.stop()\n\nasyncio.run(monitoring_system())"
  },
  {
    "objectID": "api/events.html#storage-backends",
    "href": "api/events.html#storage-backends",
    "title": "events Module",
    "section": "Storage Backends",
    "text": "Storage Backends\n\nBaseEventStorage\nAbstract interface for event storage backends.\nfrom abc import ABC, abstractmethod\nfrom naq.events import BaseEventStorage\n\nclass BaseEventStorage(ABC):\n    @abstractmethod\n    async def store_event(self, event: JobEvent) -&gt; None\n    \n    @abstractmethod\n    async def get_events(self, job_id: str) -&gt; List[JobEvent]\n    \n    @abstractmethod\n    async def stream_events(\n        self,\n        job_id: Optional[str] = None,\n        event_type: Optional[JobEventType] = None,\n        queue_name: Optional[str] = None,\n        worker_id: Optional[str] = None\n    ) -&gt; AsyncIterator[JobEvent]\n    \n    @abstractmethod\n    async def close(self) -&gt; None\n\n\nNATSJobEventStorage\nNATS JetStream-based event storage implementation.\nfrom naq.events import NATSJobEventStorage\n\nclass NATSJobEventStorage(BaseEventStorage):\n    def __init__(\n        self,\n        nats_url: str = DEFAULT_NATS_URL,\n        stream_name: str = \"NAQ_JOB_EVENTS\",\n        subject_prefix: str = \"naq.jobs.events\"\n    )\nParameters:\n\nnats_url: NATS server URL\nstream_name: JetStream stream name\nsubject_prefix: Base subject for event routing\n\nFeatures:\n\nDurable event storage using NATS JetStream\nOrdered delivery guarantees\nSubject-based filtering for efficient querying\nAutomatic stream creation and management\nConfigurable retention policies\n\nSubject Structure:\nEvents are stored with subjects following this pattern:\nnaq.jobs.events.{job_id}.{context}.{event_type}\nExamples: - naq.jobs.events.abc123.worker.worker-1.started - naq.jobs.events.def456.queue.high-priority.enqueued\nExample:\nfrom naq.events import NATSJobEventStorage, JobEvent\n\nasync def custom_storage_example():\n    storage = NATSJobEventStorage(\n        nats_url=\"nats://production-nats:4222\",\n        stream_name=\"PROD_JOB_EVENTS\",\n        subject_prefix=\"myapp.jobs.events\"\n    )\n    \n    # Store an event\n    event = JobEvent.completed(\"job-789\", \"worker-2\", \"urgent\", 1500)\n    await storage.store_event(event)\n    \n    # Get all events for a job\n    events = await storage.get_events(\"job-789\")\n    \n    # Stream new events with filtering\n    async for event in storage.stream_events(queue_name=\"urgent\"):\n        print(f\"Urgent queue event: {event.event_type}\")\n    \n    await storage.close()"
  },
  {
    "objectID": "api/events.html#configuration",
    "href": "api/events.html#configuration",
    "title": "events Module",
    "section": "Configuration",
    "text": "Configuration\nThe events module respects several environment variables for configuration:\nimport os\n\n# Enable/disable event logging\nNAQ_EVENTS_ENABLED = os.getenv(\"NAQ_EVENTS_ENABLED\", \"true\")\n\n# Storage configuration\nNAQ_EVENT_STORAGE_URL = os.getenv(\"NAQ_EVENT_STORAGE_URL\", DEFAULT_NATS_URL)\nNAQ_EVENT_STREAM_NAME = os.getenv(\"NAQ_EVENT_STREAM_NAME\", \"NAQ_JOB_EVENTS\")\nNAQ_EVENT_SUBJECT_PREFIX = os.getenv(\"NAQ_EVENT_SUBJECT_PREFIX\", \"naq.jobs.events\")\n\n# Performance tuning\nNAQ_EVENT_LOGGER_BATCH_SIZE = int(os.getenv(\"NAQ_EVENT_LOGGER_BATCH_SIZE\", \"100\"))\nNAQ_EVENT_LOGGER_FLUSH_INTERVAL = float(os.getenv(\"NAQ_EVENT_LOGGER_FLUSH_INTERVAL\", \"5.0\"))\nNAQ_EVENT_LOGGER_MAX_BUFFER_SIZE = int(os.getenv(\"NAQ_EVENT_LOGGER_MAX_BUFFER_SIZE\", \"10000\"))"
  },
  {
    "objectID": "api/events.html#error-handling",
    "href": "api/events.html#error-handling",
    "title": "events Module",
    "section": "Error Handling",
    "text": "Error Handling\nThe events module is designed to be resilient and never interfere with job processing:\n\nNon-blocking Operations: Event logging never blocks job execution\nAutomatic Retries: Failed event storage operations are retried\nGraceful Degradation: If event storage fails, jobs continue processing\nMemory Management: Buffer limits prevent memory exhaustion\nConnection Recovery: Automatic reconnection to NATS on connection loss"
  },
  {
    "objectID": "api/events.html#performance-considerations",
    "href": "api/events.html#performance-considerations",
    "title": "events Module",
    "section": "Performance Considerations",
    "text": "Performance Considerations\n\nBatching: Events are batched for efficient storage\nBackground Processing: All I/O happens in background tasks\nMemory Efficient: Configurable buffer limits and automatic cleanup\nNetwork Optimized: Minimal serialization overhead with msgspec\nFiltering: Subject-based filtering reduces network traffic"
  },
  {
    "objectID": "api/events.html#thread-safety",
    "href": "api/events.html#thread-safety",
    "title": "events Module",
    "section": "Thread Safety",
    "text": "Thread Safety\n\nAsyncJobEventLogger: Thread-safe when used properly with async/await\nJobEventLogger: Thread-safe for synchronous usage\nAsyncJobEventProcessor: Designed for single asyncio event loop\nAll operations use proper async synchronization primitives"
  },
  {
    "objectID": "api/exceptions.html",
    "href": "api/exceptions.html",
    "title": "Exceptions",
    "section": "",
    "text": "The naq library uses a set of custom exceptions to indicate specific error conditions. All custom exceptions inherit from the base NaqException.\n\nNaqException\nThe base exception for all errors raised by naq.\n\n\nNaqConnectionError\nRaised when there is an issue connecting to the NATS server or a problem with the connection during an operation.\n\n\nConfigurationError\nRaised when there is an issue with the configuration of naq, such as providing invalid parameters to a Queue or Worker.\n\n\nSerializationError\nRaised if naq fails to serialize or deserialize a job. This can happen if you are using the json serializer and try to enqueue a job with un-serializable arguments (like complex objects).\n\n\nJobExecutionError\nRaised by Job.fetch_result() if the job failed during execution on the worker. The exception message will contain the original error and traceback from the worker.\n\n\nJobNotFoundError\nRaised by Job.fetch_result() if the result for the specified job_id cannot be found. This could be because the job has not completed yet, the result has expired and been cleaned up, or the job never existed."
  },
  {
    "objectID": "api/worker.html",
    "href": "api/worker.html",
    "title": "Worker API",
    "section": "",
    "text": "The worker module contains the Worker class, which is responsible for fetching and executing jobs from one or more queues."
  },
  {
    "objectID": "api/worker.html#naq.worker.worker",
    "href": "api/worker.html#naq.worker.worker",
    "title": "Worker API",
    "section": "naq.worker.Worker",
    "text": "naq.worker.Worker\nYou can start a worker from the command line using naq worker, but you can also create and run a Worker instance programmatically.\n\nnaq.worker.Worker(queues, nats_url, concurrency, worker_name, ...)\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\nqueues\nlist[str] | str\nA list of queue names to listen to. Defaults to the single naq_default_queue.\n\n\nnats_url\nstr\nThe URL of the NATS server.\n\n\nconcurrency\nint\nThe maximum number of jobs to process concurrently. Defaults to 10.\n\n\nworker_name\nstr | None\nA name for the worker, used for creating durable consumer names. A unique ID is generated if not provided.\n\n\nheartbeat_interval\nint\nThe interval (in seconds) at which the worker sends a heartbeat. Defaults to 15.\n\n\nworker_ttl\nint\nThe time-to-live (in seconds) for the worker‚Äôs heartbeat. Defaults to 60.\n\n\nack_wait\nint | dict | None\nThe time (in seconds) the worker has to acknowledge a job before it‚Äôs re-delivered. Can be a global value or a per-queue dictionary.\n\n\nmodule_paths\nlist[str] | str | None\nA list of paths to add to sys.path to help the worker find your task modules.\n\n\n\n\n\nMethods\n\nrun()\nStarts the worker‚Äôs main processing loop. This is an async method.\nasync def run(self) -&gt; None\nThe worker will connect to NATS, subscribe to the specified queues, and start fetching jobs. It will run until a shutdown signal is received.\n\n\nrun_sync()\nA synchronous method to start the worker. This is useful when running a worker from a synchronous script.\ndef run_sync(self) -&gt; None\nThis method will block until the worker is shut down.\n\n\nlist_workers()\nA static method to list all active workers.\n@staticmethod\nasync def list_workers(nats_url: str = DEFAULT_NATS_URL) -&gt; list[dict]\nReturns a list of dictionaries, where each dictionary contains information about a worker (ID, status, hostname, etc.).\n\n\nlist_workers_sync()\nA synchronous version of list_workers()."
  },
  {
    "objectID": "installation.html",
    "href": "installation.html",
    "title": "Installation",
    "section": "",
    "text": "You can install naq directly from PyPI using pip. A Python version of 3.12 or higher is required.\npip install naq\n\n\nFor faster installation, you can also use modern package managers like uv or pixi:\n# Using uv\nuv pip install naq\n\n# Using pixi\npixi add naq\n\n\n\nnaq includes several optional feature sets that you can install as needed:\nDashboard - Web-based monitoring dashboard:\npip install naq[dashboard]\nYAML Configuration - Enhanced configuration support (recommended):\npip install naq[yaml]\n# or install specific dependencies:\npip install pyyaml jsonschema\nComplete Installation - All optional features:\npip install naq[dashboard,yaml]"
  },
  {
    "objectID": "installation.html#installing-naq",
    "href": "installation.html#installing-naq",
    "title": "Installation",
    "section": "",
    "text": "You can install naq directly from PyPI using pip. A Python version of 3.12 or higher is required.\npip install naq\n\n\nFor faster installation, you can also use modern package managers like uv or pixi:\n# Using uv\nuv pip install naq\n\n# Using pixi\npixi add naq\n\n\n\nnaq includes several optional feature sets that you can install as needed:\nDashboard - Web-based monitoring dashboard:\npip install naq[dashboard]\nYAML Configuration - Enhanced configuration support (recommended):\npip install naq[yaml]\n# or install specific dependencies:\npip install pyyaml jsonschema\nComplete Installation - All optional features:\npip install naq[dashboard,yaml]"
  },
  {
    "objectID": "installation.html#setting-up-nats",
    "href": "installation.html#setting-up-nats",
    "title": "Installation",
    "section": "Setting Up NATS",
    "text": "Setting Up NATS\nnaq requires a running NATS server with JetStream enabled to function. JetStream provides the persistence layer for jobs and results.\n\nUsing Docker (Recommended)\nThe easiest way to get a NATS server running for development is by using the provided Docker Compose file.\n\nNavigate to the docker directory in the project root: bash     cd /path/to/naq/docker\nStart the NATS server in detached mode: bash     docker-compose up -d\n\nThis will start a NATS server on localhost:4222 with JetStream enabled and ready to use.\n\n\nManual Setup\nIf you prefer to run a NATS server manually, ensure that you start it with the -js flag to enable JetStream:\nnats-server -js\nRefer to the official NATS documentation for detailed installation instructions for your operating system."
  },
  {
    "objectID": "installation.html#initial-configuration-setup",
    "href": "installation.html#initial-configuration-setup",
    "title": "Installation",
    "section": "Initial Configuration Setup",
    "text": "Initial Configuration Setup\n\nQuick Start Configuration\nAfter installation, set up your initial configuration using the CLI:\n# Create a development configuration\nnaq system config-init --environment development\n\n# Or create a production configuration\nnaq system config-init --environment production\nThis creates a naq.yaml file in your current directory with appropriate defaults for your environment.\n\n\nVerify Configuration\n# Check that your configuration is valid\nnaq system config --validate\n\n# View your current configuration\nnaq system config --show\n\n# See which configuration files are being used\nnaq system config --sources\n\n\nEnvironment Variables (Alternative)\nIf you prefer environment variables over YAML configuration:\n# Basic NATS connection\nexport NAQ_NATS_URL=nats://localhost:4222\n\n# Worker settings\nexport NAQ_WORKER_CONCURRENCY=10\nexport NAQ_LOG_LEVEL=INFO\n\n# Enable comprehensive event logging\nexport NAQ_EVENTS_ENABLED=true\n\n\nConfiguration File Locations\nNAQ searches for configuration files in the following order:\n\nCurrent directory: ./naq.yaml or ./naq.yml\nUser config: ~/.naq/config.yaml\nSystem config: /etc/naq/config.yaml\nEnvironment variables: NAQ_* variables\n\nFor detailed configuration options, see the Configuration Guide."
  },
  {
    "objectID": "installation.html#troubleshooting",
    "href": "installation.html#troubleshooting",
    "title": "Installation",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\n\n\n\n\n\nNote\n\n\n\nConnection Issues?\n\nEnsure your NATS server is running and accessible from where you are running your application and workers.\nVerify that JetStream is enabled. You can check the server logs for a line confirming ‚ÄúJetStream is enabled.‚Äù\nBy default, naq attempts to connect to nats://localhost:4222. If your server is elsewhere, set the NAQ_NATS_URL environment variable or update your YAML configuration.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nConfiguration Issues?\n\nUse naq system config --validate to check for configuration errors\nCheck configuration file locations with naq system config --sources\nVerify YAML syntax: python -c \"import yaml; yaml.safe_load(open('naq.yaml'))\"\nEnvironment variables take precedence over YAML configuration files\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nMissing Dependencies?\n\nFor YAML configuration: pip install pyyaml jsonschema\nFor validation features: pip install jsonschema\nFor dashboard: pip install naq[dashboard]"
  }
]