[
  {
    "objectID": "quickstart.html",
    "href": "quickstart.html",
    "title": "Quickstart Guide",
    "section": "",
    "text": "This guide will walk you through the basics of setting up a task, enqueuing it, and running a worker to process it."
  },
  {
    "objectID": "quickstart.html#define-a-task",
    "href": "quickstart.html#define-a-task",
    "title": "Quickstart Guide",
    "section": "1. Define a Task",
    "text": "1. Define a Task\nFirst, create a Python file to define the function you want to run in the background. Let’s call this file tasks.py.\nThis function can be any regular Python function. For this example, we’ll create a simple function that simulates some work and counts the words in a given text.\n# tasks.py\nimport time\nimport random\n\ndef count_words(text):\n    \"\"\"\n    A simple function that counts the words in a string.\n    \"\"\"\n    print(f\"Processing text: '{text[:30]}...'\")\n    # Simulate some I/O or CPU-bound work\n    time.sleep(random.randint(1, 3))\n    word_count = len(text.split())\n    print(f\"Found {word_count} words.\")\n    return word_count"
  },
  {
    "objectID": "quickstart.html#enqueue-the-job",
    "href": "quickstart.html#enqueue-the-job",
    "title": "Quickstart Guide",
    "section": "2. Enqueue the Job",
    "text": "2. Enqueue the Job\nNow, let’s enqueue the count_words function to be executed by a worker. Create another file, main.py, to send the job to the queue.\nWe’ll use the enqueue_sync function, which is a simple, blocking way to add a job to the queue.\n# main.py\nfrom naq import enqueue_sync\nfrom tasks import count_words\n\n# The text we want to process\nlong_text = (\n    \"A journey of a thousand miles begins with a single step. \"\n    \"The best time to plant a tree was 20 years ago. \"\n    \"The second best time is now.\"\n)\n\nprint(\"Enqueuing job to count words...\")\n\n# Enqueue the function `count_words` with `long_text` as its argument\njob = enqueue_sync(count_words, long_text)\n\nprint(f\"Successfully enqueued job {job.job_id}.\")\nprint(\"To process the job, run a worker with: naq worker default\")"
  },
  {
    "objectID": "quickstart.html#run-the-worker",
    "href": "quickstart.html#run-the-worker",
    "title": "Quickstart Guide",
    "section": "3. Run the Worker",
    "text": "3. Run the Worker\nWith the job enqueued, the final step is to start a worker process. The worker will connect to NATS, fetch the job from the queue, and execute the count_words function.\nOpen your terminal and run the following command:\nnaq worker default\nThe default argument tells the worker to listen to the default queue, which is where enqueue_sync sends jobs.\nYou should see output similar to this in your worker’s terminal:\n14:30:15.123 INFO     Worker listening on queue: naq_default_queue\nProcessing text: 'A journey of a thousand miles...'\nFound 25 words.\n14:30:18.245 INFO     Job 1a2b3c4d completed. Result: 25\nCongratulations! You’ve successfully enqueued and processed your first background job with naq."
  },
  {
    "objectID": "quickstart.html#connection-management",
    "href": "quickstart.html#connection-management",
    "title": "Quickstart Guide",
    "section": "Connection Management",
    "text": "Connection Management\n\nUsing Context Managers (Recommended)\nThe recommended way to manage NATS connections in naq is through context managers. They provide automatic resource management and ensure proper cleanup.\nimport asyncio\nfrom naq.connection import nats_connection, nats_jetstream, nats_kv_store\n\nasync def basic_example():\n    # Simple connection\n    async with nats_connection() as conn:\n        await conn.publish(\"hello\", b\"world\")\n    \n    # With JetStream\n    async with nats_jetstream() as (conn, js):\n        stream = await js.add_stream(name=\"mystream\", subjects=[\"mystream.*\"])\n    \n    # With KeyValue store\n    async with nats_kv_store(\"mybucket\") as kv:\n        await kv.put(\"key\", \"value\")\n\n\nUsing Decorators\nFor functions that need NATS connections, you can use decorators to automatically inject connections:\nfrom naq.connection import with_nats_connection, with_jetstream_context\n\n@with_nats_connection()\nasync def publish(conn, subject: str, data: bytes):\n    await conn.publish(subject, data)\n\n@with_jetstream_context()\nasync def create_stream(js, name: str):\n    await js.add_stream(name=name, subjects=[f\"{name}.*\"])\n\n\nConnection Testing and Monitoring\nIn production environments, you can test and monitor your connections:\nfrom naq.connection import test_nats_connection, wait_for_nats_connection, connection_monitor\n\n# Test if connection is available\nis_healthy = await test_nats_connection()\n\n# Wait for connection to be ready\nawait wait_for_nats_connection(timeout=30)\n\n# Monitor connection usage\nprint(f\"Active connections: {connection_monitor.metrics.active_connections}\")"
  },
  {
    "objectID": "quickstart.html#whats-next",
    "href": "quickstart.html#whats-next",
    "title": "Quickstart Guide",
    "section": "What’s Next?",
    "text": "What’s Next?\n\nExplore how to schedule jobs to run in the future.\nLearn about the architecture of naq.\nCheck out more complex examples.\nRead the Connection Management API documentation for advanced usage."
  },
  {
    "objectID": "api/scheduler.html",
    "href": "api/scheduler.html",
    "title": "Scheduler API",
    "section": "",
    "text": "The scheduler module contains the Scheduler class, which is responsible for finding and enqueuing scheduled and recurring jobs."
  },
  {
    "objectID": "api/scheduler.html#naq.scheduler.scheduler",
    "href": "api/scheduler.html#naq.scheduler.scheduler",
    "title": "Scheduler API",
    "section": "naq.scheduler.Scheduler",
    "text": "naq.scheduler.Scheduler\nYou typically run the scheduler from the command line using naq scheduler, but you can also create and run a Scheduler instance programmatically.\n\nnaq.scheduler.Scheduler(nats_url, poll_interval, instance_id, enable_ha, services)\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\nnats_url\nstr\nThe URL of the NATS server.\n\n\npoll_interval\nfloat\nThe interval (in seconds) at which the scheduler checks for due jobs. Defaults to 1.0.\n\n\ninstance_id\nstr | None\nA unique ID for the scheduler instance, used for High Availability. A unique ID is generated if not provided.\n\n\nenable_ha\nbool\nWhether to enable High Availability (HA) mode with leader election. Defaults to True.\n\n\nservices\nServiceManager | None\nOptional ServiceManager instance for dependency injection.\n\n\n\n\n\nMethods\n\nrun()\nStarts the scheduler’s main processing loop. This is an async method.\nasync def run(self) -&gt; None\nThe scheduler will connect to NATS and, if it becomes the leader (or if HA is disabled), it will start polling for jobs that are ready to be enqueued.\n\n\n\nHigh Availability (HA)\nWhen enable_ha is True, you can run multiple Scheduler instances for redundancy. They will use a leader election protocol built on a NATS KV store to ensure that only one instance is actively scheduling jobs at any given time. If the leader instance goes down, another instance will automatically take over."
  },
  {
    "objectID": "api/scheduler.html#service-layer-integration",
    "href": "api/scheduler.html#service-layer-integration",
    "title": "Scheduler API",
    "section": "Service Layer Integration",
    "text": "Service Layer Integration\nThe Scheduler class integrates with the service layer architecture to provide efficient resource management and connection pooling. When using the service layer, the scheduler automatically leverages centralized services for NATS connections, KV store operations, and event logging.\n\nUsing Scheduler with ServiceManager\nThe recommended way to use the Scheduler class with the service layer is to provide a ServiceManager instance:\nimport asyncio\nfrom naq.scheduler import Scheduler\nfrom naq.services import ServiceManager\n\nasync def scheduler_with_services():\n    # Create service configuration\n    config = {\n        'nats': {\n            'url': 'nats://localhost:4222',\n            'max_reconnect_attempts': 5,\n            'reconnect_delay': 1.0\n        },\n        'scheduler': {\n            'poll_interval': 1.0,\n            'enable_ha': True\n        },\n        'events': {\n            'enabled': True,\n            'batch_size': 100,\n            'flush_interval': 1.0\n        }\n    }\n    \n    # Use ServiceManager for lifecycle management\n    async with ServiceManager(config) as services:\n        # Create Scheduler with ServiceManager\n        scheduler = Scheduler(\n            services=services,\n            poll_interval=1.0,\n            enable_ha=True,\n            instance_id='scheduler-1'\n        )\n        \n        # Run the scheduler - it will automatically use the services\n        await scheduler.run()\n        \n        # Services are automatically managed and cleaned up\n\n\nBenefits of Service Layer Integration\n\nConnection Pooling: Multiple scheduler instances can share the same NATS connection, reducing resource usage.\nCentralized KV Store Operations: Scheduled job storage and retrieval is handled by the KVStoreService, providing consistent access patterns.\nEvent Logging: Scheduler events are automatically logged through the EventService, providing comprehensive monitoring.\nAutomatic Resource Management: Services are automatically initialized and cleaned up by the ServiceManager.\n\n\n\nMigration from Direct Connection Management\n\nBefore (Direct Connection Management)\n# Old approach - direct connection management\nasync def old_scheduler_usage():\n    scheduler = Scheduler(\n        nats_url='nats://localhost:4222',\n        poll_interval=1.0,\n        enable_ha=True\n    )\n    \n    try:\n        await scheduler.run()\n    finally:\n        # Manual cleanup was required\n        pass\n\n\nAfter (Service Layer)\n# New approach - service layer\nasync def new_scheduler_usage():\n    config = {'nats': {'url': 'nats://localhost:4222'}}\n    \n    async with ServiceManager(config) as services:\n        scheduler = Scheduler(\n            services=services,\n            poll_interval=1.0,\n            enable_ha=True\n        )\n        \n        await scheduler.run()\n        # No need to manually manage connections - services are automatically managed\n\n\n\nAdvanced Service Configuration\nYou can provide detailed configuration for the services used by the scheduler:\nasync def advanced_scheduler_config():\n    config = {\n        'nats': {\n            'url': 'nats://localhost:4222',\n            'max_reconnect_attempts': 10,\n            'reconnect_delay': 2.0,\n            'connection_timeout': 30.0\n        },\n        'scheduler': {\n            'poll_interval': 0.5,  # Poll every 500ms for lower latency\n            'enable_ha': True,\n            'leader_lock_ttl': 30  # Leader lock TTL in seconds\n        },\n        'events': {\n            'enabled': True,\n            'batch_size': 50,\n            'flush_interval': 0.5,\n            'max_buffer_size': 5000\n        }\n    }\n    \n    async with ServiceManager(config) as services:\n        scheduler = Scheduler(\n            services=services,\n            poll_interval=0.5,\n            enable_ha=True,\n            instance_id='production-scheduler-1'\n        )\n        \n        await scheduler.run()\n\n\nHigh Availability with Service Layer\nWhen using the service layer with HA mode, multiple scheduler instances can share the same ServiceManager:\nasync def ha_scheduler_with_services():\n    config = {\n        'nats': {'url': 'nats://localhost:4222'},\n        'scheduler': {'enable_ha': True}\n    }\n    \n    async with ServiceManager(config) as services:\n        # Create multiple scheduler instances for HA\n        scheduler1 = Scheduler(\n            services=services,\n            instance_id='scheduler-1',\n            enable_ha=True\n        )\n        \n        scheduler2 = Scheduler(\n            services=services,\n            instance_id='scheduler-2',\n            enable_ha=True\n        )\n        \n        # Run schedulers concurrently\n        scheduler1_task = asyncio.create_task(scheduler1.run())\n        scheduler2_task = asyncio.create_task(scheduler2.run())\n        \n        # One scheduler will become the leader, the other will be a follower\n        await asyncio.sleep(30)\n        \n        # Stop the leader scheduler\n        scheduler1_task.cancel()\n        \n        try:\n            await scheduler1_task\n        except asyncio.CancelledError:\n            pass\n        \n        # The follower (scheduler2) will automatically become the new leader\n        await asyncio.sleep(30)\n        \n        # Clean up\n        scheduler2_task.cancel()\n        try:\n            await scheduler2_task\n        except asyncio.CancelledError:\n            pass\n\n\nScheduler Event Integration\nWhen using the service layer, schedulers automatically integrate with the event system:\nasync def scheduler_with_events():\n    config = {\n        'nats': {'url': 'nats://localhost:4222'},\n        'scheduler': {'enable_ha': False},\n        'events': {\n            'enabled': True,\n            'batch_size': 25,\n            'flush_interval': 0.25\n        }\n    }\n    \n    async with ServiceManager(config) as services:\n        scheduler = Scheduler(\n            services=services,\n            poll_interval=1.0\n        )\n        \n        # The scheduler will automatically log:\n        # - SCHEDULED events when jobs are scheduled\n        # - SCHEDULE_TRIGGERED events when jobs are triggered for execution\n        # - STATUS_CHANGED events for scheduler state transitions\n        \n        await scheduler.run()\n\n\nCustom Scheduler with Services\nFor advanced use cases, you can create custom scheduler classes that leverage the service layer:\nfrom naq.scheduler import Scheduler\nfrom naq.services import ServiceManager, SchedulerService, EventService\n\nclass CustomScheduler(Scheduler):\n    \"\"\"Custom scheduler with enhanced service integration.\"\"\"\n    \n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._scheduler_service = None\n        self._event_service = None\n    \n    async def _setup_services(self):\n        \"\"\"Set up service references.\"\"\"\n        if self._services:\n            self._scheduler_service = await self._services.get_service(SchedulerService)\n            self._event_service = await self._services.get_service(EventService)\n    \n    async def trigger_due_jobs(self) -&gt; List[str]:\n        \"\"\"Override to add custom triggering logic with services.\"\"\"\n        await self._setup_services()\n        \n        # Log custom event before triggering\n        if self._event_service:\n            from naq.models import JobEvent, JobEventType\n            custom_event = JobEvent(\n                job_id=f\"scheduler-{self.instance_id}\",\n                event_type=\"custom_trigger_start\",\n                queue_name=\"scheduler\",\n                worker_id=self.instance_id,\n                details={\"timestamp\": time.time()}\n            )\n            await self._event_service.log_event(custom_event)\n        \n        # Use the scheduler service to trigger jobs\n        triggered_jobs = await self._scheduler_service.trigger_due_jobs()\n        \n        # Log custom event after triggering\n        if self._event_service:\n            custom_event = JobEvent(\n                job_id=f\"scheduler-{self.instance_id}\",\n                event_type=\"custom_trigger_complete\",\n                queue_name=\"scheduler\",\n                worker_id=self.instance_id,\n                details={\n                    \"triggered_count\": len(triggered_jobs),\n                    \"triggered_jobs\": triggered_jobs\n                }\n            )\n            await self._event_service.log_event(custom_event)\n        \n        return triggered_jobs\n\n# Using the custom scheduler\nasync def custom_scheduler_example():\n    config = {'nats': {'url': 'nats://localhost:4222'}}\n    \n    async with ServiceManager(config) as services:\n        scheduler = CustomScheduler(\n            services=services,\n            poll_interval=1.0,\n            instance_id='custom-scheduler-1'\n        )\n        \n        await scheduler.run()\n\n\nScheduler Statistics with Services\nThe service layer provides enhanced statistics and monitoring capabilities for schedulers:\nasync def scheduler_stats_example():\n    config = {'nats': {'url': 'nats://localhost:4222'}}\n    \n    async with ServiceManager(config) as services:\n        scheduler = Scheduler(\n            services=services,\n            poll_interval=1.0\n        )\n        \n        # Get scheduler statistics from the service\n        scheduler_service = await services.get_service(SchedulerService)\n        stats = await scheduler_service.get_scheduler_stats()\n        \n        print(f\"Scheduler statistics: {stats}\")\n        # Stats might include:\n        # - Total scheduled jobs\n        # - Jobs triggered in the last interval\n        # - Average trigger latency\n        # - HA status (leader/follower)\n        \n        await scheduler.run()\nThe service layer integration provides a robust foundation for building efficient, maintainable scheduler applications with naq, eliminating connection duplication and providing clean resource management throughout the system."
  },
  {
    "objectID": "api/job.html",
    "href": "api/job.html",
    "title": "Job API",
    "section": "",
    "text": "The Job class represents a unit of work that is enqueued and executed by a worker. You typically don’t create Job instances directly; they are created for you when you call functions like naq.enqueue().\nA Job instance is returned every time you enqueue a task, and it serves as a handle to that task."
  },
  {
    "objectID": "api/job.html#naq.job.job",
    "href": "api/job.html#naq.job.job",
    "title": "Job API",
    "section": "naq.job.Job",
    "text": "naq.job.Job\n\nProperties\n\n\n\n\n\n\n\n\nProperty\nType\nDescription\n\n\n\n\njob_id\nstr\nA unique identifier for the job.\n\n\nfunction\nCallable\nThe function that will be executed.\n\n\nargs\ntuple\nThe positional arguments passed to the function.\n\n\nkwargs\ndict\nThe keyword arguments passed to the function.\n\n\nqueue_name\nstr\nThe name of the queue the job belongs to.\n\n\nstatus\nJOB_STATUS\nThe current status of the job (pending, running, completed, failed).\n\n\nmax_retries\nint\nThe maximum number of times the job will be retried if it fails.\n\n\nretry_delay\nint | float | list\nThe delay (in seconds) between retries. Can be a single value or a list.\n\n\nretry_strategy\nstr\nThe retry strategy (linear or exponential).\n\n\ndepends_on\nlist[str] | None\nA list of job IDs that this job depends on.\n\n\nresult_ttl\nint | None\nThe time-to-live (in seconds) for the job’s result.\n\n\ntimeout\nint | None\nThe maximum time (in seconds) the job is allowed to run.\n\n\nenqueue_time\nfloat\nThe timestamp when the job was enqueued.\n\n\nerror\nstr | None\nThe error message if the job failed.\n\n\ntraceback\nstr | None\nThe traceback if the job failed.\n\n\n\n\n\nMethods\n\nfetch_result()\nA static method to fetch the result of a completed job.\n@staticmethod\nasync def fetch_result(job_id: str, nats_url: str = DEFAULT_NATS_URL) -&gt; Any\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\njob_id\nstr\nThe ID of the job whose result you want to fetch.\n\n\nnats_url\nstr\nThe URL of the NATS server.\n\n\n\nReturns: The return value of the job’s function.\nRaises:\n\nJobNotFoundError: If the job result is not found (it may not have completed, or the result may have expired).\nJobExecutionError: If the job failed. The exception message will contain the error and traceback from the worker.\n\n\n\nfetch_result_sync()\nA synchronous version of fetch_result().\n\n\n\n\n\n\nNote\n\n\n\nfetch_result_sync is deprecated and will be removed in a future version. Please use fetch_result in an async context."
  },
  {
    "objectID": "api/index.html",
    "href": "api/index.html",
    "title": "API Reference",
    "section": "",
    "text": "This section provides a detailed reference for the public API of the naq library.\nThe API is organized into modules, each providing specific functionality."
  },
  {
    "objectID": "api/index.html#core-modules",
    "href": "api/index.html#core-modules",
    "title": "API Reference",
    "section": "Core Modules",
    "text": "Core Modules\n\nqueue Module: The primary interface for enqueuing and scheduling jobs. Includes the Queue class and helper functions like enqueue, enqueue_at, and schedule.\njob Module: Defines the Job class, which represents a unit of work to be executed.\nworker Module: Contains the Worker class, responsible for executing jobs from one or more queues.\nscheduler Module: Contains the Scheduler class, responsible for enqueuing scheduled and recurring jobs.\nevents Module: Provides the event logging and monitoring system, including JobEvent, AsyncJobEventLogger, and AsyncJobEventProcessor.\nexceptions Module: Defines custom exceptions raised by naq."
  },
  {
    "objectID": "api/index.html#service-layer",
    "href": "api/index.html#service-layer",
    "title": "API Reference",
    "section": "Service Layer",
    "text": "Service Layer\nThe service layer provides a centralized architecture for managing resources and dependencies in naq. It eliminates connection duplication and provides a clean, maintainable way to handle NATS connections, streams, KV stores, and other resources.\n\nservices Package: Core service infrastructure including base classes and service manager.\n\n\nCore Services\n\nConnectionService: Manages NATS connections and JetStream contexts with connection pooling, retry logic, and health monitoring.\nStreamService: Handles NATS JetStream stream operations including creation, configuration, and management.\nKVStoreService: Manages NATS Key-Value store operations with TTL support and transaction capabilities.\nJobService: Handles job execution, result storage, and failure handling with proper error management.\nEventService: Manages event logging, batching, and processing with efficient buffering and streaming.\nSchedulerService: Handles scheduled job management, triggering, and persistence.\n\n\n\nService Infrastructure\n\nBaseService: Abstract base class for all services with lifecycle management.\nServiceManager: Central service registry and dependency injection container.\n\n\n\nService Usage\nThe service layer is designed to be used transparently by the core naq components, but can also be used directly for advanced use cases:\nimport asyncio\nfrom naq.services import ServiceManager, ConnectionService, StreamService\nfrom naq.queue import Queue\n\nasync def service_example():\n    # Create service configuration\n    config = {\n        'nats': {\n            'url': 'nats://localhost:4222',\n            'max_reconnect_attempts': 5,\n            'reconnect_delay': 1.0\n        }\n    }\n    \n    # Use ServiceManager for lifecycle management\n    async with ServiceManager(config) as services:\n        # Services are automatically created and managed\n        queue = Queue(name='example', services=services)\n        \n        # Queue uses services internally for all operations\n        job = await queue.enqueue(my_function, arg1, arg2)\n        \n        # Services are automatically cleaned up when context exits\nFor more detailed information about the service layer architecture and advanced usage patterns, see the Service Layer Architecture section in the architecture documentation."
  },
  {
    "objectID": "api/events.html",
    "href": "api/events.html",
    "title": "Events API",
    "section": "",
    "text": "The naq.events module provides a comprehensive event logging and monitoring system for naq. It allows you to track the entire lifecycle of your jobs, from enqueuing to completion or failure, and react to these events in real-time.\nThis module is built on top of NATS JetStream, ensuring that events are durable and can be processed even if consumers are offline."
  },
  {
    "objectID": "api/events.html#core-components",
    "href": "api/events.html#core-components",
    "title": "Events API",
    "section": "Core Components",
    "text": "Core Components\n\nnaq.events.JobEventType\nAn enumeration of all possible event types in a job’s lifecycle.\nclass JobEventType(str, Enum):\n    ENQUEUED = \"enqueued\"\n    STARTED = \"started\"\n    COMPLETED = \"completed\"\n    FAILED = \"failed\"\n    RETRY_SCHEDULED = \"retry_scheduled\"\n    SCHEDULED = \"scheduled\"\n    SCHEDULE_TRIGGERED = \"schedule_triggered\"\n    CANCELLED = \"cancelled\"\n    STATUS_CHANGED = \"status_changed\"\n\nEvent Type Descriptions\n\n\n\n\n\n\n\n\nEvent Type\nDescription\nTriggered By\n\n\n\n\nENQUEUED\nJob has been added to a queue\nClient/Queue\n\n\nSTARTED\nJob has begun execution by a worker\nWorker\n\n\nCOMPLETED\nJob finished successfully\nWorker\n\n\nFAILED\nJob failed during execution\nWorker\n\n\nRETRY_SCHEDULED\nJob failed and will be retried\nWorker\n\n\nSCHEDULED\nJob has been scheduled for future execution\nScheduler\n\n\nSCHEDULE_TRIGGERED\nScheduled job is now being enqueued\nScheduler\n\n\nCANCELLED\nJob was cancelled before or during execution\nWorker/Client\n\n\nSTATUS_CHANGED\nJob status has changed (any transition)\nAll Components\n\n\n\n\n\n\nnaq.events.JobEvent\nA msgspec.Struct representing a single event in a job’s lifecycle. This is the primary data structure for all event information.\n\nProperties\n\n\n\n\n\n\n\n\nProperty\nType\nDescription\n\n\n\n\njob_id\nstr\nThe unique identifier of the job this event belongs to.\n\n\nevent_type\nJobEventType\nThe type of event that occurred.\n\n\ntimestamp\nfloat\nThe UTC timestamp when the event was generated.\n\n\nworker_id\nstr \\| None\nThe ID of the worker that processed the job (if applicable).\n\n\nqueue_name\nstr \\| None\nThe name of the queue the job was in.\n\n\nmessage\nstr \\| None\nA human-readable message describing the event.\n\n\ndetails\ndict \\| None\nAdditional structured details about the event.\n\n\nerror_type\nstr \\| None\nThe type of error if the event represents a failure.\n\n\nerror_message\nstr \\| None\nThe error message if the event represents a failure.\n\n\nduration_ms\nint \\| None\nThe duration of the job execution in milliseconds (if applicable).\n\n\nnats_subject\nstr \\| None\nThe NATS subject the event was published to.\n\n\nnats_sequence\nint \\| None\nThe NATS stream sequence number of the event.\n\n\n\n\n\nConvenience Methods\nThe JobEvent class provides several class methods for easily creating specific event types:\n\nJobEvent.enqueued(job_id: str, queue_name: str, **kwargs) -&gt; JobEvent\nJobEvent.started(job_id: str, worker_id: str, queue_name: str, **kwargs) -&gt; JobEvent\nJobEvent.completed(job_id: str, worker_id: str, queue_name: str, duration_ms: int, **kwargs) -&gt; JobEvent\nJobEvent.failed(job_id: str, worker_id: str, queue_name: str, error_type: str, error_message: str, **kwargs) -&gt; JobEvent\nJobEvent.retry_scheduled(job_id: str, worker_id: str, queue_name: str, **kwargs) -&gt; JobEvent\nJobEvent.scheduled(job_id: str, **kwargs) -&gt; JobEvent\nJobEvent.schedule_triggered(job_id: str, **kwargs) -&gt; JobEvent\nJobEvent.cancelled(job_id: str, queue_name: str, **kwargs) -&gt; JobEvent\nJobEvent.status_changed(job_id: str, queue_name: str, old_status: str, new_status: str, **kwargs) -&gt; JobEvent\n\n\n\n\nnaq.events.NATSJobEventStorage\nA storage backend for job events using NATS JetStream. This class is responsible for persisting and retrieving events. You typically won’t need to interact with this class directly unless you are building custom tools.\n\nInitialization\nasync def __init__(\n    self,\n    nats_url: str = DEFAULT_NATS_URL,\n    stream_name: str = \"NAQ_JOB_EVENTS\",\n    subject_prefix: str = \"naq.jobs.events\",\n    stream_config: Optional[StreamConfig] = None\n)\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\nnats_url\nstr\nThe URL of the NATS server.\n\n\nstream_name\nstr\nThe name of the JetStream stream to use for events.\n\n\nsubject_prefix\nstr\nThe prefix for NATS subjects used for events.\n\n\nstream_config\nOptional[StreamConfig]\nCustom JetStream stream configuration. If None, defaults are used.\n\n\n\n\n\nKey Methods\n\nasync def store_event(self, event: JobEvent) -&gt; None: Stores a single event.\nasync def get_events(self, job_id: str) -&gt; List[JobEvent]: Retrieves all events for a given job ID.\nasync def stream_events(self, **kwargs) -&gt; AsyncIterator[JobEvent]: Streams new events in real-time. Accepts filters like context, event_type, etc.\n\n\n\n\nnaq.events.AsyncJobEventLogger\nA high-performance, non-blocking logger that buffers events in memory and flushes them periodically to the NATSJobEventStorage. This is the primary interface for logging events from your application.\n\nInitialization\nasync def __init__(\n    self,\n    storage: NATSJobEventStorage,\n    batch_size: int = 100,\n    flush_interval: float = 5.0,\n    max_buffer_size: int = 10000\n)\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\nstorage\nNATSJobEventStorage\nThe storage backend instance to use.\n\n\nbatch_size\nint\nThe number of events to buffer before flushing.\n\n\nflush_interval\nfloat\nThe interval in seconds to flush buffered events.\n\n\nmax_buffer_size\nint\nThe maximum number of events to hold in the buffer before dropping new ones.\n\n\n\n\n\nKey Methods\n\nasync def start(self) -&gt; None: Starts the background flush task.\nasync def stop(self) -&gt; None: Stops the background flush task and flushes any remaining events.\nasync def log_event(self, event: JobEvent) -&gt; None: Logs a generic JobEvent.\nasync def log_job_enqueued(self, job_id: str, queue_name: str, **kwargs) -&gt; None: Logs an ENQUEUED event.\nasync def log_job_started(self, job_id: str, worker_id: str, queue_name: str, **kwargs) -&gt; None: Logs a STARTED event.\nasync def log_job_completed(self, job_id: str, worker_id: str, queue_name: str, duration_ms: int, **kwargs) -&gt; None: Logs a COMPLETED event.\nasync def log_job_failed(self, job_id: str, worker_id: str, queue_name: str, error_type: str, error_message: str, **kwargs) -&gt; None: Logs a FAILED event.\nasync def log_job_retry_scheduled(self, job_id: str, worker_id: str, queue_name: str, **kwargs) -&gt; None: Logs a RETRY_SCHEDULED event.\nasync def log_job_scheduled(self, job_id: str, **kwargs) -&gt; None: Logs a SCHEDULED event.\nasync def log_job_schedule_triggered(self, job_id: str, **kwargs) -&gt; None: Logs a SCHEDULE_TRIGGERED event.\nasync def log_job_cancelled(self, job_id: str, queue_name: str, **kwargs) -&gt; None: Logs a CANCELLED event.\nasync def log_job_status_changed(self, job_id: str, queue_name: str, old_status: str, new_status: str, **kwargs) -&gt; None: Logs a STATUS_CHANGED event.\n\n\n\n\nnaq.events.JobEventLogger\nA synchronous wrapper around AsyncJobEventLogger, following the _sync pattern used elsewhere in naq. It provides the same interface but can be used in synchronous contexts.\n\nInitialization\ndef __init__(\n    self,\n    nats_url: str = DEFAULT_NATS_URL,\n    **logger_kwargs\n)\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\nnats_url\nstr\nThe URL of the NATS server.\n\n\nlogger_kwargs\ndict\nKeyword arguments to pass to the AsyncJobEventLogger constructor.\n\n\n\nThe methods are the same as AsyncJobEventLogger but without the async/await keywords.\n\n\n\nnaq.events.SharedEventLoggerManager\nA singleton manager for shared event logger instances that provides a centralized way to create, manage, and share event logger instances across different components in the NAQ system.\n\nKey Features\n\nSingleton Pattern: Ensures a single instance across the application\nThread-safe: Safe for use in multi-threaded environments\nDual Support: Manages both synchronous and asynchronous loggers\nCentralized Configuration: Configure event logging once for all components\n\n\n\nInitialization\nThe SharedEventLoggerManager is a singleton and should be accessed through the global instance:\nfrom naq.events import get_shared_event_logger_manager\n\n# Get the global instance\nmanager = get_shared_event_logger_manager()\n\n\nConfiguration\ndef configure(\n    self,\n    enabled: Optional[bool] = None,\n    storage_type: Optional[str] = None,\n    storage_url: Optional[str] = None,\n    stream_name: Optional[str] = None,\n    subject_prefix: Optional[str] = None,\n    **kwargs: Any\n) -&gt; None\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\nenabled\nOptional[bool]\nWhether event logging is enabled\n\n\nstorage_type\nOptional[str]\nType of storage backend (‘nats’, etc.)\n\n\nstorage_url\nOptional[str]\nURL for the storage backend\n\n\nstream_name\nOptional[str]\nName of the stream for events\n\n\nsubject_prefix\nOptional[str]\nBase subject for events\n\n\n**kwargs\nAny\nAdditional configuration options\n\n\n\n\n\nKey Methods\n\nget_sync_logger() -&gt; Optional[JobEventLogger]: Get the shared synchronous event logger instance.\nasync get_async_logger() -&gt; Optional[AsyncJobEventLogger]: Get the shared asynchronous event logger instance.\nconfigure(**kwargs) -&gt; None: Configure the shared event logger manager.\nreset() -&gt; None: Reset the shared event logger manager (synchronous).\nasync async_reset() -&gt; None: Reset the shared event logger manager (asynchronous).\nget_config() -&gt; Dict[str, Any]: Get the current configuration.\nis_enabled() -&gt; bool: Check if event logging is enabled.\n\n\n\nConvenience Functions\nFor easier access, the module provides global convenience functions:\nfrom naq.events import (\n    get_shared_sync_logger,\n    get_shared_async_logger,\n    configure_shared_logger,\n)\n\n# Configure the shared logger\nconfigure_shared_logger(enabled=True, storage_url=\"nats://localhost:4222\")\n\n# Get loggers\nsync_logger = get_shared_sync_logger()\nasync_logger = await get_shared_async_logger()\n\n\n\nUsing the Shared Event Logger\nThe shared event logger is automatically used by all naq components (Queue, Worker, Scheduler) when event logging is enabled. You can also use it in your own applications:\nfrom naq.events import get_shared_sync_logger, configure_shared_logger\n\n# Configure event logging\nconfigure_shared_logger(enabled=True)\n\n# Get the shared logger\nlogger = get_shared_sync_logger()\nif logger:\n    # Log events\n    logger.log_job_enqueued(\n        job_id=\"job-123\",\n        queue_name=\"default\",\n        details={\"custom_field\": \"value\"}\n    )\n\n\nnaq.events.AsyncJobEventProcessor\nA real-time event processor that subscribes to the NATS event stream and dispatches events to registered handlers. This allows you to build reactive, event-driven logic based on job lifecycles.\n\nInitialization\nasync def __init__(self, storage: NATJobEventStorage)\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\nstorage\nNATSJobEventStorage\nThe storage backend instance to stream events from.\n\n\n\n\n\nKey Methods\n\nasync def start(self) -&gt; None: Starts the event processing loop.\nasync def stop(self) -&gt; None: Stops the event processing loop.\ndef add_handler(self, event_type: JobEventType, handler: Callable[[JobEvent], Any]) -&gt; None: Registers a handler function for a specific event type.\ndef add_global_handler(self, handler: Callable[[JobEvent], Any]) -&gt; None: Registers a handler function that will be called for all events.\n\nHandlers can be either synchronous or asynchronous functions that accept a single JobEvent argument. Synchronous handlers are automatically executed in a thread pool to avoid blocking the event loop.\n\n\n\nEvent Consumption Examples\n\nBasic Event Processing\nimport asyncio\nfrom naq.events import AsyncJobEventProcessor, NATSJobEventStorage, JobEventType\n\nasync def handle_job_completion(event: JobEvent):\n    \"\"\"Handle job completion events.\"\"\"\n    print(f\"Job {event.job_id} completed in {event.duration_ms}ms\")\n    # Update your application state here\n\nasync def main():\n    # Create storage and processor\n    storage = NATSJobEventStorage()\n    processor = AsyncJobEventProcessor(storage)\n    \n    # Register handlers\n    processor.add_handler(JobEventType.COMPLETED, handle_job_completion)\n    \n    # Start processing\n    await processor.start()\n    \n    try:\n        # Keep running\n        while True:\n            await asyncio.sleep(1)\n    finally:\n        await processor.stop()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\n\nAdvanced Event Processing with Filtering\nimport asyncio\nfrom naq.events import AsyncJobEventProcessor, NATSJobEventStorage, JobEventType\n\nasync def handle_failed_jobs(event: JobEvent):\n    \"\"\"Handle failed job events with retry logic.\"\"\"\n    if event.queue_name == \"critical\":\n        # Send alert for critical queue failures\n        await send_alert(f\"Critical job failed: {event.job_id}\")\n        \n        # Check if we should retry\n        if event.details and event.details.get(\"retry_count\", 0) &lt; 3:\n            print(f\"Retrying job {event.job_id}\")\n            await retry_job(event.job_id)\n\nasync def handle_all_events(event: JobEvent):\n    \"\"\"Global handler for all events.\"\"\"\n    # Log all events to external monitoring system\n    await log_to_monitoring_system(event)\n\nasync def main():\n    storage = NATSJobEventStorage()\n    processor = AsyncJobEventProcessor(storage)\n    \n    # Register specific handler\n    processor.add_handler(JobEventType.FAILED, handle_failed_jobs)\n    \n    # Register global handler\n    processor.add_global_handler(handle_all_events)\n    \n    async with processor:  # Use context manager for automatic cleanup\n        # Keep running\n        while True:\n            await asyncio.sleep(1)\n\n\nUsing Event Details for Custom Logic\nasync def handle_status_changes(event: JobEvent):\n    \"\"\"Handle status change events with custom logic.\"\"\"\n    if event.event_type == JobEventType.STATUS_CHANGED:\n        old_status = event.details.get(\"old_status\")\n        new_status = event.details.get(\"new_status\")\n        \n        print(f\"Job {event.job_id} changed from {old_status} to {new_status}\")\n        \n        # Trigger specific actions based on status transitions\n        if old_status == \"running\" and new_status == \"failed\":\n            await notify_team_about_failure(event)\n        elif old_status == \"pending\" and new_status == \"running\":\n            await update_job_start_time(event.job_id)\n\n\n\nEvent Stream Processing\nThe NATSJobEventStorage provides methods for both retrieving historical events and streaming real-time events:\n# Get all events for a specific job\nevents = await storage.get_events(\"job-123\")\n\n# Stream events for a specific job in real-time\nasync for event in storage.stream_events(\"job-123\"):\n    print(f\"New event: {event.event_type}\")\n    \n    # Process the event\n    await process_event(event)\n\n\nPerformance Considerations\nWhen working with events, consider these performance best practices:\n\nUse Shared Loggers: Always use the shared event logger to avoid creating multiple connections.\nBatch Processing: The shared logger automatically batches events for better performance.\nAsync Handlers: Use async handlers for event processing to avoid blocking.\nSelective Subscriptions: Only subscribe to events you need to process.\nError Handling: Implement proper error handling in your event handlers to prevent processing failures."
  },
  {
    "objectID": "api/services.html",
    "href": "api/services.html",
    "title": "Services API",
    "section": "",
    "text": "The services package provides the core infrastructure for the service layer architecture in naq. It includes base classes for service implementation and a service manager for dependency injection and lifecycle management."
  },
  {
    "objectID": "api/services.html#package-overview",
    "href": "api/services.html#package-overview",
    "title": "Services API",
    "section": "Package Overview",
    "text": "Package Overview\nThe services package is designed to eliminate connection duplication and provide centralized resource management throughout the naq system. It consists of:\n\nBase Classes: Abstract base classes for service implementation\nService Manager: Central registry and dependency injection container\nCore Services: Specialized services for different aspects of the system"
  },
  {
    "objectID": "api/services.html#base-classes",
    "href": "api/services.html#base-classes",
    "title": "Services API",
    "section": "Base Classes",
    "text": "Base Classes\n\nBaseService\nThe abstract base class for all services in naq.\nclass BaseService(ABC):\n    \"\"\"Base class for all NAQ services.\"\"\"\n    \n    def __init__(self, config: Dict[str, Any]):\n        self.config = config\n        self._initialized = False\n        \n    async def initialize(self) -&gt; None:\n        \"\"\"Initialize the service.\"\"\"\n        if not self._initialized:\n            await self._do_initialize()\n            self._initialized = True\n            \n    @abstractmethod\n    async def _do_initialize(self) -&gt; None:\n        \"\"\"Implement service-specific initialization.\"\"\"\n        \n    async def cleanup(self) -&gt; None:\n        \"\"\"Cleanup service resources.\"\"\"\n        self._initialized = False\n        \n    async def __aenter__(self):\n        await self.initialize()\n        return self\n        \n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        await self.cleanup()\n\nUsage Example\nfrom naq.services.base import BaseService\n\nclass MyCustomService(BaseService):\n    def __init__(self, config: Dict[str, Any]):\n        super().__init__(config)\n        self._resource = None\n    \n    async def _do_initialize(self) -&gt; None:\n        \"\"\"Initialize the custom service.\"\"\"\n        self._resource = await self._create_resource()\n    \n    async def _create_resource(self):\n        \"\"\"Create a resource for the service.\"\"\"\n        # Implementation-specific resource creation\n        return {\"status\": \"initialized\"}\n    \n    async def cleanup(self) -&gt; None:\n        \"\"\"Cleanup the custom service.\"\"\"\n        if self._resource:\n            await self._cleanup_resource(self._resource)\n        await super().cleanup()\n    \n    async def _cleanup_resource(self, resource):\n        \"\"\"Cleanup a specific resource.\"\"\"\n        # Implementation-specific resource cleanup\n        pass\n\n# Using the custom service\nasync def use_custom_service():\n    config = {\"setting1\": \"value1\"}\n    service = MyCustomService(config)\n    \n    async with service:\n        # Service is automatically initialized\n        result = await service.do_work()\n        # Service is automatically cleaned up\n\n\n\nServiceManager\nThe central component that manages service instances and their dependencies.\nclass ServiceManager:\n    \"\"\"Manages service instances and dependencies.\"\"\"\n    \n    def __init__(self, config: Dict[str, Any]):\n        self.config = config\n        self._services: Dict[str, BaseService] = {}\n        \n    async def get_service(self, service_type: type) -&gt; BaseService:\n        \"\"\"Get or create service instance.\"\"\"\n        # Implementation handles service creation and dependency injection\n        \n    async def cleanup_all(self) -&gt; None:\n        \"\"\"Clean up all registered services.\"\"\"\n        \n    async def __aenter__(self):\n        return self\n        \n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        await self.cleanup_all()\n\nUsage Example\nfrom naq.services import ServiceManager, ConnectionService, StreamService\n\nasync def service_manager_example():\n    config = {\n        'nats': {\n            'url': 'nats://localhost:4222',\n            'max_reconnect_attempts': 5\n        }\n    }\n    \n    async with ServiceManager(config) as services:\n        # Get services - they are created and initialized on demand\n        connection_service = await services.get_service(ConnectionService)\n        stream_service = await services.get_service(StreamService)\n        \n        # Use the services\n        js = await connection_service.get_jetstream()\n        stream_info = await stream_service.get_stream_info(\"my_stream\")\n        \n        # All services are automatically cleaned up when exiting the context"
  },
  {
    "objectID": "api/services.html#core-services",
    "href": "api/services.html#core-services",
    "title": "Services API",
    "section": "Core Services",
    "text": "Core Services\n\nConnectionService\nManages NATS connections and JetStream contexts with connection pooling, retry logic, and health monitoring.\n\nKey Features\n\nConnection Pooling: Reuses existing connections to avoid duplication\nRetry Logic: Implements exponential backoff for connection failures\nHealth Monitoring: Provides connection status and health checks\nConfiguration Management: Centralized connection configuration\n\n\n\nAPI Reference\nclass ConnectionService(BaseService):\n    \"\"\"Centralized NATS connection management service.\"\"\"\n    \n    async def get_connection(self, url: Optional[str] = None) -&gt; NATSClient:\n        \"\"\"Get a pooled NATS connection.\"\"\"\n        \n    async def get_jetstream(self, url: Optional[str] = None) -&gt; JetStreamContext:\n        \"\"\"Get a JetStream context for a specific connection.\"\"\"\n        \n    @asynccontextmanager\n    async def connection_scope(self, url: Optional[str] = None) -&gt; AsyncGenerator[NATSClient, None]:\n        \"\"\"Context manager for connection operations.\"\"\"\n        \n    async def check_connection_health(self, url: Optional[str] = None) -&gt; bool:\n        \"\"\"Check if a connection is healthy.\"\"\"\n        \n    def get_connection_status(self, url: Optional[str] = None) -&gt; Dict[str, Any]:\n        \"\"\"Get status information about connections.\"\"\"\n\n\nUsage Example\nfrom naq.services import ServiceManager, ConnectionService\n\nasync def connection_service_example():\n    config = {'nats': {'url': 'nats://localhost:4222'}}\n    \n    async with ServiceManager(config) as services:\n        connection_service = await services.get_service(ConnectionService)\n        \n        # Get a connection\n        nc = await connection_service.get_connection()\n        \n        # Use the connection\n        await nc.publish(\"subject\", b\"message\")\n        \n        # Get JetStream context\n        js = await connection_service.get_jetstream()\n        \n        # Check connection health\n        is_healthy = await connection_service.check_connection_health()\n        \n        # Get connection status\n        status = connection_service.get_connection_status()\n\n\n\nStreamService\nHandles NATS JetStream stream operations including creation, configuration, and management.\n\nKey Features\n\nStream Management: Create, configure, and manage JetStream streams\nStream Information: Retrieve metadata and statistics about streams\nStream Operations: Purge, delete, and list streams\nMessage Counting: Get message counts for streams\n\n\n\nAPI Reference\nclass StreamService(BaseService):\n    \"\"\"Service for managing NATS JetStream streams.\"\"\"\n    \n    async def ensure_stream(\n        self,\n        name: str,\n        subjects: List[str],\n        **config\n    ) -&gt; StreamInfo:\n        \"\"\"Ensure a stream exists with the specified configuration.\"\"\"\n        \n    async def get_stream_info(self, name: str) -&gt; StreamInfo:\n        \"\"\"Get information about a stream.\"\"\"\n        \n    async def delete_stream(self, name: str) -&gt; None:\n        \"\"\"Delete a stream.\"\"\"\n        \n    async def purge_stream(self, name: str, subject: Optional[str] = None) -&gt; None:\n        \"\"\"Purge messages from a stream.\"\"\"\n        \n    async def list_streams(self) -&gt; List[str]:\n        \"\"\"List all streams.\"\"\"\n        \n    async def get_stream_message_count(self, name: str) -&gt; int:\n        \"\"\"Get the number of messages in a stream.\"\"\"\n\n\nUsage Example\nfrom naq.services import ServiceManager, StreamService\n\nasync def stream_service_example():\n    config = {'nats': {'url': 'nats://localhost:4222'}}\n    \n    async with ServiceManager(config) as services:\n        stream_service = await services.get_service(StreamService)\n        \n        # Ensure a stream exists\n        stream_info = await stream_service.ensure_stream(\n            name=\"my_stream\",\n            subjects=[\"my.subject.*\"],\n            retention=\"limits\",\n            max_msgs=10000\n        )\n        \n        # Get stream information\n        info = await stream_service.get_stream_info(\"my_stream\")\n        \n        # List all streams\n        streams = await stream_service.list_streams()\n        \n        # Get message count\n        count = await stream_service.get_stream_message_count(\"my_stream\")\n        \n        # Purge the stream\n        await stream_service.purge_stream(\"my_stream\")\n\n\n\nKVStoreService\nManages NATS Key-Value store operations with TTL support and transaction capabilities.\n\nKey Features\n\nKV Store Management: Create and manage Key-Value stores\nTTL Support: Automatic expiration of stored values\nTransaction Support: Atomic operations on multiple keys\nBatch Operations: Efficient bulk operations\n\n\n\nAPI Reference\nclass KVStoreService(BaseService):\n    \"\"\"Service for managing NATS Key-Value stores.\"\"\"\n    \n    async def get_kv_store(self, bucket: str, **config) -&gt; KeyValue:\n        \"\"\"Get a Key-Value store instance.\"\"\"\n        \n    async def put(self, bucket: str, key: str, value: bytes, ttl: Optional[int] = None) -&gt; None:\n        \"\"\"Put a value into the Key-Value store.\"\"\"\n        \n    async def get(self, bucket: str, key: str) -&gt; Optional[bytes]:\n        \"\"\"Get a value from the Key-Value store.\"\"\"\n        \n    async def delete(self, bucket: str, key: str) -&gt; None:\n        \"\"\"Delete a key from the Key-Value store.\"\"\"\n        \n    @asynccontextmanager\n    async def kv_transaction(self, bucket: str) -&gt; AsyncGenerator[KeyValue, None]:\n        \"\"\"Context manager for KV store transactions.\"\"\"\n        \n    async def list_keys(self, bucket: str) -&gt; list:\n        \"\"\"List all keys in a bucket.\"\"\"\n        \n    async def purge_bucket(self, bucket: str) -&gt; None:\n        \"\"\"Purge all keys from a bucket.\"\"\"\n        \n    async def delete_bucket(self, bucket: str) -&gt; None:\n        \"\"\"Delete a bucket.\"\"\"\n\n\nUsage Example\nfrom naq.services import ServiceManager, KVStoreService\n\nasync def kv_store_service_example():\n    config = {'nats': {'url': 'nats://localhost:4222'}}\n    \n    async with ServiceManager(config) as services:\n        kv_service = await services.get_service(KVStoreService)\n        \n        # Put a value with TTL\n        await kv_service.put(\"my_bucket\", \"key1\", b\"value1\", ttl=3600)\n        \n        # Get a value\n        value = await kv_service.get(\"my_bucket\", \"key1\")\n        \n        # Use transaction for atomic operations\n        async with kv_service.kv_transaction(\"my_bucket\") as kv:\n            await kv.put(\"key2\", b\"value2\")\n            await kv.put(\"key3\", b\"value3\")\n        \n        # List all keys\n        keys = await kv_service.list_keys(\"my_bucket\")\n        \n        # Delete a key\n        await kv_service.delete(\"my_bucket\", \"key1\")\n        \n        # Purge the bucket\n        await kv_service.purge_bucket(\"my_bucket\")\n\n\n\nJobService\nHandles job execution, result storage, and failure handling with proper error management.\n\nKey Features\n\nJob Execution: Execute jobs with proper error handling\nResult Storage: Store and retrieve job results\nFailure Handling: Handle job failures with retry logic\nCleanup Operations: Automatic cleanup of old results\n\n\n\nAPI Reference\nclass JobService(BaseService):\n    \"\"\"Service for handling job execution and results.\"\"\"\n    \n    async def execute_job(self, job: Job) -&gt; JobResult:\n        \"\"\"Execute a job and return the result.\"\"\"\n        \n    async def store_result(self, job_id: str, result: JobResult) -&gt; None:\n        \"\"\"Store a job result.\"\"\"\n        \n    async def get_result(self, job_id: str) -&gt; Optional[JobResult]:\n        \"\"\"Get a job result.\"\"\"\n        \n    async def handle_job_failure(self, job: Job, error: Exception) -&gt; None:\n        \"\"\"Handle a job failure.\"\"\"\n        \n    async def delete_result(self, job_id: str) -&gt; bool:\n        \"\"\"Delete a job result.\"\"\"\n        \n    async def cleanup_old_results(self, max_age_seconds: float) -&gt; int:\n        \"\"\"Clean up old job results.\"\"\"\n\n\nUsage Example\nfrom naq.services import ServiceManager, JobService\nfrom naq.models import Job\n\nasync def job_service_example():\n    config = {'nats': {'url': 'nats://localhost:4222'}}\n    \n    async with ServiceManager(config) as services:\n        job_service = await services.get_service(JobService)\n        \n        # Create a job\n        job = Job(\n            function=lambda x: x * 2,\n            args=(5,),\n            queue_name=\"test\"\n        )\n        \n        # Execute the job\n        result = await job_service.execute_job(job)\n        \n        # Store the result\n        await job_service.store_result(job.job_id, result)\n        \n        # Retrieve the result\n        stored_result = await job_service.get_result(job.job_id)\n        \n        # Clean up old results\n        cleaned_count = await job_service.cleanup_old_results(max_age_seconds=86400)\n\n\n\nEventService\nManages event logging, batching, and processing with efficient buffering and streaming.\n\nKey Features\n\nEvent Logging: Log job events with structured data\nBatching: Efficient batch processing of events\nStreaming: Real-time event streaming\nBuffer Management: Configurable buffering for high-throughput scenarios\n\n\n\nAPI Reference\nclass EventService(BaseService):\n    \"\"\"Service for managing event logging and processing.\"\"\"\n    \n    async def log_event(self, event: JobEvent) -&gt; None:\n        \"\"\"Log an event.\"\"\"\n        \n    async def log_job_started(\n        self,\n        job_id: str,\n        worker_id: str,\n        queue_name: str,\n        **kwargs\n    ) -&gt; None:\n        \"\"\"Log a job started event.\"\"\"\n        \n    async def stream_events(\n        self,\n        job_id: str,\n        event_type: Optional[JobEventType] = None,\n        **kwargs\n    ) -&gt; AsyncIterator[JobEvent]:\n        \"\"\"Stream events for a job.\"\"\"\n        \n    async def get_event_history(self, job_id: str) -&gt; List[JobEvent]:\n        \"\"\"Get the event history for a job.\"\"\"\n        \n    async def flush_events(self) -&gt; None:\n        \"\"\"Flush buffered events.\"\"\"\n        \n    def get_logger_stats(self) -&gt; Dict[str, Any]:\n        \"\"\"Get logger statistics.\"\"\"\n        \n    def get_buffer_size(self) -&gt; int:\n        \"\"\"Get the current buffer size.\"\"\"\n\n\nUsage Example\nfrom naq.services import ServiceManager, EventService\nfrom naq.models import JobEvent, JobEventType\n\nasync def event_service_example():\n    config = {'nats': {'url': 'nats://localhost:4222'}}\n    \n    async with ServiceManager(config) as services:\n        event_service = await services.get_service(EventService)\n        \n        # Create and log an event\n        event = JobEvent(\n            job_id=\"job-123\",\n            event_type=JobEventType.STARTED,\n            queue_name=\"test\",\n            worker_id=\"worker-1\"\n        )\n        await event_service.log_event(event)\n        \n        # Log a job started event\n        await event_service.log_job_started(\n            job_id=\"job-123\",\n            worker_id=\"worker-1\",\n            queue_name=\"test\"\n        )\n        \n        # Stream events for a job\n        async for event in event_service.stream_events(\"job-123\"):\n            print(f\"Event: {event.event_type} at {event.timestamp}\")\n        \n        # Get event history\n        history = await event_service.get_event_history(\"job-123\")\n        \n        # Flush buffered events\n        await event_service.flush_events()\n        \n        # Get logger statistics\n        stats = event_service.get_logger_stats()\n\n\n\nSchedulerService\nHandles scheduled job management, triggering, and persistence.\n\nKey Features\n\nJob Scheduling: Schedule jobs for future execution\nTriggering: Automatically trigger due jobs\nPersistence: Store and retrieve scheduled jobs\nManagement: Pause, resume, and modify scheduled jobs\n\n\n\nAPI Reference\nclass SchedulerService(BaseService):\n    \"\"\"Service for managing scheduled jobs.\"\"\"\n    \n    async def schedule_job(self, job: Job, schedule: Schedule) -&gt; str:\n        \"\"\"Schedule a job.\"\"\"\n        \n    async def trigger_due_jobs(self) -&gt; List[str]:\n        \"\"\"Trigger jobs that are due for execution.\"\"\"\n        \n    async def cancel_scheduled_job(self, job_id: str) -&gt; bool:\n        \"\"\"Cancel a scheduled job.\"\"\"\n        \n    async def pause_scheduled_job(self, job_id: str) -&gt; bool:\n        \"\"\"Pause a scheduled job.\"\"\"\n        \n    async def resume_scheduled_job(self, job_id: str) -&gt; bool:\n        \"\"\"Resume a paused scheduled job.\"\"\"\n        \n    async def get_scheduled_job(self, job_id: str) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"Get information about a scheduled job.\"\"\"\n        \n    async def list_scheduled_jobs(self, status: Optional[str] = None) -&gt; List[Dict[str, Any]]:\n        \"\"\"List scheduled jobs.\"\"\"\n        \n    async def get_scheduler_stats(self) -&gt; Dict[str, Any]:\n        \"\"\"Get scheduler statistics.\"\"\"\n\n\nUsage Example\nfrom naq.services import ServiceManager, SchedulerService\nfrom naq.models import Job, Schedule\nimport datetime\n\nasync def scheduler_service_example():\n    config = {'nats': {'url': 'nats://localhost:4222'}}\n    \n    async with ServiceManager(config) as services:\n        scheduler_service = await services.get_service(SchedulerService)\n        \n        # Create a job\n        job = Job(\n            function=lambda: \"Hello, World!\",\n            queue_name=\"test\"\n        )\n        \n        # Create a schedule\n        schedule = Schedule(\n            run_at=datetime.datetime.now() + datetime.timedelta(hours=1)\n        )\n        \n        # Schedule the job\n        job_id = await scheduler_service.schedule_job(job, schedule)\n        \n        # List scheduled jobs\n        jobs = await scheduler_service.list_scheduled_jobs()\n        \n        # Get scheduler statistics\n        stats = await scheduler_service.get_scheduler_stats()\n        \n        # Cancel a scheduled job\n        await scheduler_service.cancel_scheduled_job(job_id)"
  },
  {
    "objectID": "api/services.html#service-configuration",
    "href": "api/services.html#service-configuration",
    "title": "Services API",
    "section": "Service Configuration",
    "text": "Service Configuration\nServices are configured through a centralized configuration dictionary that is passed to the ServiceManager. This configuration is then distributed to the individual services based on their needs.\n\nConfiguration Structure\nconfig = {\n    'nats': {\n        'url': 'nats://localhost:4222',\n        'max_reconnect_attempts': 5,\n        'reconnect_delay': 1.0,\n        'connection_timeout': 10.0,\n        'ping_interval': 60,\n        'max_outstanding_pings': 2\n    },\n    'events': {\n        'enabled': True,\n        'batch_size': 100,\n        'flush_interval': 1.0,\n        'max_buffer_size': 10000\n    },\n    'jobs': {\n        'default_timeout': 3600,\n        'result_ttl': 604800\n    },\n    'scheduler': {\n        'poll_interval': 1.0,\n        'enable_ha': True\n    }\n}\n\n\nEnvironment Variables\nConfiguration can also be loaded from environment variables:\nimport os\n\ndef config_from_env():\n    return {\n        'nats': {\n            'url': os.getenv('NAQ_NATS_URL', 'nats://localhost:4222'),\n            'max_reconnect_attempts': int(os.getenv('NAQ_MAX_RECONNECT', '5')),\n            'reconnect_delay': float(os.getenv('NAQ_RECONNECT_DELAY', '1.0'))\n        },\n        'events': {\n            'enabled': os.getenv('NAQ_EVENTS_ENABLED', 'false').lower() == 'true',\n            'batch_size': int(os.getenv('NAQ_EVENT_BATCH_SIZE', '100'))\n        }\n    }"
  },
  {
    "objectID": "api/services.html#best-practices",
    "href": "api/services.html#best-practices",
    "title": "Services API",
    "section": "Best Practices",
    "text": "Best Practices\n\n1. Use ServiceManager for Lifecycle Management\nAlways use the ServiceManager as a context manager to ensure proper initialization and cleanup of services:\n# Good - using context manager\nasync with ServiceManager(config) as services:\n    queue = Queue(name='test', services=services)\n    # Use the queue\n\n# Avoid - manual management\nservices = ServiceManager(config)\nawait services.initialize()\ntry:\n    queue = Queue(name='test', services=services)\n    # Use the queue\nfinally:\n    await services.cleanup_all()\n\n\n2. Share ServiceManager Instances\nWhen possible, share a single ServiceManager instance across components in the same process:\n# Good - shared ServiceManager\nasync def application():\n    config = get_config()\n    \n    async with ServiceManager(config) as services:\n        queue1 = Queue(name='queue1', services=services)\n        queue2 = Queue(name='queue2', services=services)\n        # Both queues share the same services\n\n\n3. Handle Service Errors\nImplement proper error handling for service-related exceptions:\nfrom naq.services.base import ServiceError, ServiceInitializationError\n\nasync def robust_service_usage():\n    config = get_config()\n    \n    try:\n        async with ServiceManager(config) as services:\n            connection_service = await services.get_service(ConnectionService)\n            # Use the service\n    except ServiceInitializationError as e:\n        logger.error(f\"Failed to initialize services: {e}\")\n        # Handle initialization failure\n    except ServiceError as e:\n        logger.error(f\"Service error: {e}\")\n        # Handle service error\n\n\n4. Use Dependency Injection\nLeverage the service dependency system rather than manually creating service instances:\n# Good - using dependency injection\nasync def using_services():\n    async with ServiceManager(config) as services:\n        stream_service = await services.get_service(StreamService)\n        # StreamService automatically gets ConnectionService as dependency\n\n# Avoid - manual dependency management\nasync def manual_dependencies():\n    connection_service = ConnectionService(config)\n    await connection_service.initialize()\n    stream_service = StreamService(config, connection_service)\n    await stream_service.initialize()\n    try:\n        # Use the services\n    finally:\n        await stream_service.cleanup()\n        await connection_service.cleanup()\n\n\n5. Configure Services Properly\nProvide appropriate configuration for connection parameters, retry logic, and timeouts:\n# Good - comprehensive configuration\nconfig = {\n    'nats': {\n        'url': 'nats://localhost:4222',\n        'max_reconnect_attempts': 10,\n        'reconnect_delay': 2.0,\n        'connection_timeout': 30.0,\n        'ping_interval': 60,\n        'max_outstanding_pings': 3\n    }\n}\n\n# Avoid - minimal configuration\nconfig = {'nats': {'url': 'nats://localhost:4222'}}"
  },
  {
    "objectID": "api/services.html#migration-guide",
    "href": "api/services.html#migration-guide",
    "title": "Services API",
    "section": "Migration Guide",
    "text": "Migration Guide\n\nFrom Direct Connection Management\n\nBefore\nclass OldQueue:\n    def __init__(self, name, nats_url):\n        self.name = name\n        self.nats_url = nats_url\n        self._nc = None\n        self._js = None\n    \n    async def _get_js(self):\n        if self._js is None:\n            self._nc = await nats.connect(self.nats_url)\n            self._js = self._nc.jetstream()\n        return self._js\n    \n    async def close(self):\n        if self._nc:\n            await self._nc.close()\n\n\nAfter\nclass NewQueue:\n    def __init__(self, name, nats_url, services=None):\n        self.name = name\n        self.nats_url = nats_url\n        self._services = services or ServiceManager({'nats': {'url': nats_url}})\n        self._js = None\n    \n    async def _get_js(self):\n        if self._js is None:\n            connection_service = await self._services.get_service(ConnectionService)\n            self._js = await connection_service.get_jetstream(self.nats_url)\n        return self._js\n    \n    async def close(self):\n        await self._services.cleanup_all()\n\n\n\nFrom Manual Service Creation\n\nBefore\nasync def old_approach():\n    # Create services manually\n    connection_service = ConnectionService(config)\n    await connection_service.initialize()\n    \n    stream_service = StreamService(config, connection_service)\n    await stream_service.initialize()\n    \n    try:\n        # Use services\n        pass\n    finally:\n        await stream_service.cleanup()\n        await connection_service.cleanup()\n\n\nAfter\nasync def new_approach():\n    # Use ServiceManager\n    async with ServiceManager(config) as services:\n        # Services are created and managed automatically\n        connection_service = await services.get_service(ConnectionService)\n        stream_service = await services.get_service(StreamService)\n        \n        # Use services\n        pass\n        # Services are automatically cleaned up\nThe services package provides a robust foundation for building applications with naq, eliminating connection duplication and providing clean, maintainable resource management throughout the system."
  },
  {
    "objectID": "advanced.html",
    "href": "advanced.html",
    "title": "Advanced Usage",
    "section": "",
    "text": "This section covers advanced features and configuration options for optimizing naq in production environments."
  },
  {
    "objectID": "advanced.html#efficient-connection-handling-batching",
    "href": "advanced.html#efficient-connection-handling-batching",
    "title": "Advanced Usage",
    "section": "Efficient Connection Handling & Batching",
    "text": "Efficient Connection Handling & Batching\nWhen enqueuing many jobs in a tight loop, creating a new NATS connection for each job is inefficient. naq provides several ways to manage connections for high-throughput scenarios.\n\nUsing a Queue Instance (Async)\nFor asynchronous applications, the most efficient way to enqueue jobs is to instantiate a Queue object and reuse it. The Queue instance manages a persistent connection to NATS.\n# async_batch_enqueue.py\nimport asyncio\nfrom naq.queue import Queue\n\nasync def my_task(i):\n    return f\"Processed item {i}\"\n\nasync def main():\n    # Create a single Queue instance for the 'high_volume' queue\n    queue = Queue(name=\"high_volume\")\n\n    print(\"Enqueuing 1,000 jobs using a single connection...\")\n    tasks = []\n    for i in range(1000):\n        task = queue.enqueue(my_task, i)\n        tasks.append(task)\n\n    await asyncio.gather(*tasks)\n    print(\"All jobs enqueued.\")\n\n    # The connection remains open until the Queue object is no longer in use\n    # or explicitly closed.\n    await queue.close()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\n\nThread-Local Connections (Sync)\nThe synchronous helper functions (enqueue_sync, enqueue_at_sync, etc.) are optimized for batching out of the box. They automatically use a thread-local NATS connection. This means that all calls to these functions from the same thread will reuse the same connection, avoiding the overhead of reconnecting each time.\n# sync_batch_enqueue.py\nfrom naq import enqueue_sync, close_sync_connections\n\ndef my_task(i):\n    return f\"Processed item {i}\"\n\ndef main():\n    print(\"Enqueuing 1,000 jobs using a thread-local connection...\")\n    for i in range(1000):\n        enqueue_sync(my_task, i)\n    print(\"All jobs enqueued.\")\n\n    # Optionally, you can explicitly close the thread-local connection\n    # when you are done with a batch. This is not required, as connections\n    # are also closed on process exit.\n    close_sync_connections()\n    print(\"Thread-local connection closed.\")\n\nif __name__ == \"__main__\":\n    main()"
  },
  {
    "objectID": "advanced.html#configuration-via-environment-variables",
    "href": "advanced.html#configuration-via-environment-variables",
    "title": "Advanced Usage",
    "section": "Configuration via Environment Variables",
    "text": "Configuration via Environment Variables\nMany of naq’s settings can be configured using environment variables, which is ideal for production and containerized deployments.\n\n\n\n\n\n\n\n\nVariable\nDefault\nDescription\n\n\n\n\nNAQ_NATS_URL\nnats://localhost:4222\nThe URL of the NATS server.\n\n\nNAQ_DEFAULT_QUEUE\nnaq_default_queue\nThe default queue name used when none is specified.\n\n\nNAQ_JOB_SERIALIZER\npickle\nThe serializer for jobs. Can be pickle or json. See security note below.\n\n\nNAQ_DEFAULT_RESULT_TTL\n604800 (7 days)\nDefault time-to-live (in seconds) for job results stored in NATS.\n\n\nNAQ_SCHEDULER_LOCK_TTL\n30\nTTL (in seconds) for the scheduler’s high-availability leader lock.\n\n\nNAQ_WORKER_TTL\n60\nTTL (in seconds) for a worker’s heartbeat. If a worker is silent for this long, it’s considered dead.\n\n\nNAQ_WORKER_HEARTBEAT_INTERVAL\n15\nHow often (in seconds) a worker sends a heartbeat to NATS.\n\n\nNAQ_LOG_LEVEL\nCRITICAL\nThe logging level for naq components. Can be DEBUG, INFO, WARNING, ERROR."
  },
  {
    "objectID": "advanced.html#job-serialization-pickle-vs.-json",
    "href": "advanced.html#job-serialization-pickle-vs.-json",
    "title": "Advanced Usage",
    "section": "Job Serialization (pickle vs. json)",
    "text": "Job Serialization (pickle vs. json)\nnaq uses a serializer to convert job data (the function and its arguments) into a format that can be stored in NATS. You can choose between two built-in serializers.\n\npickle (Default)\n\nPros: Can serialize almost any Python object, including complex custom classes, lambdas, and functions defined in a REPL.\nCons: Not secure. A malicious actor who can enqueue jobs could craft a pickle payload that executes arbitrary code on your workers.\n\n\n\njson (Recommended for Production)\n\nPros: Secure. Only serializes basic data types (strings, numbers, lists, dicts). Functions are referenced by their import path (e.g., my_app.tasks.process_data), not serialized directly. This prevents arbitrary code execution.\nCons: Less flexible. Cannot serialize complex Python objects that don’t have a natural JSON representation.\n\nTo use the json serializer, set the following environment variable:\nexport NAQ_JOB_SERIALIZER=json\n\n\n\n\n\n\nWarning\n\n\n\nSecurity Warning\nIt is strongly recommended to use the json serializer in any environment where the job producer is not fully trusted."
  },
  {
    "objectID": "advanced.html#event-logging-monitoring",
    "href": "advanced.html#event-logging-monitoring",
    "title": "Advanced Usage",
    "section": "Event Logging & Monitoring",
    "text": "Event Logging & Monitoring\nnaq includes a powerful, built-in event logging and monitoring system that provides deep visibility into your job queue. This system captures all key events throughout a job’s lifecycle—from enqueuing to completion or failure—and stores them durably in NATS JetStream.\n\nKey Concepts\nThe event system is composed of a few core components:\n\nJobEvent: A structured data object representing a single event in a job’s lifecycle (e.g., ENQUEUED, STARTED, FAILED).\nAsyncJobEventLogger: A high-performance, non-blocking logger that buffers events and flushes them to NATS in batches. This is integrated into the core naq components (Queue, Worker, Scheduler).\nNATSJobEventStorage: The storage backend that persists events to a dedicated NATS JetStream stream.\nAsyncJobEventProcessor: A real-time event processor that allows you to subscribe to event streams and react to events as they happen.\nSharedEventLoggerManager: A singleton manager that provides centralized event logging across all components.\n\n\n\nEnabling Event Logging\nEvent logging is controlled by the NAQ_EVENTS_ENABLED environment variable. By default, it is disabled.\n# Enable event logging\nexport NAQ_EVENTS_ENABLED=true\nOnce enabled, naq workers and queues will automatically start logging events without any code changes.\n\n\nConfiguration\nThe event system is highly configurable through environment variables:\n\n\n\n\n\n\n\n\nVariable\nDefault\nDescription\n\n\n\n\nNAQ_EVENTS_ENABLED\nFalse\nWhether to enable the event logging system.\n\n\nNAQ_EVENT_STORAGE_TYPE\nnats\nThe type of storage backend (currently only nats is supported).\n\n\nNAQ_EVENT_STORAGE_URL\nnats://localhost:4222\nThe URL of the NATS server for event storage.\n\n\nNAQ_EVENT_STREAM_NAME\nNAQ_JOB_EVENTS\nThe name of the JetStream stream where events are stored.\n\n\nNAQ_EVENT_SUBJECT_PREFIX\nnaq.jobs.events\nThe base subject prefix for publishing events.\n\n\n\n\n\nMonitoring Events in Real-Time\nnaq provides a built-in CLI command to monitor events in real-time. This is incredibly useful for debugging and observing the behavior of your queues during development and in production.\nTo start monitoring, simply run:\nnaq events\nThis command will start an AsyncJobEventProcessor with a default handler that prints a formatted, colorized representation of each event to your console. You will see output like this:\n12:34:56.789 [ENQUEUED]   job-abc-123 on queue 'default'\n12:34:56.790 [STARTED]    job-abc-123 by worker 'worker-1'\n12:35:01.123 [COMPLETED]  job-abc-123 by worker 'worker-1' (duration: 4333ms)\n\n\nBuilding Reactive Applications with Event Handlers\nThe real power of the event system lies in its ability to let you build reactive, event-driven applications. You can register custom handlers to react to specific job events.\nHere’s an example of how to set up a custom event processor to send a Slack notification whenever a job fails:\n# custom_handler.py\nimport asyncio\nfrom naq.events import AsyncJobEventProcessor, NATSJobEventStorage, JobEventType\n\nasync def send_failure_alert(event: naq.events.JobEvent):\n    \"\"\"Sends a Slack alert when a job fails.\"\"\"\n    print(f\"ALERT: Job {event.job_id} failed!\")\n    print(f\"  - Error: {event.error_message}\")\n    print(f\"  - Queue: {event.queue_name}\")\n    print(f\"  - Worker: {event.worker_id}\")\n    # In a real application, you would integrate with the Slack API here.\n    # await slack_client.post_message(channel=\"#alerts\", text=f\"Job {event.job_id} failed!\")\n\nasync def main():\n    # Create the storage and processor\n    storage = NATSJobEventStorage()\n    processor = AsyncJobEventProcessor(storage)\n\n    # Register our custom handler for FAILED events\n    processor.add_handler(JobEventType.FAILED, send_failure_alert)\n\n    # Start the processor\n    await processor.start()\n\n    try:\n        # Keep the processor running indefinitely\n        while True:\n            await asyncio.sleep(1)\n    finally:\n        # Ensure the processor is stopped cleanly\n        await processor.stop()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nYou would run this script alongside your workers and other naq components. It will connect to the same NATS server and receive a copy of all FAILED events, allowing you to trigger custom logic.\n\n\nEvent Data Structure\nEach JobEvent contains rich contextual information. Here’s a breakdown of the data available for a COMPLETED event:\n{\n  \"job_id\": \"job-xyz-789\",\n  \"event_type\": \"completed\",\n  \"timestamp\": 1678886400.123,\n  \"worker_id\": \"worker-2\",\n  \"queue_name\": \"high_priority\",\n  \"message\": \"Job completed successfully\",\n  \"details\": {\n    \"result_type\": \"str\"\n  },\n  \"error_type\": null,\n  \"error_message\": null,\n  \"duration_ms\": 2500,\n  \"nats_subject\": \"naq.jobs.events.job-xyz-789.worker.worker-2.completed\",\n  \"nats_sequence\": 12345\n}\nThis structured data makes it easy to build complex monitoring, analytics, and alerting systems on top of naq’s event stream."
  },
  {
    "objectID": "advanced.html#building-event-driven-applications-with-naq",
    "href": "advanced.html#building-event-driven-applications-with-naq",
    "title": "Advanced Usage",
    "section": "Building Event-Driven Applications with naq",
    "text": "Building Event-Driven Applications with naq\nThe event-driven architecture of naq enables you to build reactive applications that respond to job lifecycle events in real-time. This section covers how to leverage the event system for building sophisticated, event-driven applications.\n\nCore Principles\nEvent-driven applications built with naq follow these principles:\n\nReactive Processing: Your application reacts to events as they occur rather than polling for state changes.\nDecoupled Components: Event producers and consumers are completely decoupled, communicating only through the event stream.\nEvent Correlation: All events for a job can be correlated using the job ID, enabling complete lifecycle tracking.\nReal-time Processing: Events are processed as they arrive, enabling immediate responses to state changes.\n\n\n\nSetting Up an Event-Driven Application\n# event_driven_app.py\nimport asyncio\nfrom naq.events import (\n    AsyncJobEventProcessor,\n    NATSJobEventStorage,\n    JobEventType,\n    configure_shared_logger\n)\n\nasync def handle_job_events():\n    \"\"\"Main event processing function.\"\"\"\n    # Configure event logging\n    configure_shared_logger(enabled=True)\n    \n    # Create storage and processor\n    storage = NATSJobEventStorage()\n    processor = AsyncJobEventProcessor(storage)\n    \n    # Register event handlers\n    processor.add_handler(JobEventType.COMPLETED, handle_job_completed)\n    processor.add_handler(JobEventType.FAILED, handle_job_failed)\n    processor.add_handler(JobEventType.STATUS_CHANGED, handle_status_change)\n    processor.add_global_handler(log_all_events)\n    \n    # Start processing events\n    await processor.start()\n    \n    try:\n        # Keep the application running\n        while True:\n            await asyncio.sleep(1)\n    finally:\n        await processor.stop()\n\nasync def handle_job_completed(event):\n    \"\"\"Handle job completion events.\"\"\"\n    print(f\"Job {event.job_id} completed successfully\")\n    \n    # Update application state\n    await update_job_status(event.job_id, \"completed\")\n    \n    # Trigger post-processing tasks\n    if event.details.get(\"requires_post_processing\"):\n        await enqueue_post_processing(event.job_id)\n\nasync def handle_job_failed(event):\n    \"\"\"Handle job failure events.\"\"\"\n    print(f\"Job {event.job_id} failed: {event.error_message}\")\n    \n    # Update application state\n    await update_job_status(event.job_id, \"failed\")\n    \n    # Send alert if critical\n    if event.queue_name == \"critical\":\n        await send_failure_alert(event)\n    \n    # Log failure for analysis\n    await log_failure_for_analysis(event)\n\nasync def handle_status_change(event):\n    \"\"\"Handle job status change events.\"\"\"\n    old_status = event.details.get(\"old_status\")\n    new_status = event.details.get(\"new_status\")\n    \n    print(f\"Job {event.job_id} status changed: {old_status} -&gt; {new_status}\")\n    \n    # Update dashboards\n    await update_dashboard_metrics(event)\n    \n    # Check for specific transitions\n    if old_status == \"running\" and new_status == \"failed\":\n        await handle_runtime_failure(event)\n\nasync def log_all_events(event):\n    \"\"\"Log all events for auditing.\"\"\"\n    await persist_event_to_database(event)\n\nif __name__ == \"__main__\":\n    asyncio.run(handle_job_events())\n\n\nBest Practices for Event Logging and Monitoring\n\n1. Use Structured Event Details\nAlways include structured data in event details to enable better filtering and processing:\n# Good - structured details\nevent.details = {\n    \"user_id\": \"user-123\",\n    \"priority\": \"high\",\n    \"retry_count\": 2,\n    \"custom_metadata\": {\n        \"source\": \"api\",\n        \"version\": \"1.0\"\n    }\n}\n\n# Avoid - unstructured details\nevent.details = \"Job failed, will retry\"\n\n\n2. Implement Proper Error Handling\nEvent handlers should be resilient to errors:\nasync def robust_event_handler(event):\n    \"\"\"Robust event handler with proper error handling.\"\"\"\n    try:\n        # Process the event\n        await process_event(event)\n    except Exception as e:\n        # Log the error but don't crash\n        print(f\"Error processing event {event.job_id}: {e}\")\n        \n        # Optionally send to dead-letter queue\n        await send_to_dead_letter_queue(event, e)\n\n\n3. Use Selective Event Processing\nOnly process the events you need to avoid unnecessary overhead:\n# Good - specific event types\nprocessor.add_handler(JobEventType.COMPLETED, handle_completion)\nprocessor.add_handler(JobEventType.FAILED, handle_failure)\n\n# Avoid - processing all events when not needed\nprocessor.add_global_handler(handle_all_events)  # Only if truly needed\n\n\n4. Implement Event Batching for High Volume\nFor high-volume event processing, implement batching:\nclass BatchEventProcessor:\n    def __init__(self, batch_size=100, flush_interval=5.0):\n        self.batch = []\n        self.batch_size = batch_size\n        self.flush_interval = flush_interval\n        self.last_flush = time.time()\n    \n    async def process_event(self, event):\n        \"\"\"Process events in batches.\"\"\"\n        self.batch.append(event)\n        \n        # Flush if batch is full or interval elapsed\n        if (len(self.batch) &gt;= self.batch_size or\n            time.time() - self.last_flush &gt;= self.flush_interval):\n            await self.flush_batch()\n    \n    async def flush_batch(self):\n        \"\"\"Flush the current batch of events.\"\"\"\n        if not self.batch:\n            return\n            \n        # Process the batch\n        await process_event_batch(self.batch)\n        \n        # Reset batch\n        self.batch = []\n        self.last_flush = time.time()\n\n\n\nExtending the Event System for Custom Use Cases\n\nCustom Event Types\nYou can extend the event system with custom event types:\nfrom naq.models import JobEvent, JobEventType\n\n# Add custom event type\nclass CustomJobEventType(str, Enum):\n    CUSTOM_EVENT = \"custom_event\"\n    BUSINESS_LOGIC_COMPLETED = \"business_logic_completed\"\n\n# Create custom event\ncustom_event = JobEvent(\n    job_id=\"job-123\",\n    event_type=CustomJobEventType.CUSTOM_EVENT,\n    queue_name=\"custom_queue\",\n    details={\n        \"custom_field\": \"custom_value\",\n        \"business_metrics\": {\"metric1\": 100, \"metric2\": 200}\n    }\n)\n\n# Log the custom event\nlogger = get_shared_sync_logger()\nif logger:\n    logger.log_event(custom_event)\n\n\nCustom Event Storage Backends\nYou can implement custom storage backends by extending BaseEventStorage:\nfrom naq.events.storage import BaseEventStorage\n\nclass CustomEventStorage(BaseEventStorage):\n    \"\"\"Custom event storage backend.\"\"\"\n    \n    async def store_event(self, event: JobEvent) -&gt; None:\n        \"\"\"Store event in custom backend.\"\"\"\n        # Implement custom storage logic\n        await self._store_in_custom_backend(event)\n    \n    async def get_events(self, job_id: str) -&gt; List[JobEvent]:\n        \"\"\"Retrieve events for a job.\"\"\"\n        # Implement custom retrieval logic\n        return await self._get_from_custom_backend(job_id)\n    \n    async def stream_events(self, job_id: str, **kwargs) -&gt; AsyncIterator[JobEvent]:\n        \"\"\"Stream events in real-time.\"\"\"\n        # Implement custom streaming logic\n        async for event in self._stream_from_custom_backend(job_id, **kwargs):\n            yield event\n\n\nCustom Event Processors\nYou can create specialized event processors for specific use cases:\nclass AnalyticsEventProcessor:\n    \"\"\"Event processor for analytics and metrics.\"\"\"\n    \n    def __init__(self, storage):\n        self.storage = storage\n        self.metrics = {}\n    \n    async def start(self):\n        \"\"\"Start processing events for analytics.\"\"\"\n        async for event in self.storage.stream_events(job_id=\"*\"):\n            await self.update_metrics(event)\n    \n    async def update_metrics(self, event):\n        \"\"\"Update analytics metrics based on events.\"\"\"\n        # Update queue metrics\n        queue = event.queue_name or \"unknown\"\n        if queue not in self.metrics:\n            self.metrics[queue] = {\n                \"total\": 0,\n                \"completed\": 0,\n                \"failed\": 0,\n                \"avg_duration\": 0\n            }\n        \n        self.metrics[queue][\"total\"] += 1\n        \n        if event.event_type == JobEventType.COMPLETED:\n            self.metrics[queue][\"completed\"] += 1\n            if event.duration_ms:\n                # Update average duration\n                current_avg = self.metrics[queue][\"avg_duration\"]\n                completed = self.metrics[queue][\"completed\"]\n                self.metrics[queue][\"avg_duration\"] = (\n                    (current_avg * (completed - 1) + event.duration_ms) / completed\n                )\n        elif event.event_type == JobEventType.FAILED:\n            self.metrics[queue][\"failed\"] += 1\n\n\n\nPerformance Considerations for Event Logging\n\n1. Buffering and Batching\nThe shared event logger automatically handles buffering and batching, but you can optimize it for your use case:\n# Configure for high-throughput scenarios\nconfigure_shared_logger(\n    enabled=True,\n    batch_size=500,  # Larger batch size\n    flush_interval=1.0,  # More frequent flushes\n    max_buffer_size=50000  # Larger buffer\n)\n\n# Configure for low-latency scenarios\nconfigure_shared_logger(\n    enabled=True,\n    batch_size=10,  # Smaller batch size\n    flush_interval=0.1,  # Very frequent flushes\n    max_buffer_size=1000  # Smaller buffer\n)\n\n\n2. Connection Management\nAlways use the shared event logger to avoid connection overhead:\n# Good - using shared logger\nlogger = get_shared_sync_logger()\nif logger:\n    logger.log_job_completed(job_id=\"job-123\", worker_id=\"worker-1\", duration_ms=1000)\n\n# Avoid - creating new loggers\nfrom naq.events import JobEventLogger\nstorage = NATSJobEventStorage()\nlogger = JobEventLogger(storage)  # Creates new connection\n\n\n3. Event Filtering\nFilter events at the source to reduce processing overhead:\n# Good - filter by job_id\nasync for event in storage.stream_events(job_id=\"specific-job\"):\n    await process_event(event)\n\n# Good - filter by event type\nasync for event in storage.stream_events(job_id=\"*\", event_type=JobEventType.COMPLETED):\n    await process_event(event)\n\n# Avoid - processing all events and filtering manually\nasync for event in storage.stream_events(job_id=\"*\"):\n    if event.event_type == JobEventType.COMPLETED:\n        await process_event(event)\n\n\n4. Memory Management\nBe mindful of memory usage when processing large volumes of events:\nclass MemoryEfficientProcessor:\n    def __init__(self):\n        self.recent_events = {}\n        self.max_events_per_job = 100\n    \n    async def process_event(self, event):\n        \"\"\"Process events with memory constraints.\"\"\"\n        job_id = event.job_id\n        \n        # Initialize job event list if needed\n        if job_id not in self.recent_events:\n            self.recent_events[job_id] = []\n        \n        # Add event\n        self.recent_events[job_id].append(event)\n        \n        # Prune old events if limit exceeded\n        if len(self.recent_events[job_id]) &gt; self.max_events_per_job:\n            self.recent_events[job_id] = self.recent_events[job_id][-self.max_events_per_job:]\n        \n        # Process the event\n        await self._process_single_event(event)\nBy following these best practices and patterns, you can build efficient, scalable event-driven applications with naq that leverage the full power of the event system."
  },
  {
    "objectID": "advanced.html#event-driven-architecture-patterns",
    "href": "advanced.html#event-driven-architecture-patterns",
    "title": "Advanced Usage",
    "section": "Event-Driven Architecture Patterns",
    "text": "Event-Driven Architecture Patterns\nThe event-driven architecture of naq enables the implementation of sophisticated patterns for workflow orchestration, monitoring, and system integration. This section explores advanced patterns and techniques for building robust event-driven systems.\n\nEvent-Driven Architecture Patterns\n\n1. Event Sourcing Pattern\nEvent sourcing is a pattern where state changes are captured as a sequence of events. naq’s event system naturally supports this pattern:\nclass JobEventSourcing:\n    \"\"\"Event sourcing implementation for job state management.\"\"\"\n    \n    def __init__(self, storage):\n        self.storage = storage\n    \n    async def get_job_state(self, job_id: str) -&gt; dict:\n        \"\"\"Reconstruct job state from events.\"\"\"\n        events = await self.storage.get_events(job_id)\n        \n        state = {\n            \"job_id\": job_id,\n            \"status\": \"unknown\",\n            \"start_time\": None,\n            \"end_time\": None,\n            \"error\": None,\n            \"worker_id\": None,\n            \"queue_name\": None\n        }\n        \n        for event in events:\n            self._apply_event_to_state(state, event)\n        \n        return state\n    \n    def _apply_event_to_state(self, state: dict, event: JobEvent):\n        \"\"\"Apply a single event to the state.\"\"\"\n        if event.event_type == JobEventType.ENQUEUED:\n            state[\"status\"] = \"enqueued\"\n            state[\"queue_name\"] = event.queue_name\n        elif event.event_type == JobEventType.STARTED:\n            state[\"status\"] = \"running\"\n            state[\"worker_id\"] = event.worker_id\n            state[\"start_time\"] = event.timestamp\n        elif event.event_type == JobEventType.COMPLETED:\n            state[\"status\"] = \"completed\"\n            state[\"end_time\"] = event.timestamp\n        elif event.event_type == JobEventType.FAILED:\n            state[\"status\"] = \"failed\"\n            state[\"error\"] = event.error_message\n            state[\"end_time\"] = event.timestamp\n        elif event.event_type == JobEventType.CANCELLED:\n            state[\"status\"] = \"cancelled\"\n            state[\"end_time\"] = event.timestamp\n\n\n2. CQRS (Command Query Responsibility Segregation)\nCQRS separates read and write operations, using events to synchronize between them:\nclass JobReadModel:\n    \"\"\"Read model for job queries.\"\"\"\n    \n    def __init__(self):\n        self.jobs = {}  # job_id -&gt; job_data\n    \n    async def update_from_event(self, event: JobEvent):\n        \"\"\"Update read model from events.\"\"\"\n        if event.job_id not in self.jobs:\n            self.jobs[event.job_id] = {\n                \"id\": event.job_id,\n                \"status\": \"unknown\",\n                \"created_at\": event.timestamp,\n                \"updated_at\": event.timestamp,\n                \"events\": []\n            }\n        \n        job = self.jobs[event.job_id]\n        job[\"events\"].append(event)\n        job[\"updated_at\"] = event.timestamp\n        \n        # Update status based on event type\n        if event.event_type == JobEventType.ENQUEUED:\n            job[\"status\"] = \"enqueued\"\n            job[\"queue_name\"] = event.queue_name\n        elif event.event_type == JobEventType.STARTED:\n            job[\"status\"] = \"running\"\n            job[\"worker_id\"] = event.worker_id\n        elif event.event_type == JobEventType.COMPLETED:\n            job[\"status\"] = \"completed\"\n            job[\"duration_ms\"] = event.duration_ms\n        elif event.event_type == JobEventType.FAILED:\n            job[\"status\"] = \"failed\"\n            job[\"error\"] = event.error_message\n    \n    def get_job(self, job_id: str) -&gt; Optional[dict]:\n        \"\"\"Get job data from read model.\"\"\"\n        return self.jobs.get(job_id)\n    \n    def get_jobs_by_status(self, status: str) -&gt; List[dict]:\n        \"\"\"Get jobs by status.\"\"\"\n        return [job for job in self.jobs.values() if job[\"status\"] == status]\n\n\n\nEvent-Driven Workflow Orchestration\nUse events to orchestrate complex workflows with multiple steps:\nclass WorkflowOrchestrator:\n    \"\"\"Orchestrates complex workflows using events.\"\"\"\n    \n    def __init__(self, storage, queue):\n        self.storage = storage\n        self.queue = queue\n        self.workflows = {}  # workflow_id -&gt; workflow_state\n    \n    async def start_workflow(self, workflow_id: str, initial_job_data: dict):\n        \"\"\"Start a new workflow.\"\"\"\n        workflow_state = {\n            \"id\": workflow_id,\n            \"status\": \"started\",\n            \"current_step\": 0,\n            \"steps\": [\n                {\"name\": \"data_processing\", \"status\": \"pending\"},\n                {\"name\": \"validation\", \"status\": \"pending\"},\n                {\"name\": \"enrichment\", \"status\": \"pending\"},\n                {\"name\": \"notification\", \"status\": \"pending\"}\n            ],\n            \"job_ids\": {}\n        }\n        \n        self.workflows[workflow_id] = workflow_state\n        \n        # Enqueue first step\n        await self._enqueue_workflow_step(workflow_id, 0, initial_job_data)\n    \n    async def handle_job_event(self, event: JobEvent):\n        \"\"\"Handle job events to advance workflow.\"\"\"\n        # Find workflow that contains this job\n        for workflow_id, workflow in self.workflows.items():\n            if event.job_id in workflow[\"job_ids\"].values():\n                await self._advance_workflow(workflow_id, event)\n                break\n    \n    async def _advance_workflow(self, workflow_id: str, event: JobEvent):\n        \"\"\"Advance workflow based on job completion.\"\"\"\n        workflow = self.workflows[workflow_id]\n        \n        if event.event_type == JobEventType.COMPLETED:\n            # Find which step this job belongs to\n            for i, step in enumerate(workflow[\"steps\"]):\n                if step.get(\"job_id\") == event.job_id:\n                    step[\"status\"] = \"completed\"\n                    \n                    # Enqueue next step\n                    if i + 1 &lt; len(workflow[\"steps\"]):\n                        await self._enqueue_workflow_step(\n                            workflow_id,\n                            i + 1,\n                            {\"previous_result\": event.details}\n                        )\n                    else:\n                        # Workflow completed\n                        workflow[\"status\"] = \"completed\"\n                    break\n        \n        elif event.event_type == JobEventType.FAILED:\n            # Handle workflow failure\n            workflow[\"status\"] = \"failed\"\n            workflow[\"error\"] = event.error_message\n    \n    async def _enqueue_workflow_step(self, workflow_id: str, step_index: int, data: dict):\n        \"\"\"Enqueue a specific workflow step.\"\"\"\n        workflow = self.workflows[workflow_id]\n        step = workflow[\"steps\"][step_index]\n        \n        # Enqueue job for this step\n        job = await self.queue.enqueue(\n            workflow_step_function,\n            step_name=step[\"name\"],\n            workflow_id=workflow_id,\n            data=data\n        )\n        \n        step[\"status\"] = \"running\"\n        step[\"job_id\"] = job.job_id\n        workflow[\"job_ids\"][step[\"name\"]] = job.job_id\n\n\nEvent Correlation and Tracing\n\n1. Distributed Tracing\nImplement distributed tracing to track requests across multiple services:\nclass EventTracer:\n    \"\"\"Distributed tracing for job events.\"\"\"\n    \n    def __init__(self):\n        self.traces = {}  # trace_id -&gt; trace_data\n    \n    def start_trace(self, trace_id: str, job_id: str, operation: str) -&gt; str:\n        \"\"\"Start a new trace.\"\"\"\n        span_id = f\"{trace_id}-{uuid.uuid4().hex[:8]}\"\n        \n        if trace_id not in self.traces:\n            self.traces[trace_id] = {\n                \"id\": trace_id,\n                \"root_job_id\": job_id,\n                \"spans\": [],\n                \"start_time\": time.time()\n            }\n        \n        self.traces[trace_id][\"spans\"].append({\n            \"id\": span_id,\n            \"job_id\": job_id,\n            \"operation\": operation,\n            \"start_time\": time.time(),\n            \"status\": \"running\"\n        })\n        \n        return span_id\n    \n    def finish_span(self, trace_id: str, span_id: str, status: str = \"completed\"):\n        \"\"\"Finish a span.\"\"\"\n        if trace_id in self.traces:\n            for span in self.traces[trace_id][\"spans\"]:\n                if span[\"id\"] == span_id:\n                    span[\"end_time\"] = time.time()\n                    span[\"duration_ms\"] = (span[\"end_time\"] - span[\"start_time\"]) * 1000\n                    span[\"status\"] = status\n                    break\n    \n    def get_trace(self, trace_id: str) -&gt; Optional[dict]:\n        \"\"\"Get trace data.\"\"\"\n        return self.traces.get(trace_id)\n    \n    async def trace_job_event(self, event: JobEvent):\n        \"\"\"Trace job events with correlation.\"\"\"\n        # Extract trace context from event details\n        trace_id = event.details.get(\"trace_id\")\n        span_id = event.details.get(\"span_id\")\n        \n        if trace_id and span_id:\n            if event.event_type in [JobEventType.STARTED]:\n                self.start_trace(trace_id, event.job_id, event.queue_name or \"unknown\")\n            elif event.event_type in [JobEventType.COMPLETED, JobEventType.FAILED]:\n                status = \"completed\" if event.event_type == JobEventType.COMPLETED else \"failed\"\n                self.finish_span(trace_id, span_id, status)\n\n\n2. Event Correlation\nCorrelate related events across different jobs and workflows:\nclass EventCorrelator:\n    \"\"\"Correlates related events across jobs and workflows.\"\"\"\n    \n    def __init__(self):\n        self.correlations = {}  # correlation_id -&gt; related_events\n        self.event_index = {}   # job_id -&gt; correlation_ids\n    \n    def create_correlation(self, correlation_id: str, job_ids: List[str]):\n        \"\"\"Create a correlation between multiple jobs.\"\"\"\n        self.correlations[correlation_id] = {\n            \"id\": correlation_id,\n            \"job_ids\": job_ids,\n            \"events\": [],\n            \"created_at\": time.time()\n        }\n        \n        # Index jobs by correlation\n        for job_id in job_ids:\n            if job_id not in self.event_index:\n                self.event_index[job_id] = []\n            self.event_index[job_id].append(correlation_id)\n    \n    async def add_event(self, event: JobEvent):\n        \"\"\"Add an event to relevant correlations.\"\"\"\n        if event.job_id in self.event_index:\n            for correlation_id in self.event_index[event.job_id]:\n                if correlation_id in self.correlations:\n                    self.correlations[correlation_id][\"events\"].append(event)\n    \n    def get_correlation(self, correlation_id: str) -&gt; Optional[dict]:\n        \"\"\"Get correlation data.\"\"\"\n        return self.correlations.get(correlation_id)\n    \n    def get_correlated_jobs(self, job_id: str) -&gt; List[str]:\n        \"\"\"Get all jobs correlated with the given job.\"\"\"\n        correlated_jobs = []\n        \n        if job_id in self.event_index:\n            for correlation_id in self.event_index[job_id]:\n                correlation = self.correlations.get(correlation_id)\n                if correlation:\n                    correlated_jobs.extend(correlation[\"job_ids\"])\n        \n        # Remove the original job from the list\n        return [job for job in correlated_jobs if job != job_id]\n\n\n\nBuilding Monitoring and Alerting on Events\n\n1. Real-time Dashboard\nBuild a real-time dashboard that updates based on events:\nclass DashboardMetrics:\n    \"\"\"Real-time dashboard metrics based on events.\"\"\"\n    \n    def __init__(self):\n        self.metrics = {\n            \"total_jobs\": 0,\n            \"completed_jobs\": 0,\n            \"failed_jobs\": 0,\n            \"running_jobs\": 0,\n            \"queue_sizes\": {},\n            \"average_duration\": {},\n            \"error_rates\": {}\n        }\n        self.recent_events = []\n    \n    async def update_from_event(self, event: JobEvent):\n        \"\"\"Update metrics based on events.\"\"\"\n        self.recent_events.append(event)\n        \n        # Keep only recent events (last 1000)\n        if len(self.recent_events) &gt; 1000:\n            self.recent_events = self.recent_events[-1000:]\n        \n        # Update queue metrics\n        queue = event.queue_name or \"unknown\"\n        if queue not in self.metrics[\"queue_sizes\"]:\n            self.metrics[\"queue_sizes\"][queue] = 0\n            self.metrics[\"average_duration\"][queue] = 0\n            self.metrics[\"error_rates\"][queue] = 0\n        \n        # Update based on event type\n        if event.event_type == JobEventType.ENQUEUED:\n            self.metrics[\"total_jobs\"] += 1\n            self.metrics[\"queue_sizes\"][queue] += 1\n        elif event.event_type == JobEventType.STARTED:\n            self.metrics[\"running_jobs\"] += 1\n        elif event.event_type == JobEventType.COMPLETED:\n            self.metrics[\"completed_jobs\"] += 1\n            self.metrics[\"running_jobs\"] -= 1\n            self.metrics[\"queue_sizes\"][queue] -= 1\n            \n            # Update average duration\n            if event.duration_ms:\n                current_avg = self.metrics[\"average_duration\"][queue]\n                completed_count = self.metrics[\"completed_jobs\"]\n                self.metrics[\"average_duration\"][queue] = (\n                    (current_avg * (completed_count - 1) + event.duration_ms) / completed_count\n                )\n        elif event.event_type == JobEventType.FAILED:\n            self.metrics[\"failed_jobs\"] += 1\n            self.metrics[\"running_jobs\"] -= 1\n            self.metrics[\"queue_sizes\"][queue] -= 1\n            \n            # Update error rate\n            total_jobs = self.metrics[\"total_jobs\"]\n            if total_jobs &gt; 0:\n                self.metrics[\"error_rates\"][queue] = (\n                    self.metrics[\"failed_jobs\"] / total_jobs\n                ) * 100\n    \n    def get_dashboard_data(self) -&gt; dict:\n        \"\"\"Get current dashboard data.\"\"\"\n        return {\n            \"metrics\": self.metrics,\n            \"recent_events\": self.recent_events[-50:],  # Last 50 events\n            \"timestamp\": time.time()\n        }\n\n\n2. Alerting System\nImplement an alerting system based on event patterns:\nclass AlertingSystem:\n    \"\"\"Alerting system based on event patterns.\"\"\"\n    \n    def __init__(self):\n        self.alerts = []\n        self.rules = []\n        self.event_history = []\n    \n    def add_alert_rule(self, rule: dict):\n        \"\"\"Add an alert rule.\"\"\"\n        self.rules.append(rule)\n    \n    async def process_event(self, event: JobEvent):\n        \"\"\"Process event against alert rules.\"\"\"\n        self.event_history.append(event)\n        \n        # Keep history manageable\n        if len(self.event_history) &gt; 10000:\n            self.event_history = self.event_history[-5000:]\n        \n        # Check each rule\n        for rule in self.rules:\n            if await self._evaluate_rule(rule, event):\n                await self._trigger_alert(rule, event)\n    \n    async def _evaluate_rule(self, rule: dict, event: JobEvent) -&gt; bool:\n        \"\"\"Evaluate if an alert rule is triggered.\"\"\"\n        rule_type = rule.get(\"type\")\n        \n        if rule_type == \"threshold\":\n            return await self._evaluate_threshold_rule(rule, event)\n        elif rule_type == \"pattern\":\n            return await self._evaluate_pattern_rule(rule, event)\n        elif rule_type == \"rate\":\n            return await self._evaluate_rate_rule(rule, event)\n        \n        return False\n    \n    async def _evaluate_threshold_rule(self, rule: dict, event: JobEvent) -&gt; bool:\n        \"\"\"Evaluate threshold-based alert rule.\"\"\"\n        metric = rule.get(\"metric\")\n        threshold = rule.get(\"threshold\")\n        operator = rule.get(\"operator\", \"&gt;\")\n        \n        if metric == \"failure_rate\":\n            # Calculate failure rate for the queue\n            queue_events = [e for e in self.event_history\n                           if e.queue_name == event.queue_name]\n            if len(queue_events) &gt; 0:\n                failures = len([e for e in queue_events\n                              if e.event_type == JobEventType.FAILED])\n                rate = (failures / len(queue_events)) * 100\n                \n                if operator == \"&gt;\" and rate &gt; threshold:\n                    return True\n                elif operator == \"&lt;\" and rate &lt; threshold:\n                    return True\n        \n        return False\n    \n    async def _evaluate_pattern_rule(self, rule: dict, event: JobEvent) -&gt; bool:\n        \"\"\"Evaluate pattern-based alert rule.\"\"\"\n        pattern = rule.get(\"pattern\")\n        time_window = rule.get(\"time_window\", 300)  # 5 minutes\n        \n        # Look for patterns in recent events\n        recent_events = [e for e in self.event_history\n                        if time.time() - e.timestamp &lt;= time_window]\n        \n        if pattern == \"consecutive_failures\":\n            # Check for consecutive failures\n            consecutive_failures = 0\n            for e in reversed(recent_events):\n                if e.event_type == JobEventType.FAILED:\n                    consecutive_failures += 1\n                    if consecutive_failures &gt;= rule.get(\"count\", 3):\n                        return True\n                else:\n                    break\n        \n        return False\n    \n    async def _evaluate_rate_rule(self, rule: dict, event: JobEvent) -&gt; bool:\n        \"\"\"Evaluate rate-based alert rule.\"\"\"\n        metric = rule.get(\"metric\")\n        threshold = rule.get(\"threshold\")\n        time_window = rule.get(\"time_window\", 60)  # 1 minute\n        \n        # Count events in time window\n        recent_events = [e for e in self.event_history\n                        if time.time() - e.timestamp &lt;= time_window]\n        \n        if metric == \"events_per_minute\":\n            rate = len(recent_events) / (time_window / 60)\n            return rate &gt; threshold\n        \n        return False\n    \n    async def _trigger_alert(self, rule: dict, event: JobEvent):\n        \"\"\"Trigger an alert.\"\"\"\n        alert = {\n            \"id\": f\"alert-{uuid.uuid4().hex[:8]}\",\n            \"rule_id\": rule.get(\"id\"),\n            \"rule_name\": rule.get(\"name\"),\n            \"severity\": rule.get(\"severity\", \"medium\"),\n            \"message\": rule.get(\"message\", \"Alert triggered\"),\n            \"triggered_by\": event.job_id,\n            \"timestamp\": time.time(),\n            \"status\": \"active\"\n        }\n        \n        self.alerts.append(alert)\n        \n        # Send notification\n        await self._send_notification(alert)\n    \n    async def _send_notification(self, alert: dict):\n        \"\"\"Send alert notification.\"\"\"\n        # Implement notification logic (email, Slack, PagerDuty, etc.)\n        print(f\"ALERT: {alert['message']} (Severity: {alert['severity']})\")\n        \n        # In a real implementation, you would integrate with notification services\n        # await send_slack_notification(alert)\n        # await send_email_notification(alert)\n        # await send_pagerduty_notification(alert)\n\n\n3. Event-Driven Auto-scaling\nImplement auto-scaling based on event metrics:\nclass AutoScaler:\n    \"\"\"Auto-scaling based on event metrics.\"\"\"\n    \n    def __init__(self, min_workers=1, max_workers=10):\n        self.min_workers = min_workers\n        self.max_workers = max_workers\n        self.current_workers = min_workers\n        self.metrics_history = []\n    \n    async def process_event(self, event: JobEvent):\n        \"\"\"Process events for auto-scaling decisions.\"\"\"\n        # Update metrics\n        self._update_metrics(event)\n        \n        # Check scaling conditions every 10 events\n        if len(self.metrics_history) % 10 == 0:\n            await self._evaluate_scaling()\n    \n    def _update_metrics(self, event: JobEvent):\n        \"\"\"Update scaling metrics.\"\"\"\n        self.metrics_history.append({\n            \"timestamp\": time.time(),\n            \"event_type\": event.event_type,\n            \"queue_name\": event.queue_name,\n            \"duration_ms\": event.duration_ms\n        })\n        \n        # Keep only recent metrics (last hour)\n        one_hour_ago = time.time() - 3600\n        self.metrics_history = [\n            m for m in self.metrics_history\n            if m[\"timestamp\"] &gt; one_hour_ago\n        ]\n    \n    async def _evaluate_scaling(self):\n        \"\"\"Evaluate if scaling is needed.\"\"\"\n        if len(self.metrics_history) &lt; 10:\n            return\n        \n        # Calculate current metrics\n        recent_metrics = [m for m in self.metrics_history\n                         if time.time() - m[\"timestamp\"] &lt;= 300]  # Last 5 minutes\n        \n        # Calculate queue depth\n        enqueued = len([m for m in recent_metrics if m[\"event_type\"] == JobEventType.ENQUEUED])\n        processed = len([m for m in recent_metrics\n                        if m[\"event_type\"] in [JobEventType.COMPLETED, JobEventType.FAILED]])\n        queue_depth = enqueued - processed\n        \n        # Calculate average processing time\n        completed_events = [m for m in recent_metrics\n                           if m[\"event_type\"] == JobEventType.COMPLETED and m[\"duration_ms\"]]\n        avg_duration = sum(m[\"duration_ms\"] for m in completed_events) / len(completed_events) if completed_events else 0\n        \n        # Scaling decisions\n        if queue_depth &gt; 50 and avg_duration &gt; 5000:  # Scale up\n            await self._scale_up()\n        elif queue_depth &lt; 10 and avg_duration &lt; 1000:  # Scale down\n            await self._scale_down()\n    \n    async def _scale_up(self):\n        \"\"\"Scale up worker count.\"\"\"\n        if self.current_workers &lt; self.max_workers:\n            self.current_workers += 1\n            await self._start_worker()\n            print(f\"Scaled up to {self.current_workers} workers\")\n    \n    async def _scale_down(self):\n        \"\"\"Scale down worker count.\"\"\"\n        if self.current_workers &gt; self.min_workers:\n            self.current_workers -= 1\n            await self._stop_worker()\n            print(f\"Scaled down to {self.current_workers} workers\")\n    \n    async def _start_worker(self):\n        \"\"\"Start a new worker.\"\"\"\n        # Implement worker startup logic\n        pass\n    \n    async def _stop_worker(self):\n        \"\"\"Stop a worker.\"\"\"\n        # Implement worker shutdown logic\n        pass\nThese patterns and techniques demonstrate the power and flexibility of naq’s event-driven architecture. By combining these patterns, you can build sophisticated, resilient, and scalable systems that can handle complex business requirements while maintaining excellent performance and reliability."
  },
  {
    "objectID": "advanced.html#advanced-service-layer-usage",
    "href": "advanced.html#advanced-service-layer-usage",
    "title": "Advanced Usage",
    "section": "Advanced Service Layer Usage",
    "text": "Advanced Service Layer Usage\nThe service layer architecture in naq provides a powerful foundation for building advanced applications with efficient resource management and clean separation of concerns. This section explores advanced patterns and techniques for leveraging the service layer in production environments.\n\nCustom Service Implementation\nYou can extend the service layer by implementing custom services that integrate with the existing service ecosystem.\n\nCreating a Custom Service\nfrom typing import Dict, Any, Optional\nfrom naq.services.base import BaseService\nfrom naq.services import ServiceManager\n\nclass AnalyticsService(BaseService):\n    \"\"\"Custom service for collecting and analyzing job metrics.\"\"\"\n    \n    def __init__(self, config: Dict[str, Any]):\n        super().__init__(config)\n        self._metrics = {}\n        self._event_service = None\n        self._job_service = None\n    \n    async def _do_initialize(self) -&gt; None:\n        \"\"\"Initialize the analytics service.\"\"\"\n        # Initialize metrics storage\n        self._metrics = {\n            'queue_stats': {},\n            'job_timings': {},\n            'error_rates': {},\n            'throughput': {}\n        }\n        \n        # Get required services from ServiceManager\n        # Note: In a real implementation, these would be injected\n        logger.info(\"AnalyticsService initialized\")\n    \n    async def cleanup(self) -&gt; None:\n        \"\"\"Cleanup analytics service resources.\"\"\"\n        await super().cleanup()\n        # Persist metrics if needed\n        logger.info(\"AnalyticsService cleaned up\")\n    \n    async def record_job_timing(self, queue_name: str, duration_ms: float) -&gt; None:\n        \"\"\"Record job execution timing.\"\"\"\n        if queue_name not in self._metrics['job_timings']:\n            self._metrics['job_timings'][queue_name] = []\n        \n        self._metrics['job_timings'][queue_name].append(duration_ms)\n        \n        # Keep only recent timings (last 1000)\n        if len(self._metrics['job_timings'][queue_name]) &gt; 1000:\n            self._metrics['job_timings'][queue_name] = self._metrics['job_timings'][queue_name][-1000:]\n    \n    async def get_queue_stats(self, queue_name: str) -&gt; Dict[str, Any]:\n        \"\"\"Get statistics for a specific queue.\"\"\"\n        timings = self._metrics['job_timings'].get(queue_name, [])\n        \n        if not timings:\n            return {\n                'queue_name': queue_name,\n                'job_count': 0,\n                'avg_duration_ms': 0,\n                'min_duration_ms': 0,\n                'max_duration_ms': 0\n            }\n        \n        return {\n            'queue_name': queue_name,\n            'job_count': len(timings),\n            'avg_duration_ms': sum(timings) / len(timings),\n            'min_duration_ms': min(timings),\n            'max_duration_ms': max(timings)\n        }\n    \n    async def get_system_stats(self) -&gt; Dict[str, Any]:\n        \"\"\"Get system-wide statistics.\"\"\"\n        all_stats = {\n            'total_queues': len(self._metrics['job_timings']),\n            'total_jobs': sum(len(timings) for timings in self._metrics['job_timings'].values()),\n            'queue_stats': {}\n        }\n        \n        # Collect stats for each queue\n        for queue_name in self._metrics['job_timings']:\n            all_stats['queue_stats'][queue_name] = await self.get_queue_stats(queue_name)\n        \n        return all_stats\n\n\nRegistering Custom Services\nfrom naq.services import ServiceManager\n\nasync def custom_service_example():\n    \"\"\"Example of using a custom service with ServiceManager.\"\"\"\n    \n    # Create configuration\n    config = {\n        'nats': {\n            'url': 'nats://localhost:4222',\n            'max_reconnect_attempts': 5,\n            'reconnect_delay': 1.0\n        },\n        'analytics': {\n            'retention_period_hours': 24,\n            'metrics_batch_size': 100\n        }\n    }\n    \n    async with ServiceManager(config) as services:\n        # Manually create and register custom service\n        analytics_service = AnalyticsService(config.get('analytics', {}))\n        await analytics_service.initialize()\n        \n        # Use the service\n        await analytics_service.record_job_timing('high_priority', 150.5)\n        await analytics_service.record_job_timing('high_priority', 120.3)\n        \n        stats = await analytics_service.get_queue_stats('high_priority')\n        print(f\"Queue stats: {stats}\")\n        \n        # Cleanup\n        await analytics_service.cleanup()\n\n\n\nService Configuration Patterns\n\nEnvironment-Based Configuration\nimport os\nfrom typing import Dict, Any\n\ndef get_service_config() -&gt; Dict[str, Any]:\n    \"\"\"Build service configuration from environment variables.\"\"\"\n    config = {\n        'nats': {\n            'url': os.getenv('NAQ_NATS_URL', 'nats://localhost:4222'),\n            'max_reconnect_attempts': int(os.getenv('NAQ_MAX_RECONNECT', '5')),\n            'reconnect_delay': float(os.getenv('NAQ_RECONNECT_DELAY', '1.0')),\n            'connection_timeout': float(os.getenv('NAQ_CONNECTION_TIMEOUT', '10.0')),\n            'ping_interval': int(os.getenv('NAQ_PING_INTERVAL', '60')),\n            'max_outstanding_pings': int(os.getenv('NAQ_MAX_OUTSTANDING_PINGS', '2'))\n        },\n        'events': {\n            'enabled': os.getenv('NAQ_EVENTS_ENABLED', 'false').lower() == 'true',\n            'batch_size': int(os.getenv('NAQ_EVENT_BATCH_SIZE', '100')),\n            'flush_interval': float(os.getenv('NAQ_EVENT_FLUSH_INTERVAL', '1.0')),\n            'max_buffer_size': int(os.getenv('NAQ_EVENT_MAX_BUFFER', '10000'))\n        },\n        'jobs': {\n            'default_timeout': int(os.getenv('NAQ_DEFAULT_TIMEOUT', '3600')),\n            'result_ttl': int(os.getenv('NAQ_RESULT_TTL', '604800'))\n        }\n    }\n    \n    return config\n\nasync def environment_configured_example():\n    \"\"\"Example using environment-based configuration.\"\"\"\n    config = get_service_config()\n    \n    async with ServiceManager(config) as services:\n        queue = Queue(name='production', services=services)\n        # Queue will use environment-configured services\n        job = await queue.enqueue(process_data, data_item)\n\n\nMulti-Environment Configuration\ndef get_environment_config(env: str) -&gt; Dict[str, Any]:\n    \"\"\"Get configuration for different environments.\"\"\"\n    configs = {\n        'development': {\n            'nats': {\n                'url': 'nats://localhost:4222',\n                'max_reconnect_attempts': 3,\n                'reconnect_delay': 1.0\n            },\n            'events': {\n                'enabled': True,\n                'batch_size': 10,\n                'flush_interval': 0.5\n            }\n        },\n        'staging': {\n            'nats': {\n                'url': 'nats://staging.example.com:4222',\n                'max_reconnect_attempts': 5,\n                'reconnect_delay': 2.0\n            },\n            'events': {\n                'enabled': True,\n                'batch_size': 50,\n                'flush_interval': 1.0\n            }\n        },\n        'production': {\n            'nats': {\n                'url': 'nats://prod.example.com:4222',\n                'max_reconnect_attempts': 10,\n                'reconnect_delay': 5.0\n            },\n            'events': {\n                'enabled': True,\n                'batch_size': 100,\n                'flush_interval': 2.0\n            }\n        }\n    }\n    \n    return configs.get(env, configs['development'])\n\nasync def multi_environment_example():\n    \"\"\"Example of multi-environment configuration.\"\"\"\n    env = os.getenv('NAQ_ENVIRONMENT', 'development')\n    config = get_environment_config(env)\n    \n    async with ServiceManager(config) as services:\n        # Services configured for the specific environment\n        queue = Queue(name='app_queue', services=services)\n        # ...\n\n\n\nAdvanced Service Patterns\n\nService Inter-Communication\nclass MonitoringService(BaseService):\n    \"\"\"Service that monitors other services.\"\"\"\n    \n    def __init__(self, config: Dict[str, Any]):\n        super().__init__(config)\n        self._connection_service = None\n        self._event_service = None\n        self._health_checks = {}\n    \n    async def _do_initialize(self) -&gt; None:\n        \"\"\"Initialize monitoring service.\"\"\"\n        # Set up health checks for other services\n        self._health_checks = {\n            'connection': self._check_connection_health,\n            'events': self._check_event_health,\n            'jobs': self._check_job_health\n        }\n        \n        logger.info(\"MonitoringService initialized\")\n    \n    async def _check_connection_health(self) -&gt; Dict[str, Any]:\n        \"\"\"Check connection service health.\"\"\"\n        if not self._connection_service:\n            return {'status': 'error', 'message': 'Connection service not available'}\n        \n        try:\n            status = self._connection_service.get_connection_status()\n            healthy = all(conn.get('connected', False) for conn in status.values())\n            return {\n                'status': 'healthy' if healthy else 'unhealthy',\n                'connections': status\n            }\n        except Exception as e:\n            return {'status': 'error', 'message': str(e)}\n    \n    async def _check_event_health(self) -&gt; Dict[str, Any]:\n        \"\"\"Check event service health.\"\"\"\n        if not self._event_service:\n            return {'status': 'error', 'message': 'Event service not available'}\n        \n        try:\n            stats = self._event_service.get_logger_stats()\n            buffer_size = self._event_service.get_buffer_size()\n            return {\n                'status': 'healthy',\n                'stats': stats,\n                'buffer_size': buffer_size\n            }\n        except Exception as e:\n            return {'status': 'error', 'message': str(e)}\n    \n    async def _check_job_health(self) -&gt; Dict[str, Any]:\n        \"\"\"Check job service health.\"\"\"\n        # Implementation depends on job service metrics\n        return {'status': 'healthy', 'message': 'Job service operational'}\n    \n    async def get_system_health(self) -&gt; Dict[str, Any]:\n        \"\"\"Get overall system health.\"\"\"\n        health_report = {\n            'timestamp': time.time(),\n            'services': {}\n        }\n        \n        for service_name, check_func in self._health_checks.items():\n            try:\n                health_report['services'][service_name] = await check_func()\n            except Exception as e:\n                health_report['services'][service_name] = {\n                    'status': 'error',\n                    'message': str(e)\n                }\n        \n        # Determine overall system status\n        all_healthy = all(\n            service.get('status') == 'healthy'\n            for service in health_report['services'].values()\n        )\n        \n        health_report['overall_status'] = 'healthy' if all_healthy else 'unhealthy'\n        \n        return health_report\n\n\nService Decorators for Enhanced Functionality\nfrom functools import wraps\nfrom typing import Callable, Any\n\ndef with_service_retry(max_retries: int = 3, delay: float = 1.0):\n    \"\"\"Decorator to add retry logic to service methods.\"\"\"\n    def decorator(func: Callable) -&gt; Callable:\n        @wraps(func)\n        async def wrapper(self, *args, **kwargs):\n            last_error = None\n            \n            for attempt in range(max_retries):\n                try:\n                    return await func(self, *args, **kwargs)\n                except Exception as e:\n                    last_error = e\n                    if attempt &lt; max_retries - 1:\n                        await asyncio.sleep(delay * (2 ** attempt))  # Exponential backoff\n                    else:\n                        break\n            \n            # All retries failed\n            raise ServiceError(f\"Service operation failed after {max_retries} retries: {last_error}\") from last_error\n        \n        return wrapper\n    return decorator\n\ndef with_service_metrics(metric_name: str):\n    \"\"\"Decorator to collect metrics for service operations.\"\"\"\n    def decorator(func: Callable) -&gt; Callable:\n        @wraps(func)\n        async def wrapper(self, *args, **kwargs):\n            start_time = time.time()\n            \n            try:\n                result = await func(self, *args, **kwargs)\n                duration_ms = (time.time() - start_time) * 1000\n                \n                # Record success metric\n                if hasattr(self, '_metrics_collector'):\n                    await self._metrics_collector.record_success(\n                        metric_name, duration_ms\n                    )\n                \n                return result\n            except Exception as e:\n                duration_ms = (time.time() - start_time) * 1000\n                \n                # Record failure metric\n                if hasattr(self, '_metrics_collector'):\n                    await self._metrics_collector.record_failure(\n                        metric_name, duration_ms, str(e)\n                    )\n                \n                raise\n        \n        return wrapper\n    return decorator\n\n# Usage example\nclass EnhancedConnectionService(ConnectionService):\n    \"\"\"Connection service with enhanced functionality.\"\"\"\n    \n    @with_service_retry(max_retries=5, delay=2.0)\n    @with_service_metrics('get_connection')\n    async def get_connection(self, url: Optional[str] = None) -&gt; NATSClient:\n        \"\"\"Get connection with retry and metrics.\"\"\"\n        return await super().get_connection(url)\n\n\n\nService Testing Strategies\n\nMock Services for Testing\nclass MockConnectionService(BaseService):\n    \"\"\"Mock connection service for testing.\"\"\"\n    \n    def __init__(self, config: Dict[str, Any]):\n        super().__init__(config)\n        self._connections = {}\n        self._should_fail = False\n    \n    async def _do_initialize(self) -&gt; None:\n        \"\"\"Initialize mock service.\"\"\"\n        pass\n    \n    def set_failure_mode(self, should_fail: bool):\n        \"\"\"Set whether the service should fail.\"\"\"\n        self._should_fail = should_fail\n    \n    async def get_connection(self, url: Optional[str] = None) -&gt; Any:\n        \"\"\"Mock connection method.\"\"\"\n        if self._should_fail:\n            raise NaqConnectionError(\"Mock connection failure\")\n        \n        target_url = url or self.config.get('nats', {}).get('url', 'nats://localhost:4222')\n        if target_url not in self._connections:\n            self._connections[target_url] = {'mock': True, 'url': target_url}\n        \n        return self._connections[target_url]\n\nasync def test_with_mock_services():\n    \"\"\"Example of testing with mock services.\"\"\"\n    config = {'nats': {'url': 'nats://test:4222'}}\n    \n    # Create mock service\n    mock_connection = MockConnectionService(config)\n    await mock_connection.initialize()\n    \n    # Test success case\n    conn = await mock_connection.get_connection()\n    assert conn['mock'] is True\n    \n    # Test failure case\n    mock_connection.set_failure_mode(True)\n    try:\n        await mock_connection.get_connection()\n        assert False, \"Should have raised an exception\"\n    except NaqConnectionError:\n        pass  # Expected\n    \n    await mock_connection.cleanup()\n\n\nService Integration Testing\nimport pytest\n\nclass ServiceIntegrationTest:\n    \"\"\"Integration tests for service layer.\"\"\"\n    \n    @pytest.mark.asyncio\n    async def test_service_lifecycle(self):\n        \"\"\"Test service initialization and cleanup.\"\"\"\n        config = {'nats': {'url': 'nats://localhost:4222'}}\n        \n        async with ServiceManager(config) as services:\n            # Get services\n            connection_service = await services.get_service(ConnectionService)\n            stream_service = await services.get_service(StreamService)\n            \n            # Verify services are initialized\n            assert connection_service._initialized is True\n            assert stream_service._initialized is True\n            \n            # Use services\n            js = await connection_service.get_jetstream()\n            assert js is not None\n        \n        # Verify services are cleaned up\n        assert connection_service._initialized is False\n        assert stream_service._initialized is False\n    \n    @pytest.mark.asyncio\n    async def test_service_dependency_injection(self):\n        \"\"\"Test that service dependencies are properly injected.\"\"\"\n        config = {'nats': {'url': 'nats://localhost:4222'}}\n        \n        async with ServiceManager(config) as services:\n            # Get StreamService which depends on ConnectionService\n            stream_service = await services.get_service(StreamService)\n            \n            # Verify dependency is injected\n            assert stream_service._connection_service is not None\n            assert isinstance(stream_service._connection_service, ConnectionService)\n            \n            # Verify both services share the same connection\n            conn_service = await services.get_service(ConnectionService)\n            assert stream_service._connection_service is conn_service\n\n\n\nPerformance Optimization with Services\n\nConnection Pool Tuning\nclass TunedConnectionService(ConnectionService):\n    \"\"\"Connection service with performance tuning.\"\"\"\n    \n    def __init__(self, config: Dict[str, Any]):\n        super().__init__(config)\n        self._connection_pool_size = config.get('pool_size', 10)\n        self._connection_timeout = config.get('connection_timeout', 30.0)\n        self._connection_idle_timeout = config.get('idle_timeout', 300.0)\n        self._last_used = {}\n    \n    async def _do_initialize(self) -&gt; None:\n        \"\"\"Initialize with performance tuning.\"\"\"\n        await super()._do_initialize()\n        \n        # Start connection cleanup task\n        asyncio.create_task(self._cleanup_idle_connections())\n    \n    async def _cleanup_idle_connections(self):\n        \"\"\"Clean up idle connections.\"\"\"\n        while True:\n            try:\n                await asyncio.sleep(60)  # Check every minute\n                \n                current_time = time.time()\n                idle_connections = []\n                \n                async with self._lock:\n                    for url, last_used in self._last_used.items():\n                        if current_time - last_used &gt; self._connection_idle_timeout:\n                            idle_connections.append(url)\n                    \n                    # Close idle connections\n                    for url in idle_connections:\n                        if url in self._connections:\n                            try:\n                                await self._connections[url].close()\n                                del self._connections[url]\n                                del self._last_used[url]\n                                logger.info(f\"Closed idle connection to {url}\")\n                            except Exception as e:\n                                logger.error(f\"Error closing idle connection to {url}: {e}\")\n            \n            except Exception as e:\n                logger.error(f\"Error in connection cleanup task: {e}\")\n    \n    async def get_connection(self, url: Optional[str] = None) -&gt; NATSClient:\n        \"\"\"Get connection with performance tracking.\"\"\"\n        target_url = url if url is not None else self._default_url\n        \n        # Update last used time\n        self._last_used[target_url] = time.time()\n        \n        return await super().get_connection(target_url)\n\n\nBatch Processing with Services\nclass BatchJobService(JobService):\n    \"\"\"Job service optimized for batch processing.\"\"\"\n    \n    def __init__(self, config: Dict[str, Any]):\n        super().__init__(config)\n        self._batch_size = config.get('batch_size', 100)\n        self._batch_timeout = config.get('batch_timeout', 5.0)\n        self._pending_jobs = []\n        self._batch_event = asyncio.Event()\n    \n    async def _do_initialize(self) -&gt; None:\n        \"\"\"Initialize batch processing.\"\"\"\n        await super()._do_initialize()\n        \n        # Start batch processing task\n        asyncio.create_task(self._process_batches())\n    \n    async def _process_batches(self):\n        \"\"\"Process jobs in batches.\"\"\"\n        while True:\n            try:\n                # Wait for batch to fill or timeout\n                await asyncio.wait_for(\n                    self._batch_event.wait(),\n                    timeout=self._batch_timeout\n                )\n                \n                # Process batch\n                async with self._lock:\n                    if not self._pending_jobs:\n                        self._batch_event.clear()\n                        continue\n                    \n                    batch = self._pending_jobs[:self._batch_size]\n                    self._pending_jobs = self._pending_jobs[self._batch_size:]\n                    self._batch_event.clear()\n                \n                # Execute batch\n                await self._execute_batch(batch)\n                \n            except asyncio.TimeoutError:\n                # Timeout is normal, process any pending jobs\n                async with self._lock:\n                    if self._pending_jobs:\n                        batch = self._pending_jobs[:]\n                        self._pending_jobs = []\n                        await self._execute_batch(batch)\n            except Exception as e:\n                logger.error(f\"Error in batch processing: {e}\")\n    \n    async def _execute_batch(self, batch: List[Job]):\n        \"\"\"Execute a batch of jobs.\"\"\"\n        logger.info(f\"Executing batch of {len(batch)} jobs\")\n        \n        # Execute jobs concurrently\n        tasks = [self.execute_job(job) for job in batch]\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n        \n        # Process results\n        for job, result in zip(batch, results):\n            if isinstance(result, Exception):\n                logger.error(f\"Job {job.job_id} failed: {result}\")\n            else:\n                logger.debug(f\"Job {job.job_id} completed successfully\")\n    \n    async def execute_job(self, job: Job) -&gt; JobResult:\n        \"\"\"Execute a single job (overridden for batch processing).\"\"\"\n        # For batch processing, add to pending queue\n        async with self._lock:\n            self._pending_jobs.append(job)\n            self._batch_event.set()\n        \n        # Return a placeholder result\n        return JobResult(\n            job_id=job.job_id,\n            status=\"pending\",\n            result=None,\n            error=None,\n            start_time=time.time(),\n            end_time=time.time()\n        )\nThese advanced patterns demonstrate the flexibility and power of the service layer architecture in naq. By leveraging these techniques, you can build sophisticated, high-performance applications that make the most of the service layer’s capabilities while maintaining clean, maintainable code."
  },
  {
    "objectID": "contributing.html",
    "href": "contributing.html",
    "title": "Contributing",
    "section": "",
    "text": "We welcome contributions to naq! Whether you’re fixing a bug, adding a new feature, or improving the documentation, your help is appreciated."
  },
  {
    "objectID": "contributing.html#getting-started",
    "href": "contributing.html#getting-started",
    "title": "Contributing",
    "section": "Getting Started",
    "text": "Getting Started\nIf you’re new to the project, a good place to start is by looking at the open issues on GitHub.\n\nDevelopment Setup\nTo get your development environment set up, follow these steps:\n\nFork and Clone the Repository\nStart by forking the main repository on GitHub, and then clone your fork locally:\ngit clone https://github.com/YOUR_USERNAME/naq.git\ncd naq\nInstall Dependencies\nWe recommend using a virtual environment. naq uses uv for dependency management.\n# Create a virtual environment\npython -m venv .venv\nsource .venv/bin/activate\n\n# Install dependencies, including development tools\nuv pip install -e \".[dev]\"\nRun the NATS Server\nThe test suite and examples require a running NATS server. You can use the provided Docker Compose file:\ncd docker\ndocker-compose up -d\nRun the Tests\nTo make sure everything is set up correctly, run the test suite:\npytest"
  },
  {
    "objectID": "contributing.html#making-changes",
    "href": "contributing.html#making-changes",
    "title": "Contributing",
    "section": "Making Changes",
    "text": "Making Changes\n\nCreate a New Branch\nCreate a new branch for your changes:\ngit checkout -b feature/my-new-feature\nWrite Your Code\nMake your changes to the codebase. If you’re adding a new feature, please include tests.\nFormat Your Code\nBefore committing, make sure your code is formatted correctly:\nruff format .\nruff check --fix .\nCommit and Push\nCommit your changes with a clear and descriptive message, and push them to your fork:\ngit commit -m \"feat: Add my new feature\"\ngit push origin feature/my-new-feature\nCreate a Pull Request\nOpen a pull request from your fork to the main branch of the naq repository. Provide a clear description of your changes and reference any related issues."
  },
  {
    "objectID": "contributing.html#code-of-conduct",
    "href": "contributing.html#code-of-conduct",
    "title": "Contributing",
    "section": "Code of Conduct",
    "text": "Code of Conduct\nPlease note that this project is released with a Contributor Code of Conduct. By participating in this project you agree to abide by its terms.\nThank you for contributing to naq!"
  },
  {
    "objectID": "examples.html",
    "href": "examples.html",
    "title": "Usage Examples",
    "section": "",
    "text": "This page provides practical examples for some of naq’s key features."
  },
  {
    "objectID": "examples.html#example-1-scheduled-and-recurring-jobs",
    "href": "examples.html#example-1-scheduled-and-recurring-jobs",
    "title": "Usage Examples",
    "section": "Example 1: Scheduled and Recurring Jobs",
    "text": "Example 1: Scheduled and Recurring Jobs\nnaq allows you to schedule jobs to run at a specific time in the future or on a recurring basis using cron expressions.\nTo run these examples, you need both a worker and the scheduler process running:\n# Terminal 1: Start the scheduler\nnaq scheduler\n\n# Terminal 2: Start a worker\nnaq worker scheduled_queue\n\nOne-Time Scheduled Job\nYou can enqueue a job to run after a specific delay or at a precise time.\n# schedule_task.py\nimport asyncio\nfrom datetime import datetime, timedelta\nfrom naq import enqueue_at, enqueue_in\n\nasync def send_reminder(user_id, message):\n    print(f\"Sending reminder to {user_id}: {message}\")\n    # Add logic to send an email or push notification\n    return f\"Reminder sent to {user_id}\"\n\nasync def main():\n    # Schedule a job to run in 5 minutes\n    run_in_5_min = await enqueue_in(\n        send_reminder,\n        delay=timedelta(minutes=5),\n        user_id=\"user123\",\n        message=\"Your meeting starts in 5 minutes.\",\n        queue_name=\"scheduled_queue\"\n    )\n    print(f\"Job {run_in_5_min.job_id} scheduled to run in 5 minutes.\")\n\n    # Schedule a job to run at a specific time (UTC)\n    run_at_time = datetime.utcnow() + timedelta(hours=1)\n    run_at = await enqueue_at(\n        send_reminder,\n        run_at=run_at_time,\n        user_id=\"user456\",\n        message=\"Don't forget your 1-hour follow-up.\",\n        queue_name=\"scheduled_queue\"\n    )\n    print(f\"Job {run_at.job_id} scheduled to run at {run_at_time.isoformat()}.\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\n\nRecurring Job (Cron)\nFor tasks that need to run on a regular schedule (e.g., nightly reports, weekly cleanups), you can use the schedule function with a cron string.\n# recurring_task.py\nimport asyncio\nfrom naq import schedule\n\nasync def generate_nightly_report():\n    print(\"Generating the nightly sales report...\")\n    # Logic to aggregate data and create a report\n    print(\"Nightly report complete.\")\n    return \"Report generated successfully.\"\n\nasync def main():\n    # Schedule the report to run every day at 2:00 AM UTC\n    cron_schedule = await schedule(\n        generate_nightly_report,\n        cron=\"0 2 * * *\",  # Standard cron format\n        schedule_id=\"nightly-sales-report\",\n        queue_name=\"scheduled_queue\"\n    )\n    print(f\"Cron job '{cron_schedule.schedule_id}' is now active.\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
  },
  {
    "objectID": "examples.html#example-2-automatic-job-retries",
    "href": "examples.html#example-2-automatic-job-retries",
    "title": "Usage Examples",
    "section": "Example 2: Automatic Job Retries",
    "text": "Example 2: Automatic Job Retries\nnaq can automatically retry failed jobs with configurable strategies. This is useful for tasks that might fail due to transient issues, like network hiccups.\n# retry_task.py\nimport asyncio\nimport random\nfrom naq import enqueue\n\nasync def flaky_api_call(request_id):\n    \"\"\"\n    This function simulates an API call that sometimes fails.\n    \"\"\"\n    print(f\"Attempting to call API for request {request_id}...\")\n    if random.random() &gt; 0.5:\n        print(\"API call successful!\")\n        return \"Success\"\n    else:\n        print(\"API call failed. Will retry...\")\n        raise ConnectionError(\"Could not connect to the API\")\n\nasync def main():\n    # Enqueue the job with a retry policy\n    job = await enqueue(\n        flaky_api_call,\n        request_id=\"abc-123\",\n        queue_name=\"default\",\n        max_retries=3,          # Attempt the job up to 3 more times\n        retry_delay=5,          # Wait 5 seconds between retries\n        retry_strategy=\"linear\" # Use a fixed delay\n    )\n    print(f\"Enqueued job {job.job_id} with 3 linear retries.\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\n\n\n\n\n\nNote\n\n\n\nRetry Strategies\nnaq supports both linear (fixed delay) and exponential backoff strategies for retries. You can also provide a list of integers to retry_delay for a custom delay sequence."
  },
  {
    "objectID": "examples.html#example-3-job-dependencies",
    "href": "examples.html#example-3-job-dependencies",
    "title": "Usage Examples",
    "section": "Example 3: Job Dependencies",
    "text": "Example 3: Job Dependencies\nYou can create workflows by making jobs dependent on the successful completion of others. The dependent job will only run after its dependencies have finished.\n# dependency_workflow.py\nimport asyncio\nfrom naq import enqueue\n\nasync def download_data(source_url):\n    print(f\"Downloading data from {source_url}...\")\n    await asyncio.sleep(2)  # Simulate download\n    file_path = f\"/tmp/{source_url.split('/')[-1]}.csv\"\n    print(f\"Data downloaded to {file_path}\")\n    return file_path\n\nasync def process_data(file_path):\n    print(f\"Processing data from {file_path}...\")\n    await asyncio.sleep(3)  # Simulate processing\n    result_path = f\"{file_path}.processed\"\n    print(f\"Data processed and saved to {result_path}\")\n    return result_path\n\nasync def upload_results(result_path):\n    print(f\"Uploading {result_path} to cloud storage...\")\n    await asyncio.sleep(1)  # Simulate upload\n    print(\"Upload complete.\")\n    return \"Workflow finished successfully.\"\n\nasync def main():\n    # Step 1: Download data\n    download_job = await enqueue(download_data, source_url=\"http://example.com/data\")\n\n    # Step 2: Process data (depends on download)\n    process_job = await enqueue(\n        process_data,\n        file_path=download_job, # Pass the result of the dependency\n        depends_on=[download_job]\n    )\n\n    # Step 3: Upload results (depends on processing)\n    upload_job = await enqueue(\n        upload_results,\n        result_path=process_job,\n        depends_on=[process_job]\n    )\n\n    print(f\"Workflow started. Final job: {upload_job.job_id}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
  },
  {
    "objectID": "examples.html#example-4-using-multiple-queues",
    "href": "examples.html#example-4-using-multiple-queues",
    "title": "Usage Examples",
    "section": "Example 4: Using Multiple Queues",
    "text": "Example 4: Using Multiple Queues\nYou can use different queues to prioritize jobs or to dedicate workers to specific types of tasks.\nStart workers for each queue in separate terminals:\n# Terminal 1: High-priority worker\nnaq worker notifications --log-level info\n\n# Terminal 2: Low-priority worker\nnaq worker data_processing --log-level info\nNow, you can enqueue jobs to the appropriate queues.\n# multi_queue_example.py\nimport asyncio\nfrom naq import enqueue\n\nasync def send_email(address, subject, body):\n    print(f\"Sending high-priority email to {address}...\")\n    await asyncio.sleep(0.5)\n    return \"Email sent.\"\n\nasync def transcode_video(video_id):\n    print(f\"Starting low-priority video transcoding for {video_id}...\")\n    await asyncio.sleep(10) # Simulate long-running task\n    return \"Video transcoded.\"\n\nasync def main():\n    # Enqueue a high-priority job\n    email_job = await enqueue(\n        send_email,\n        address=\"user@example.com\",\n        subject=\"Your order\",\n        body=\"...\",\n        queue_name=\"notifications\" # Target the 'notifications' queue\n    )\n    print(f\"Enqueued notification job {email_job.job_id}\")\n\n    # Enqueue a low-priority job\n    video_job = await enqueue(\n        transcode_video,\n        video_id=12345,\n        queue_name=\"data_processing\" # Target the 'data_processing' queue\n    )\n    print(f\"Enqueued data processing job {video_job.job_id}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\n## Example 5: Custom Event Handlers\n\nThe event system allows you to build powerful, reactive workflows by responding to job lifecycle events. This example demonstrates how to create a custom event handler that performs an action when a job of a certain type fails.\n\nTo run this example, you will need:\n1. A NATS server running.\n2. Event logging enabled (`NAQ_EVENTS_ENABLED=true`).\n3. A worker running to process jobs.\n4. This script running to handle events.\n\n```python\n# custom_event_handler.py\nimport asyncio\nimport os\nfrom naq import enqueue\nfrom naq.events import AsyncJobEventProcessor, NATSJobEventStorage, JobEventType, JobEvent\n\n# Ensure event logging is enabled for this example to work\nos.environ[\"NAQ_EVENTS_ENABLED\"] = \"true\"\n\n# A task that might fail\nasync def flaky_task(task_id: str):\n    print(f\"[Task {task_id}] Starting risky operation...\")\n    await asyncio.sleep(1)\n    if task_id == \"fail-on-purpose\":\n        raise ValueError(\"This task was designed to fail!\")\n    print(f\"[Task {task_id}] Operation completed successfully.\")\n    return f\"Task {task_id} finished.\"\n\n# Our custom event handler\nasync def on_job_failure(event: JobEvent):\n    \"\"\"\n    This function is called whenever a job fails.\n    \"\"\"\n    print(\"=\" * 50)\n    print(\"🚨 CUSTOM HANDLER TRIGGERED: JOB FAILURE DETECTED! 🚨\")\n    print(\"=\" * 50)\n    print(f\"  Job ID:       {event.job_id}\")\n    print(f\"  Error Type:    {event.error_type}\")\n    print(f\"  Error Message: {event.error_message}\")\n    print(f\"  Queue:         {event.queue_name}\")\n    print(f\"  Worker:        {event.worker_id}\")\n    print(f\"  Timestamp:     {event.timestamp}\")\n    print(\"=\" * 50)\n    # Here you could add more complex logic, like:\n    # - Sending a notification to Slack or PagerDuty\n    # - Writing to a database for analytics\n    # - Triggering a recovery workflow\n\nasync def main():\n    # --- Setup ---\n    print(\"Setting up event processor and enqueuing jobs...\")\n\n    # 1. Create the event storage and processor\n    storage = NATSJobEventStorage()\n    processor = AsyncJobEventProcessor(storage)\n\n    # 2. Register our custom handler for FAILED events\n    processor.add_handler(JobEventType.FAILED, on_job_failure)\n    \n    # 3. Start the processor in the background\n    # It will begin listening for events immediately.\n    processor_task = asyncio.create_task(processor.start())\n\n    # Give the processor a moment to start up\n    await asyncio.sleep(2)\n\n    # --- Enqueue Jobs ---\n    print(\"\\nEnqueuing a successful job and a job that will fail...\")\n    # Enqueue a job that will succeed\n    await enqueue(flaky_task, \"success-1\", queue_name=\"example_queue\")\n    \n    # Enqueue a job that is designed to fail\n    await enqueue(flaky_task, \"fail-on-purpose\", queue_name=\"example_queue\")\n\n    print(\"Jobs enqueued. Make sure a worker is running to process them.\")\n    print(\"The custom handler will be called when the 'fail-on-purpose' job fails.\")\n\n    # --- Keep Running ---\n    try:\n        # Keep the script running to allow the processor to handle events\n        print(\"\\nWaiting for events... Press Ctrl+C to exit.\")\n        while True:\n            await asyncio.sleep(1)\n    except KeyboardInterrupt:\n        print(\"\\nShutting down...\")\n    finally:\n        # Ensure the processor is stopped cleanly\n        await processor.stop()\n        await processor_task # Wait for the processor task to finish\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nTo see this in action:\n\nStart a worker in one terminal: bash     NAQ_EVENTS_ENABLED=true naq worker example_queue\nRun the custom handler script in another terminal: bash     python custom_event_handler.py\n\nYou will see the handler script enqueue two jobs. The worker will pick them up. When the fail-on-purpose job fails, you will see the custom “🚨 CUSTOM HANDLER TRIGGERED” message printed by the on_job_failure function in the handler script’s terminal, demonstrating the reactive power of the event system."
  },
  {
    "objectID": "examples.html#example-6-connection-management-with-context-managers",
    "href": "examples.html#example-6-connection-management-with-context-managers",
    "title": "Usage Examples",
    "section": "Example 6: Connection Management with Context Managers",
    "text": "Example 6: Connection Management with Context Managers\nThe new connection management system provides context managers for automatic resource management. These examples demonstrate how to use the different context managers available in naq.\n\nBasic Connection Management\n# connection_example.py\nimport asyncio\nfrom naq.connection import nats_connection, nats_jetstream, nats_kv_store\n\nasync def basic_connection_example():\n    \"\"\"Example of basic NATS connection management.\"\"\"\n    \n    # Simple connection for publishing messages\n    async with nats_connection() as conn:\n        await conn.publish(\"hello.world\", b\"Hello from context manager!\")\n        print(\"Message published successfully\")\n    \n    # Connection is automatically closed when exiting the context\n\nasync def jetstream_example():\n    \"\"\"Example of JetStream context management.\"\"\"\n    \n    # Combined connection and JetStream context\n    async with nats_jetstream() as (conn, js):\n        # Create a stream\n        await js.add_stream(\n            name=\"example_stream\",\n            subjects=[\"example.*\"]\n        )\n        print(\"Stream created successfully\")\n        \n        # Publish a message to the stream\n        await js.publish(\"example.data\", b\"Stream data\")\n        print(\"Message published to stream\")\n    \n    # Both connection and JetStream context are automatically closed\n\nasync def kv_store_example():\n    \"\"\"Example of KeyValue store management.\"\"\"\n    \n    # KeyValue store context\n    async with nats_kv_store(\"example_bucket\") as kv:\n        # Store a value\n        await kv.put(\"user:123\", '{\"name\": \"Alice\", \"role\": \"admin\"}')\n        print(\"Value stored in KV bucket\")\n        \n        # Retrieve the value\n        result = await kv.get(\"user:123\")\n        print(f\"Retrieved value: {result.value.decode()}\")\n        \n        # List all keys\n        keys = await kv.keys()\n        print(f\"Keys in bucket: {list(keys)}\")\n    \n    # KV store context is automatically closed\n\nif __name__ == \"__main__\":\n    asyncio.run(basic_connection_example())\n    asyncio.run(jetstream_example())\n    asyncio.run(kv_store_example())\n\n\nConnection Management with Decorators\n# decorator_example.py\nimport asyncio\nfrom naq.connection import with_nats_connection, with_jetstream_context\n\n@with_nats_connection()\nasync def publish_message(conn, subject: str, message: str):\n    \"\"\"Publish a message using the injected connection.\"\"\"\n    await conn.publish(subject, message.encode())\n    print(f\"Message published to {subject}\")\n\n@with_jetstream_context()\nasync def create_consumer(js, stream_name: str, consumer_name: str):\n    \"\"\"Create a consumer using the injected JetStream context.\"\"\"\n    await js.add_consumer(\n        stream_name,\n        durable_name=consumer_name,\n        ack_policy=\"explicit\"\n    )\n    print(f\"Consumer '{consumer_name}' created for stream '{stream_name}'\")\n\nasync def decorator_example():\n    \"\"\"Example of using connection decorators.\"\"\"\n    \n    # Publish a message - connection is automatically injected\n    await publish_message(\"decorator.test\", \"Hello from decorator!\")\n    \n    # Create a consumer - JetStream context is automatically injected\n    await create_consumer(\"example_stream\", \"example_consumer\")\n\nif __name__ == \"__main__\":\n    asyncio.run(decorator_example())\n\n\nConnection Testing and Monitoring\n# monitoring_example.py\nimport asyncio\nfrom naq.connection import (\n    test_nats_connection,\n    wait_for_nats_connection,\n    connection_monitor,\n    nats_connection\n)\n\nasync def monitoring_example():\n    \"\"\"Example of connection testing and monitoring.\"\"\"\n    \n    # Test if connection is available\n    is_healthy = await test_nats_connection()\n    print(f\"Connection healthy: {is_healthy}\")\n    \n    # Wait for connection to be ready (useful in production)\n    is_ready = await wait_for_nats_connection(timeout=10)\n    print(f\"Connection ready: {is_ready}\")\n    \n    # Use connection and monitor metrics\n    async with nats_connection() as conn:\n        # Do some work\n        await conn.publish(\"monitoring.test\", b\"Test message\")\n        \n        # Check connection metrics\n        metrics = connection_monitor.metrics\n        print(f\"Total connections: {metrics.total_connections}\")\n        print(f\"Active connections: {metrics.active_connections}\")\n        print(f\"Failed connections: {metrics.failed_connections}\")\n        print(f\"Average connection time: {metrics.average_connection_time:.2f}s\")\n\nif __name__ == \"__main__\":\n    asyncio.run(monitoring_example())\n\n\nCustom Configuration\n# custom_config_example.py\nimport asyncio\nfrom naq.connection import Config, nats_connection, nats_jetstream\n\nasync def custom_config_example():\n    \"\"\"Example of using custom connection configuration.\"\"\"\n    \n    # Create custom configuration\n    config = Config(\n        servers=[\"nats://localhost:4222\", \"nats://localhost:4223\"],\n        client_name=\"my-custom-app\",\n        max_reconnect_attempts=10,\n        reconnect_time_wait=5.0\n    )\n    \n    # Use custom configuration\n    async with nats_connection(config) as conn:\n        await conn.publish(\"config.test\", b\"Using custom config\")\n        print(\"Message published with custom configuration\")\n    \n    # Use with JetStream\n    async with nats_jetstream(config) as (conn, js):\n        await js.add_stream(\n            name=\"config_stream\",\n            subjects=[\"config.*\"]\n        )\n        print(\"Stream created with custom configuration\")\n\nif __name__ == \"__main__\":\n    asyncio.run(custom_config_example())\n\n\nMigration from Legacy Patterns\n# migration_example.py\nimport asyncio\nfrom naq.connection import (\n    # Legacy functions (still work)\n    get_nats_connection,\n    get_jetstream_context,\n    close_nats_connection,\n    \n    # New context managers\n    nats_connection,\n    nats_jetstream\n)\n\nasync def legacy_pattern():\n    \"\"\"Old way of managing connections.\"\"\"\n    print(\"=== Legacy Pattern ===\")\n    \n    nc = await get_nats_connection()\n    try:\n        js = await get_jetstream_context(nc)\n        await js.add_stream(name=\"legacy_stream\", subjects=[\"legacy.*\"])\n        print(\"Stream created using legacy pattern\")\n    finally:\n        await close_nats_connection(nc)\n        print(\"Connection closed manually\")\n\nasync def modern_pattern():\n    \"\"\"New way of managing connections.\"\"\"\n    print(\"=== Modern Pattern ===\")\n    \n    async with nats_jetstream() as (conn, js):\n        await js.add_stream(name=\"modern_stream\", subjects=[\"modern.*\"])\n        print(\"Stream created using modern pattern\")\n    \n    print(\"Connection closed automatically\")\n\nasync def migration_example():\n    \"\"\"Example showing both patterns.\"\"\"\n    \n    # Legacy pattern (still works)\n    await legacy_pattern()\n    \n    # Modern pattern (recommended)\n    await modern_pattern()\n\nif __name__ == \"__main__\":\n    asyncio.run(migration_example())\n\n\nProduction-Ready Connection Management\n# production_example.py\nimport asyncio\nfrom naq.connection import (\n    nats_connection,\n    nats_jetstream,\n    test_nats_connection,\n    wait_for_nats_connection,\n    connection_monitor,\n    Config\n)\n\nclass ProductionService:\n    \"\"\"Example of production-ready connection management.\"\"\"\n    \n    def __init__(self, config: Config):\n        self.config = config\n    \n    async def ensure_connection(self):\n        \"\"\"Ensure connection is available before proceeding.\"\"\"\n        if not await test_nats_connection(self.config):\n            print(\"Waiting for connection...\")\n            if not await wait_for_nats_connection(self.config, timeout=30):\n                raise ConnectionError(\"Failed to establish connection\")\n    \n    async def process_data(self):\n        \"\"\"Process data with robust connection management.\"\"\"\n        \n        # Ensure connection is available\n        await self.ensure_connection()\n        \n        try:\n            # Use JetStream with automatic connection management\n            async with nats_jetstream(self.config) as (conn, js):\n                # Create stream if it doesn't exist\n                try:\n                    await js.add_stream(\n                        name=\"production_stream\",\n                        subjects=[\"production.*\"]\n                    )\n                except Exception as e:\n                    print(f\"Stream might already exist: {e}\")\n                \n                # Process and publish data\n                for i in range(5):\n                    data = f\"Production data {i}\"\n                    await js.publish(\"production.data\", data.encode())\n                    print(f\"Published: {data}\")\n                \n                # Log connection metrics\n                metrics = connection_monitor.metrics\n                print(f\"Connection metrics - Total: {metrics.total_connections}, \"\n                      f\"Active: {metrics.active_connections}, \"\n                      f\"Failed: {metrics.failed_connections}\")\n        \n        except Exception as e:\n            print(f\"Error in production processing: {e}\")\n            raise\n\nasync def production_example():\n    \"\"\"Example of production-ready connection management.\"\"\"\n    \n    # Production configuration\n    config = Config(\n        servers=[\"nats://localhost:4222\"],\n        client_name=\"production-service\",\n        max_reconnect_attempts=10,\n        reconnect_time_wait=5.0\n    )\n    \n    # Create and run production service\n    service = ProductionService(config)\n    await service.process_data()\n\nif __name__ == \"__main__\":\n    asyncio.run(production_example())\nTo run these examples:\n\nStart a NATS server in one terminal: bash     nats-server\nRun the examples in another terminal: bash     python connection_example.py     python decorator_example.py     python monitoring_example.py     python custom_config_example.py     python migration_example.py     python production_example.py\n\nThese examples demonstrate the various ways to use the new connection management system in naq, from basic usage to production-ready patterns. The context managers provide automatic resource management, while the decorators offer a convenient way to inject connections into functions."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NAQ - NATS Asynchronous Queue",
    "section": "",
    "text": "NAQ is a simple, asynchronous job queueing library for Python, inspired by RQ (Redis Queue), but built entirely on top of NATS and its JetStream persistence layer.\nIt allows you to easily enqueue Python functions to be executed asynchronously by worker processes, leveraging the power and resilience of NATS JetStream for message persistence and delivery."
  },
  {
    "objectID": "index.html#get-started",
    "href": "index.html#get-started",
    "title": "NAQ - NATS Asynchronous Queue",
    "section": "Get Started",
    "text": "Get Started\nReady to dive in? Check out the Quickstart Guide to get your first worker up and running in minutes."
  },
  {
    "objectID": "index.html#key-features",
    "href": "index.html#key-features",
    "title": "NAQ - NATS Asynchronous Queue",
    "section": "Key Features",
    "text": "Key Features\n\nSimple API: Familiar and easy-to-use API, similar to RQ.\nAsynchronous Core: Built with asyncio and nats-py for high performance.\nPersistent & Reliable: Uses NATS JetStream for robust job persistence and guaranteed delivery.\nEvent Logging & Monitoring: Comprehensive event system for tracking job lifecycles and building reactive applications.\nScheduled & Recurring Jobs: Supports cron-style, interval-based, and one-time scheduled tasks.\nJob Dependencies: Create complex workflows by defining dependencies between jobs.\nAutomatic Retries: Configurable retry mechanism with exponential backoff for failed jobs.\nWeb Dashboard: An optional, real-time dashboard for monitoring queues, workers, and jobs.\nCLI: A powerful command-line interface for managing your queues and workers, including real-time event monitoring."
  },
  {
    "objectID": "architecture.html",
    "href": "architecture.html",
    "title": "Architecture Overview",
    "section": "",
    "text": "naq is designed to be a simple yet powerful distributed task queue with a comprehensive event-driven architecture. Its architecture is fundamentally built around NATS and its persistence layer, JetStream, which serve as the central nervous system for communication between clients, workers, and the scheduler."
  },
  {
    "objectID": "architecture.html#core-components",
    "href": "architecture.html#core-components",
    "title": "Architecture Overview",
    "section": "Core Components",
    "text": "Core Components\nThe naq ecosystem consists of four main components:\n\nThe Client (Producer): Any application that enqueues jobs. This could be a web server, a script, or any other part of your system that needs to offload work.\nNATS Server (with JetStream): The message broker that provides persistence, message delivery, and storage for job results and worker metadata.\nThe Worker(s): The processes that subscribe to queues, execute jobs, and report back their results. You can run as many workers as you need, on as many machines as you want.\nThe Scheduler: A dedicated process that handles time-based events, such as scheduled jobs and recurring tasks."
  },
  {
    "objectID": "architecture.html#how-it-works-the-job-lifecycle",
    "href": "architecture.html#how-it-works-the-job-lifecycle",
    "title": "Architecture Overview",
    "section": "How It Works: The Job Lifecycle",
    "text": "How It Works: The Job Lifecycle\nHere is a high-level overview of what happens when a job is enqueued and processed, including the comprehensive event-driven architecture:\n\n\n\n\n\ngraph TD\n    subgraph \"Your Application\"\n        Client[Client]\n        PythonCode[Python Code]\n        EventHandler[Event Handler]\n    end\n\n    subgraph \"NAQ Processes\"\n        Worker[Worker]\n        Scheduler[Scheduler]\n        EventProcessor[Event Processor]\n    end\n\n    subgraph \"NATS Server (JetStream)\"\n        QueueStream[Queue Stream]\n        ResultStore[Result KV Store]\n        ScheduledJobs[Scheduled Jobs KV]\n        EventStream[Event Stream]\n    end\n\n    Client -- \"1. Enqueue Job\" --&gt; QueueStream\n    Scheduler -- \"8. Check for Due Jobs\" --&gt; ScheduledJobs\n    ScheduledJobs -- \"Job is Due\" --&gt; Scheduler\n    Scheduler -- \"9. Enqueue Due Job\" --&gt; QueueStream\n    Worker -- \"3. Fetch Job\" --&gt; QueueStream\n    Worker -- \"4. Execute Function\" --&gt; PythonCode\n    PythonCode -- \"5. Return Result\" --&gt; Worker\n    Worker -- \"6. Store Result\" --&gt; ResultStore\n    \n    Client -- \"2. Log ENQUEUED Event\" --&gt; EventStream\n    Worker -- \"Log STARTED/COMPLETED/FAILED Events\" --&gt; EventStream\n    Scheduler -- \"Log SCHEDULED Event\" --&gt; EventStream\n    Worker -- \"Log RETRY_SCHEDULED Event\" --&gt; EventStream\n    Scheduler -- \"Log SCHEDULE_TRIGGERED Event\" --&gt; EventStream\n    Worker -- \"Log CANCELLED Event\" --&gt; EventStream\n    All Components -- \"Log STATUS_CHANGED Events\" --&gt; EventStream\n    \n    EventStream -- \"Stream Events\" --&gt; EventProcessor\n    EventProcessor -- \"Dispatch to Handlers\" --&gt; EventHandler"
  },
  {
    "objectID": "architecture.html#the-central-event-stream-approach",
    "href": "architecture.html#the-central-event-stream-approach",
    "title": "Architecture Overview",
    "section": "The Central Event Stream Approach",
    "text": "The Central Event Stream Approach\nAt the heart of naq’s architecture is a centralized event stream that captures all job lifecycle events. This event-driven approach enables:\n\nComplete visibility: Every job state change is captured and stored durably\nReal-time monitoring: Events are available immediately as they occur\nDecoupled processing: Components can react to events without direct coupling\nShared event logging: All components use the same event logging infrastructure\n\n\nShared Event Logging\nAll naq components (Client, Worker, Scheduler) use the shared event logging system provided by the SharedEventLoggerManager. This ensures:\n\nConsistency: All events follow the same format and structure\nReliability: Events are buffered and flushed efficiently to prevent loss\nPerformance: Non-blocking logging with configurable batching\nCentralized configuration: Event logging is managed through a single point\n\n\n\nThe Complete Event Lifecycle\n\nEnqueueing: The client calls an enqueue function (e.g., enqueue_sync). The function, its arguments, and other metadata are serialized into a job payload. This payload is then published as a message to a NATS subject that corresponds to the target queue.\nEvent Logging (Enqueue): Upon successful enqueuing, the client (or Queue instance) logs an ENQUEUED event to the dedicated Event Stream using the shared event logger.\nPersistence: NATS JetStream receives this message and persists it in a Stream. Each queue in naq maps directly to a JetStream Stream. This ensures that even if no workers are online, the job is safely stored and will be processed later.\nFetching: A naq worker process is constantly listening on the queue’s Stream. When a new job is available, the worker consumes the message, acknowledging it to NATS so that it isn’t delivered to another worker.\nStatus Change: As the worker begins processing, it logs a STATUS_CHANGED event to indicate the job has moved from PENDING to RUNNING state.\nEvent Logging (Worker - Start): The worker logs a STARTED event when it begins processing a job.\nExecution: The worker deserializes the job payload and executes the specified Python function with the provided arguments.\nResult Handling: Once the function completes, its return value is serialized. The worker then stores this result in a NATS Key-Value (KV) Store, using the unique job ID as the key. This result has a configurable Time-To-Live (TTL), after which it is automatically purged by NATS.\nStatus Change: Upon completion, the worker logs another STATUS_CHANGED event to indicate the final state (COMPLETED or FAILED).\nEvent Logging (Worker - Completion): The worker logs COMPLETED events when jobs finish successfully, or FAILED events when jobs encounter errors. If a job fails and is configured for retry, a RETRY_SCHEDULED event is logged. If a job is cancelled, a CANCELLED event is logged.\nScheduled Jobs: The naq scheduler process periodically scans a dedicated KV store for jobs that are due to run. When it finds one, it enqueues it into the appropriate queue, and the job follows the normal lifecycle from there.\nEvent Logging (Scheduler): The scheduler logs SCHEDULED events when a new scheduled job is created and SCHEDULE_TRIGGERED events when a scheduled job is enqueued for execution.\nEvent Processing: The AsyncJobEventProcessor subscribes to the Event Stream. It reads events in real-time and dispatches them to any registered event handlers, allowing for reactive, event-driven applications.\n\n\n\nNew Event Types\nThe event-driven architecture includes several new event types that provide comprehensive visibility into job processing:\n\nCANCELLED: Logged when a job is cancelled before or during execution\nSTATUS_CHANGED: Logged whenever a job’s status changes, providing detailed state transition tracking\n\nThese new events, combined with the existing event types (ENQUEUED, STARTED, COMPLETED, FAILED, RETRY_SCHEDULED, SCHEDULED, SCHEDULE_TRIGGERED), provide a complete picture of the job lifecycle from creation to completion.\n\n\nEvent Correlation\nAll events include the job ID, enabling easy correlation of events across the entire lifecycle. This allows you to:\n\nTrack the complete journey of a single job\nMeasure timing between different stages\nIdentify bottlenecks in processing\nBuild comprehensive monitoring and alerting systems"
  },
  {
    "objectID": "architecture.html#why-nats",
    "href": "architecture.html#why-nats",
    "title": "Architecture Overview",
    "section": "Why NATS?",
    "text": "Why NATS?\nUsing NATS and JetStream as the foundation provides several key advantages:\n\nDecoupling: Clients, workers, and the scheduler are completely decoupled. They only need to know how to talk to NATS, not to each other directly.\nScalability: You can add more workers at any time to increase your processing capacity. NATS handles the load balancing of jobs to available workers automatically.\nResilience: If a worker crashes, JetStream ensures that the job it was processing will be re-delivered to another worker after a timeout. If the entire naq system goes down, the jobs are safe in the NATS stream, ready to be processed when the system comes back online.\nSimplicity: By offloading the complexities of persistence, delivery guarantees, and storage to NATS, the naq codebase can remain focused on the core logic of job execution and scheduling."
  },
  {
    "objectID": "architecture.html#service-layer-architecture",
    "href": "architecture.html#service-layer-architecture",
    "title": "Architecture Overview",
    "section": "Service Layer Architecture",
    "text": "Service Layer Architecture\nThe service layer architecture is a recent addition to naq that addresses the challenge of connection management and resource utilization across the system. Previously, the codebase suffered from over 44+ duplicated NATS connection patterns, leading to inefficient resource usage and complex connection management.\n\nThe Problem: Connection Pattern Duplication\nBefore the service layer, each component in naq (Queue, Worker, Scheduler, etc.) was responsible for creating and managing its own NATS connections. This approach led to several issues:\n\nResource Inefficiency: Multiple connections to the same NATS server from the same process\nCode Duplication: Similar connection logic repeated across 44+ locations in the codebase\nInconsistent Configuration: Connection parameters and retry logic varied between components\nDifficult Maintenance: Changes to connection handling required updates in multiple files\nConnection Leaks: No centralized management for connection lifecycle\n\n\n\nThe Solution: Centralized Service Layer\nThe service layer architecture introduces a centralized approach to managing resources and dependencies through a set of specialized services:\n\nCore Services\n\nConnectionService: Manages NATS connections and JetStream contexts\n\nProvides connection pooling and reuse\nHandles connection retries with exponential backoff\nCentralizes connection configuration\nManages connection lifecycle and health checks\n\nStreamService: Handles NATS JetStream stream operations\n\nStream creation, configuration, and management\nStream purging and message deletion\nStream information retrieval\n\nKVStoreService: Manages NATS Key-Value store operations\n\nCentralized KV store access patterns\nTTL management for stored values\nTransaction support for atomic operations\n\nJobService: Handles job execution and result management\n\nJob execution with proper error handling\nResult storage and retrieval\nJob failure handling and retry logic\n\nEventService: Manages event logging and processing\n\nEvent batching and buffering\nEvent stream management\nEvent correlation and tracking\n\nSchedulerService: Handles scheduled job management\n\nJob scheduling and triggering\nRecurring job management\nSchedule persistence and retrieval\n\n\n\n\nService Manager\nThe ServiceManager is the central component that: - Creates and manages service instances - Handles service dependencies - Ensures proper initialization and cleanup - Provides a unified interface for accessing services\n\n\n\nBenefits of the Service Layer\n\n1. Connection Pooling and Reuse\nThe service layer eliminates connection duplication by maintaining a pool of connections that can be shared across components:\n# Before: Each component creates its own connection\nclass Queue:\n    async def _get_js(self):\n        self._nc = await nats.connect(self._nats_url)\n        self._js = self._nc.jetstream()\n        return self._js\n\n# After: Use centralized ConnectionService\nclass Queue:\n    async def _get_js(self):\n        if self._services is None:\n            self._services = ServiceManager(self._config)\n        \n        connection_service = await self._services.get_service(ConnectionService)\n        self._js = await connection_service.get_jetstream(self._nats_url)\n        return self._js\n\n\n2. Centralized Error Handling\nAll services implement consistent error handling patterns with proper logging and retry logic:\nasync def get_connection(self, url: Optional[str] = None) -&gt; NATSClient:\n    target_url = url if url is not None else self._default_url\n    \n    # Check if we already have a connection\n    async with self._lock:\n        if target_url in self._connections and self._connections[target_url].is_connected:\n            return self._connections[target_url]\n    \n    # Create a new connection with retry logic\n    return await self._create_connection_with_retry(target_url)\n\n\n3. Dependency Injection\nServices can depend on other services, creating a clean dependency graph:\n# StreamService depends on ConnectionService\nasync def get_service(self, service_type: type) -&gt; BaseService:\n    if service_type == StreamService:\n        # Get ConnectionService dependency\n        connection_service = await self.get_service(ConnectionService)\n        service = service_type(self.config, connection_service)\n    # ... other service types\n\n\n4. Resource Lifecycle Management\nThe service layer ensures proper initialization and cleanup of all resources:\nasync def __aenter__(self):\n    await self.initialize()\n    return self\n    \nasync def __aexit__(self, exc_type, exc_val, exc_tb):\n    await self.cleanup()\n\n\n\nMigration from Old Patterns\n\nBefore: Direct Connection Management\n# Old pattern - direct connection management\nclass OldQueue:\n    def __init__(self, name, nats_url):\n        self.name = name\n        self.nats_url = nats_url\n        self._nc = None\n        self._js = None\n    \n    async def _get_js(self):\n        if self._js is None:\n            self._nc = await nats.connect(self.nats_url)\n            self._js = self._nc.jetstream()\n        return self._js\n    \n    async def close(self):\n        if self._nc:\n            await self._nc.close()\n\n\nAfter: Service-Based Architecture\n# New pattern - service-based architecture\nclass NewQueue:\n    def __init__(self, name, nats_url, services=None):\n        self.name = name\n        self.nats_url = nats_url\n        self._services = services or ServiceManager({'nats': {'url': nats_url}})\n        self._js = None\n    \n    async def _get_js(self):\n        if self._js is None:\n            connection_service = await self._services.get_service(ConnectionService)\n            self._js = await connection_service.get_jetstream(self.nats_url)\n        return self._js\n    \n    async def close(self):\n        await self._services.cleanup_all()\n\n\n\nService Layer in Action\nHere’s how the service layer works across different components:\n# Queue using services\nasync def enqueue_example():\n    config = {'nats': {'url': 'nats://localhost:4222'}}\n    \n    async with ServiceManager(config) as services:\n        queue = Queue(name='example', services=services)\n        job = await queue.enqueue(my_function, arg1, arg2)\n        \n        # All connections managed by the service layer\n        # No need to manually close connections\n\n# Worker using services\nasync def worker_example():\n    config = {'nats': {'url': 'nats://localhost:4222'}}\n    \n    async with ServiceManager(config) as services:\n        worker = Worker(queues=['example'], services=services)\n        await worker.run()\n        \n        # Services automatically cleaned up when context exits\n\n\nBest Practices for Using the Service Layer\n\nUse ServiceManager for Lifecycle Management: Always use the ServiceManager as a context manager to ensure proper cleanup.\nShare ServiceManager Instances: When possible, share a single ServiceManager instance across components in the same process.\nConfigure Services Properly: Provide appropriate configuration for connection parameters, retry logic, and timeouts.\nHandle Service Errors: Implement proper error handling for service-related exceptions.\nUse Dependency Injection: Leverage the service dependency system rather than manually creating service instances.\n\nThe service layer architecture represents a significant improvement in the design of naq, eliminating code duplication, improving resource efficiency, and providing a solid foundation for future enhancements."
  },
  {
    "objectID": "architecture.html#connection-management",
    "href": "architecture.html#connection-management",
    "title": "Architecture Overview",
    "section": "Connection Management",
    "text": "Connection Management\nThe connection management system has been completely refactored to provide:\n\nContext Manager API: Modern, resource-safe connection management\nService Integration: Seamless integration with the service layer\nMonitoring Capabilities: Built-in connection metrics and health monitoring\nConfiguration-Driven: All connection parameters driven by configuration\n\n\nArchitecture Flow\nApplication Code\n    ↓ (Context Managers or Decorators)\nConnection Module (context_managers.py, utils.py, decorators.py)\n    ↓ (Optional Service Integration)\nConnectionService (services/connection.py)\n    ↓\nNATS Client\n\n\nKey Benefits\n\nAutomatic Resource Management: Context managers ensure proper cleanup\nReduced Boilerplate: Eliminates manual connection handling code\nConsistent Patterns: Uniform connection handling across all modules\nProduction Ready: Built-in monitoring and health checking\n\n\n\nContext Manager Implementation\nThe new connection management system provides several context managers for different use cases:\n# Basic NATS connection\nasync with nats_connection() as conn:\n    await conn.publish(\"subject\", b\"message\")\n\n# JetStream context\nasync with nats_jetstream() as (conn, js):\n    await js.add_stream(name=\"stream\", subjects=[\"stream.*\"])\n\n# KeyValue operations\nasync with nats_kv_store(\"bucket\") as kv:\n    await kv.put(\"key\", \"value\")\n\n\nConnection Monitoring\nThe system includes comprehensive monitoring capabilities:\nfrom naq.connection import connection_monitor\n\n# Track connection metrics\nprint(f\"Total connections: {connection_monitor.metrics.total_connections}\")\nprint(f\"Active connections: {connection_monitor.metrics.active_connections}\")\nprint(f\"Failed connections: {connection_monitor.metrics.failed_connections}\")\n\n\nIntegration with Service Layer\nThe connection management system integrates seamlessly with the service layer:\n# Using context managers with services\nasync with ServiceManager(config) as services:\n    connection_service = await services.get_service(ConnectionService)\n    \n    # Context managers can use the same underlying connections\n    async with nats_connection() as conn:\n        # Connection is managed by both the context manager and service layer\n        await conn.publish(\"subject\", b\"message\")\n\n\nMigration Path\nThe connection management system provides a clear migration path from legacy patterns:\n\nLegacy Pattern\n# Old approach - manual connection management\nnc = await get_nats_connection(url)\ntry:\n    js = await get_jetstream_context(nc)\n    await js.add_stream(config)\nfinally:\n    await close_nats_connection(nc)\n\n\nModern Pattern\n# New approach - automatic resource management\nasync with nats_jetstream(config) as (conn, js):\n    await js.add_stream(config)\n\n\n\nBackward Compatibility\nThe new connection management system maintains full backward compatibility with existing code. All legacy functions continue to work unchanged, while new code can benefit from the improved context manager API.\n\n\nProduction Considerations\nFor production deployments, the connection management system provides:\n\nConnection Pooling: Efficient reuse of connections across components\nHealth Monitoring: Built-in connection health checks and metrics\nError Handling: Comprehensive error handling and retry logic\nConfiguration Management: Centralized configuration for all connection parameters\n\nThis refactored connection management system significantly improves the reliability and maintainability of naq while providing a modern, Pythonic API for developers."
  },
  {
    "objectID": "api/queue.html",
    "href": "api/queue.html",
    "title": "Queue API",
    "section": "",
    "text": "The queue module provides the primary interface for adding jobs to naq."
  },
  {
    "objectID": "api/queue.html#queue-class",
    "href": "api/queue.html#queue-class",
    "title": "Queue API",
    "section": "Queue Class",
    "text": "Queue Class\nThe Queue class represents a job queue and is the main entry point for enqueuing tasks.\n\n\n\n\n\n\nNote\n\n\n\nFor simple, one-off enqueueing, you might prefer the helper functions like enqueue which manage the Queue instance for you.\n\n\n\nnaq.queue.Queue(name, nats_url, default_timeout, prefer_thread_local, services)\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\nname\nstr\nThe name of the queue. Defaults to naq_default_queue.\n\n\nnats_url\nstr\nThe URL of the NATS server. Defaults to nats://localhost:4222.\n\n\ndefault_timeout\nint | None\nThe default timeout in seconds for jobs in this queue.\n\n\nprefer_thread_local\nbool\nWhen True, reuse a thread-local connection/JS context.\n\n\nservices\nServiceManager | None\nOptional ServiceManager instance for dependency injection.\n\n\n\n\n\nMethods\n\nenqueue()\nEnqueues a job for immediate execution.\nasync def enqueue(\n    self,\n    func: Callable,\n    *args: Any,\n    max_retries: Optional[int] = 0,\n    retry_delay: RetryDelayType = 0,\n    depends_on: Optional[Union[str, List[str], Job, List[Job]]] = None,\n    timeout: Optional[int] = None,\n    **kwargs: Any\n) -&gt; Job\n\n\nenqueue_at()\nSchedules a job to be enqueued at a specific datetime.\nasync def enqueue_at(\n    self,\n    dt: datetime.datetime,\n    func: Callable,\n    *args: Any,\n    ...\n) -&gt; Job\n\n\nenqueue_in()\nSchedules a job to be enqueued after a timedelta.\nasync def enqueue_in(\n    self,\n    delta: timedelta,\n    func: Callable,\n    *args: Any,\n    ...\n) -&gt; Job\n\n\nschedule()\nSchedules a job to run on a recurring basis.\nasync def schedule(\n    self,\n    func: Callable,\n    *args: Any,\n    cron: Optional[str] = None,\n    interval: Optional[Union[timedelta, float, int]] = None,\n    repeat: Optional[int] = None,\n    ...\n) -&gt; Job\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\ncron\nstr\nA cron string (e.g., '*/5 * * * *') for the schedule.\n\n\ninterval\ntimedelta | float | int\nThe interval in seconds or as a timedelta between job runs.\n\n\nrepeat\nint | None\nThe number of times to repeat the job. None for indefinitely.\n\n\n\n\n\npurge()\nRemoves all jobs from the queue.\nasync def purge(self) -&gt; int\nReturns the number of jobs purged.\n\n\ncancel_scheduled_job()\nCancels a scheduled or recurring job.\nasync def cancel_scheduled_job(self, job_id: str) -&gt; bool\nReturns True if the job was found and canceled."
  },
  {
    "objectID": "api/queue.html#service-layer-integration",
    "href": "api/queue.html#service-layer-integration",
    "title": "Queue API",
    "section": "Service Layer Integration",
    "text": "Service Layer Integration\nThe Queue class integrates with the service layer architecture to provide efficient resource management and connection pooling. When using the service layer, the queue automatically leverages centralized services for NATS connections, stream management, and other resources.\n\nUsing Queue with ServiceManager\nThe recommended way to use the Queue class with the service layer is to provide a ServiceManager instance:\nimport asyncio\nfrom naq.queue import Queue\nfrom naq.services import ServiceManager\n\nasync def queue_with_services():\n    # Create service configuration\n    config = {\n        'nats': {\n            'url': 'nats://localhost:4222',\n            'max_reconnect_attempts': 5,\n            'reconnect_delay': 1.0\n        }\n    }\n    \n    # Use ServiceManager for lifecycle management\n    async with ServiceManager(config) as services:\n        # Create Queue with ServiceManager\n        queue = Queue(name='example', services=services)\n        \n        # Use the queue - it will automatically use the services\n        job = await queue.enqueue(my_function, arg1, arg2)\n        \n        # Services are automatically managed and cleaned up\n\n\nBenefits of Service Layer Integration\n\nConnection Pooling: Multiple queue instances can share the same NATS connection, reducing resource usage.\nCentralized Configuration: Connection parameters, retry logic, and timeouts are configured in one place.\nAutomatic Resource Management: Services are automatically initialized and cleaned up by the ServiceManager.\nDependency Injection: Services can depend on each other, creating a clean dependency graph.\n\n\n\nMigration from Direct Connection Management\n\nBefore (Direct Connection Management)\n# Old approach - direct connection management\nasync def old_queue_usage():\n    queue = Queue(name='example', nats_url='nats://localhost:4222')\n    \n    try:\n        job = await queue.enqueue(my_function, arg1, arg2)\n        # Use the queue\n    finally:\n        await queue.close()  # Manual cleanup\n\n\nAfter (Service Layer)\n# New approach - service layer\nasync def new_queue_usage():\n    config = {'nats': {'url': 'nats://localhost:4222'}}\n    \n    async with ServiceManager(config) as services:\n        queue = Queue(name='example', services=services)\n        \n        job = await queue.enqueue(my_function, arg1, arg2)\n        # No need to manually close - services are automatically managed\n\n\n\nAdvanced Service Configuration\nYou can provide detailed configuration for the services used by the queue:\nasync def advanced_service_config():\n    config = {\n        'nats': {\n            'url': 'nats://localhost:4222',\n            'max_reconnect_attempts': 10,\n            'reconnect_delay': 2.0,\n            'connection_timeout': 30.0,\n            'ping_interval': 60,\n            'max_outstanding_pings': 3\n        },\n        'events': {\n            'enabled': True,\n            'batch_size': 100,\n            'flush_interval': 1.0,\n            'max_buffer_size': 10000\n        }\n    }\n    \n    async with ServiceManager(config) as services:\n        queue = Queue(name='production', services=services)\n        # Queue will use the configured services\n\n\nSharing ServiceManager Across Queues\nMultiple queues can share the same ServiceManager for efficient resource usage:\nasync def shared_services_example():\n    config = {'nats': {'url': 'nats://localhost:4222'}}\n    \n    async with ServiceManager(config) as services:\n        # Create multiple queues sharing the same services\n        high_priority_queue = Queue(name='high_priority', services=services)\n        low_priority_queue = Queue(name='low_priority', services=services)\n        \n        # Both queues share the same connection and other services\n        job1 = await high_priority_queue.enqueue(urgent_task)\n        job2 = await low_priority_queue.enqueue(background_task)"
  },
  {
    "objectID": "api/queue.html#enqueue-functions",
    "href": "api/queue.html#enqueue-functions",
    "title": "Queue API",
    "section": "Enqueue Functions",
    "text": "Enqueue Functions\nThese helper functions provide a simpler way to enqueue jobs without needing to manage a Queue instance yourself. They are available in both async and sync versions.\n\nAsync Helpers\n\nnaq.enqueue()\nnaq.enqueue_at()\nnaq.enqueue_in()\nnaq.schedule()\nnaq.purge_queue()\nnaq.cancel_scheduled_job()\n\n\n\nSync Helpers\nFor use in synchronous code, naq provides sync versions of the enqueue functions. These functions automatically manage an event loop and use a thread-local connection for efficiency.\n\nnaq.enqueue_sync()\nnaq.enqueue_at_sync()\nnaq.enqueue_in_sync()\nnaq.schedule_sync()\nnaq.purge_queue_sync()\nnaq.cancel_scheduled_job_sync()\nnaq.close_sync_connections()"
  },
  {
    "objectID": "api/connection.html",
    "href": "api/connection.html",
    "title": "Connection Management API",
    "section": "",
    "text": "The naq library provides sophisticated NATS connection management with both traditional and context manager-based approaches.\n\n\nThe connection management system has been refactored to provide:\n\nContext Managers: Modern, Pythonic way to manage NATS connections\nConnection Utilities: Monitoring and testing utilities for production use\nDecorators: Function-based connection injection\nBackward Compatibility: Existing code continues to work without changes\n\n\n\n\n\n\nContext manager for NATS connections with automatic resource management.\nimport asyncio\nfrom naq.connection import nats_connection\n\nasync def example():\n    async with nats_connection() as conn:\n        await conn.publish(\"subject\", b\"message\")\nParameters: - config (Optional[Config]): Configuration object for the connection. If not provided, uses default configuration.\nReturns: - A NATS client connection that is automatically closed when exiting the context.\n\n\n\nContext manager for JetStream contexts.\nasync def example():\n    async with nats_connection() as conn:\n        async with jetstream_context(conn) as js:\n            await js.add_stream(name=\"stream\", subjects=[\"stream.*\"])\nParameters: - conn (NATSClient): A NATS client connection.\nReturns: - A JetStream context for working with streams and consumers.\n\n\n\nCombined context manager for NATS connection and JetStream.\nasync def example():\n    async with nats_jetstream() as (conn, js):\n        await js.add_stream(name=\"stream\", subjects=[\"stream.*\"])\nParameters: - config (Optional[Config]): Configuration object for the connection. If not provided, uses default configuration.\nReturns: - A tuple of (NATSClient, JetStreamContext) for working with both connections and JetStream.\n\n\n\nContext manager for NATS KeyValue operations.\nasync def example():\n    async with nats_kv_store(\"my_bucket\") as kv:\n        await kv.put(\"key\", \"value\")\n        result = await kv.get(\"key\")\nParameters: - bucket_name (str): Name of the KeyValue bucket. - config (Optional[Config]): Configuration object for the connection. If not provided, uses default configuration.\nReturns: - A KeyValue context for working with NATS KeyValue store.\n\n\n\n\n\n\nfrom naq.connection import connection_monitor\n\n# Monitor connection usage\nprint(f\"Total connections: {connection_monitor.metrics.total_connections}\")\nprint(f\"Active connections: {connection_monitor.metrics.active_connections}\")\nThe ConnectionMonitor class provides metrics for tracking connection usage:\n\ntotal_connections: Total number of connections created\nactive_connections: Number of currently active connections\nfailed_connections: Number of failed connection attempts\naverage_connection_time: Average time to establish connections\n\n\n\n\nfrom naq.connection import test_nats_connection, wait_for_nats_connection\n\n# Test connection health\nis_healthy = await test_nats_connection()\n\n# Wait for connection to be available\nawait wait_for_nats_connection(timeout=30)\nFunctions: - test_nats_connection(config=None): Tests if a NATS connection can be established. Returns True if successful. - wait_for_nats_connection(config=None, timeout=30): Waits for a NATS connection to be available. Returns True if connection becomes available within timeout.\n\n\n\n\n\n\nDecorator to inject NATS connection into functions.\nfrom naq.connection import with_nats_connection\n\n@with_nats_connection()\nasync def publish_message(conn, subject: str, message: bytes):\n    await conn.publish(subject, message)\nThe decorator automatically establishes a NATS connection and passes it as the first argument to the decorated function.\n\n\n\nDecorator to inject JetStream context into functions.\nfrom naq.connection import with_jetstream_context\n\n@with_jetstream_context()\nasync def create_stream(js, stream_name: str):\n    await js.add_stream(name=stream_name, subjects=[f\"{stream_name}.*\"])\nThe decorator automatically establishes a NATS connection and JetStream context, passing the JetStream context as the first argument to the decorated function.\n\n\n\n\nConnection parameters are configured through the standard naq configuration system:\nfrom naq.connection import Config, get_config\n\n# Get default configuration\nconfig = get_config()\n\n# Create custom configuration\ncustom_config = Config(\n    servers=[\"nats://localhost:4222\"],\n    client_name=\"my-app\",\n    max_reconnect_attempts=10,\n    reconnect_time_wait=5.0\n)\nConfiguration Options: - servers: List of NATS server URLs (default: [“nats://localhost:4222”]) - client_name: Client name for connection identification (default: “naq-client”) - max_reconnect_attempts: Maximum number of reconnection attempts (default: 5) - reconnect_time_wait: Time to wait between reconnection attempts in seconds (default: 2.0)\n\n\n\n\n\nfrom naq.connection import get_nats_connection, get_jetstream_context\n\nnc = await get_nats_connection(url)\ntry:\n    js = await get_jetstream_context(nc)\n    await js.add_stream(config)\nfinally:\n    await close_nats_connection(nc)\n\n\n\nfrom naq.connection import nats_jetstream\n\nasync with nats_jetstream(config) as (conn, js):\n    await js.add_stream(config)\n\n\n\n\nAll existing functions (get_nats_connection, get_jetstream_context, etc.) continue to work. The new context managers provide an alternative, more Pythonic approach while maintaining full compatibility.\n\n\nThe following legacy functions are still available for backward compatibility:\n\nget_nats_connection(url, prefer_thread_local=False): Gets a NATS client connection\nget_jetstream_context(nc, prefer_thread_local=False): Gets a JetStream context\nclose_nats_connection(url, thread_local=False): Closes a specific NATS connection\nclose_all_connections(): Closes all NATS connections\nensure_stream(js, stream_name, subjects): Ensures a JetStream stream exists\nConnectionManager: Manages NATS connections with pooling\n\n\n\n\n\nThe connection management system provides comprehensive error handling:\nfrom naq.exceptions import NaqConnectionError\n\ntry:\n    async with nats_connection() as conn:\n        await conn.publish(\"subject\", b\"message\")\nexcept NaqConnectionError as e:\n    print(f\"Connection error: {e}\")\n\n\n\n\nUse Context Managers: Always prefer context managers for automatic resource management\nHandle Exceptions: Implement proper error handling for connection-related exceptions\nMonitor Connections: Use the connection monitor to track connection usage in production\nTest Connections: Use the testing utilities to verify connection health before critical operations\nConfigure Appropriately: Set appropriate connection parameters for your environment"
  },
  {
    "objectID": "api/connection.html#overview",
    "href": "api/connection.html#overview",
    "title": "Connection Management API",
    "section": "",
    "text": "The connection management system has been refactored to provide:\n\nContext Managers: Modern, Pythonic way to manage NATS connections\nConnection Utilities: Monitoring and testing utilities for production use\nDecorators: Function-based connection injection\nBackward Compatibility: Existing code continues to work without changes"
  },
  {
    "objectID": "api/connection.html#context-managers",
    "href": "api/connection.html#context-managers",
    "title": "Connection Management API",
    "section": "",
    "text": "Context manager for NATS connections with automatic resource management.\nimport asyncio\nfrom naq.connection import nats_connection\n\nasync def example():\n    async with nats_connection() as conn:\n        await conn.publish(\"subject\", b\"message\")\nParameters: - config (Optional[Config]): Configuration object for the connection. If not provided, uses default configuration.\nReturns: - A NATS client connection that is automatically closed when exiting the context.\n\n\n\nContext manager for JetStream contexts.\nasync def example():\n    async with nats_connection() as conn:\n        async with jetstream_context(conn) as js:\n            await js.add_stream(name=\"stream\", subjects=[\"stream.*\"])\nParameters: - conn (NATSClient): A NATS client connection.\nReturns: - A JetStream context for working with streams and consumers.\n\n\n\nCombined context manager for NATS connection and JetStream.\nasync def example():\n    async with nats_jetstream() as (conn, js):\n        await js.add_stream(name=\"stream\", subjects=[\"stream.*\"])\nParameters: - config (Optional[Config]): Configuration object for the connection. If not provided, uses default configuration.\nReturns: - A tuple of (NATSClient, JetStreamContext) for working with both connections and JetStream.\n\n\n\nContext manager for NATS KeyValue operations.\nasync def example():\n    async with nats_kv_store(\"my_bucket\") as kv:\n        await kv.put(\"key\", \"value\")\n        result = await kv.get(\"key\")\nParameters: - bucket_name (str): Name of the KeyValue bucket. - config (Optional[Config]): Configuration object for the connection. If not provided, uses default configuration.\nReturns: - A KeyValue context for working with NATS KeyValue store."
  },
  {
    "objectID": "api/connection.html#connection-utilities",
    "href": "api/connection.html#connection-utilities",
    "title": "Connection Management API",
    "section": "",
    "text": "from naq.connection import connection_monitor\n\n# Monitor connection usage\nprint(f\"Total connections: {connection_monitor.metrics.total_connections}\")\nprint(f\"Active connections: {connection_monitor.metrics.active_connections}\")\nThe ConnectionMonitor class provides metrics for tracking connection usage:\n\ntotal_connections: Total number of connections created\nactive_connections: Number of currently active connections\nfailed_connections: Number of failed connection attempts\naverage_connection_time: Average time to establish connections\n\n\n\n\nfrom naq.connection import test_nats_connection, wait_for_nats_connection\n\n# Test connection health\nis_healthy = await test_nats_connection()\n\n# Wait for connection to be available\nawait wait_for_nats_connection(timeout=30)\nFunctions: - test_nats_connection(config=None): Tests if a NATS connection can be established. Returns True if successful. - wait_for_nats_connection(config=None, timeout=30): Waits for a NATS connection to be available. Returns True if connection becomes available within timeout."
  },
  {
    "objectID": "api/connection.html#decorators",
    "href": "api/connection.html#decorators",
    "title": "Connection Management API",
    "section": "",
    "text": "Decorator to inject NATS connection into functions.\nfrom naq.connection import with_nats_connection\n\n@with_nats_connection()\nasync def publish_message(conn, subject: str, message: bytes):\n    await conn.publish(subject, message)\nThe decorator automatically establishes a NATS connection and passes it as the first argument to the decorated function.\n\n\n\nDecorator to inject JetStream context into functions.\nfrom naq.connection import with_jetstream_context\n\n@with_jetstream_context()\nasync def create_stream(js, stream_name: str):\n    await js.add_stream(name=stream_name, subjects=[f\"{stream_name}.*\"])\nThe decorator automatically establishes a NATS connection and JetStream context, passing the JetStream context as the first argument to the decorated function."
  },
  {
    "objectID": "api/connection.html#configuration",
    "href": "api/connection.html#configuration",
    "title": "Connection Management API",
    "section": "",
    "text": "Connection parameters are configured through the standard naq configuration system:\nfrom naq.connection import Config, get_config\n\n# Get default configuration\nconfig = get_config()\n\n# Create custom configuration\ncustom_config = Config(\n    servers=[\"nats://localhost:4222\"],\n    client_name=\"my-app\",\n    max_reconnect_attempts=10,\n    reconnect_time_wait=5.0\n)\nConfiguration Options: - servers: List of NATS server URLs (default: [“nats://localhost:4222”]) - client_name: Client name for connection identification (default: “naq-client”) - max_reconnect_attempts: Maximum number of reconnection attempts (default: 5) - reconnect_time_wait: Time to wait between reconnection attempts in seconds (default: 2.0)"
  },
  {
    "objectID": "api/connection.html#migration-guide",
    "href": "api/connection.html#migration-guide",
    "title": "Connection Management API",
    "section": "",
    "text": "from naq.connection import get_nats_connection, get_jetstream_context\n\nnc = await get_nats_connection(url)\ntry:\n    js = await get_jetstream_context(nc)\n    await js.add_stream(config)\nfinally:\n    await close_nats_connection(nc)\n\n\n\nfrom naq.connection import nats_jetstream\n\nasync with nats_jetstream(config) as (conn, js):\n    await js.add_stream(config)"
  },
  {
    "objectID": "api/connection.html#backward-compatibility",
    "href": "api/connection.html#backward-compatibility",
    "title": "Connection Management API",
    "section": "",
    "text": "All existing functions (get_nats_connection, get_jetstream_context, etc.) continue to work. The new context managers provide an alternative, more Pythonic approach while maintaining full compatibility.\n\n\nThe following legacy functions are still available for backward compatibility:\n\nget_nats_connection(url, prefer_thread_local=False): Gets a NATS client connection\nget_jetstream_context(nc, prefer_thread_local=False): Gets a JetStream context\nclose_nats_connection(url, thread_local=False): Closes a specific NATS connection\nclose_all_connections(): Closes all NATS connections\nensure_stream(js, stream_name, subjects): Ensures a JetStream stream exists\nConnectionManager: Manages NATS connections with pooling"
  },
  {
    "objectID": "api/connection.html#error-handling",
    "href": "api/connection.html#error-handling",
    "title": "Connection Management API",
    "section": "",
    "text": "The connection management system provides comprehensive error handling:\nfrom naq.exceptions import NaqConnectionError\n\ntry:\n    async with nats_connection() as conn:\n        await conn.publish(\"subject\", b\"message\")\nexcept NaqConnectionError as e:\n    print(f\"Connection error: {e}\")"
  },
  {
    "objectID": "api/connection.html#best-practices",
    "href": "api/connection.html#best-practices",
    "title": "Connection Management API",
    "section": "",
    "text": "Use Context Managers: Always prefer context managers for automatic resource management\nHandle Exceptions: Implement proper error handling for connection-related exceptions\nMonitor Connections: Use the connection monitor to track connection usage in production\nTest Connections: Use the testing utilities to verify connection health before critical operations\nConfigure Appropriately: Set appropriate connection parameters for your environment"
  },
  {
    "objectID": "api/exceptions.html",
    "href": "api/exceptions.html",
    "title": "Exceptions",
    "section": "",
    "text": "The naq library uses a set of custom exceptions to indicate specific error conditions. All custom exceptions inherit from the base NaqException.\n\nNaqException\nThe base exception for all errors raised by naq.\n\n\nNaqConnectionError\nRaised when there is an issue connecting to the NATS server or a problem with the connection during an operation.\n\n\nConfigurationError\nRaised when there is an issue with the configuration of naq, such as providing invalid parameters to a Queue or Worker.\n\n\nSerializationError\nRaised if naq fails to serialize or deserialize a job. This can happen if you are using the json serializer and try to enqueue a job with un-serializable arguments (like complex objects).\n\n\nJobExecutionError\nRaised by Job.fetch_result() if the job failed during execution on the worker. The exception message will contain the original error and traceback from the worker.\n\n\nJobNotFoundError\nRaised by Job.fetch_result() if the result for the specified job_id cannot be found. This could be because the job has not completed yet, the result has expired and been cleaned up, or the job never existed."
  },
  {
    "objectID": "api/worker.html",
    "href": "api/worker.html",
    "title": "Worker API",
    "section": "",
    "text": "The worker module contains the Worker class, which is responsible for fetching and executing jobs from one or more queues."
  },
  {
    "objectID": "api/worker.html#naq.worker.worker",
    "href": "api/worker.html#naq.worker.worker",
    "title": "Worker API",
    "section": "naq.worker.Worker",
    "text": "naq.worker.Worker\nYou can start a worker from the command line using naq worker, but you can also create and run a Worker instance programmatically.\n\nnaq.worker.Worker(queues, nats_url, concurrency, worker_name, services, ...)\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\nqueues\nlist[str] | str\nA list of queue names to listen to. Defaults to the single naq_default_queue.\n\n\nnats_url\nstr\nThe URL of the NATS server.\n\n\nconcurrency\nint\nThe maximum number of jobs to process concurrently. Defaults to 10.\n\n\nworker_name\nstr | None\nA name for the worker, used for creating durable consumer names. A unique ID is generated if not provided.\n\n\nservices\nServiceManager | None\nOptional ServiceManager instance for dependency injection.\n\n\nheartbeat_interval\nint\nThe interval (in seconds) at which the worker sends a heartbeat. Defaults to 15.\n\n\nworker_ttl\nint\nThe time-to-live (in seconds) for the worker’s heartbeat. Defaults to 60.\n\n\nack_wait\nint | dict | None\nThe time (in seconds) the worker has to acknowledge a job before it’s re-delivered. Can be a global value or a per-queue dictionary.\n\n\nmodule_paths\nlist[str] | str | None\nA list of paths to add to sys.path to help the worker find your task modules.\n\n\n\n\n\nMethods\n\nrun()\nStarts the worker’s main processing loop. This is an async method.\nasync def run(self) -&gt; None\nThe worker will connect to NATS, subscribe to the specified queues, and start fetching jobs. It will run until a shutdown signal is received.\n\n\nrun_sync()\nA synchronous method to start the worker. This is useful when running a worker from a synchronous script.\ndef run_sync(self) -&gt; None\nThis method will block until the worker is shut down.\n\n\nlist_workers()\nA static method to list all active workers.\n@staticmethod\nasync def list_workers(nats_url: str = DEFAULT_NATS_URL) -&gt; list[dict]\nReturns a list of dictionaries, where each dictionary contains information about a worker (ID, status, hostname, etc.).\n\n\nlist_workers_sync()\nA synchronous version of list_workers()."
  },
  {
    "objectID": "api/worker.html#service-layer-integration",
    "href": "api/worker.html#service-layer-integration",
    "title": "Worker API",
    "section": "Service Layer Integration",
    "text": "Service Layer Integration\nThe Worker class integrates with the service layer architecture to provide efficient resource management and connection pooling. When using the service layer, the worker automatically leverages centralized services for NATS connections, job execution, and event logging.\n\nUsing Worker with ServiceManager\nThe recommended way to use the Worker class with the service layer is to provide a ServiceManager instance:\nimport asyncio\nfrom naq.worker import Worker\nfrom naq.services import ServiceManager\n\nasync def worker_with_services():\n    # Create service configuration\n    config = {\n        'nats': {\n            'url': 'nats://localhost:4222',\n            'max_reconnect_attempts': 5,\n            'reconnect_delay': 1.0\n        },\n        'jobs': {\n            'default_timeout': 3600,\n            'result_ttl': 604800\n        },\n        'events': {\n            'enabled': True,\n            'batch_size': 100,\n            'flush_interval': 1.0\n        }\n    }\n    \n    # Use ServiceManager for lifecycle management\n    async with ServiceManager(config) as services:\n        # Create Worker with ServiceManager\n        worker = Worker(\n            queues=['high_priority', 'low_priority'],\n            services=services,\n            concurrency=5\n        )\n        \n        # Run the worker - it will automatically use the services\n        await worker.run()\n        \n        # Services are automatically managed and cleaned up\n\n\nBenefits of Service Layer Integration\n\nConnection Pooling: Multiple workers can share the same NATS connection, reducing resource usage.\nCentralized Job Execution: Job execution is handled by the JobService, providing consistent error handling and result management.\nEvent Logging: Worker events are automatically logged through the EventService, providing comprehensive monitoring.\nAutomatic Resource Management: Services are automatically initialized and cleaned up by the ServiceManager.\n\n\n\nMigration from Direct Connection Management\n\nBefore (Direct Connection Management)\n# Old approach - direct connection management\nasync def old_worker_usage():\n    worker = Worker(\n        queues=['example'],\n        nats_url='nats://localhost:4222',\n        concurrency=10\n    )\n    \n    try:\n        await worker.run()\n    finally:\n        # Manual cleanup was required\n        pass\n\n\nAfter (Service Layer)\n# New approach - service layer\nasync def new_worker_usage():\n    config = {'nats': {'url': 'nats://localhost:4222'}}\n    \n    async with ServiceManager(config) as services:\n        worker = Worker(\n            queues=['example'],\n            services=services,\n            concurrency=10\n        )\n        \n        await worker.run()\n        # No need to manually manage connections - services are automatically managed\n\n\n\nAdvanced Service Configuration\nYou can provide detailed configuration for the services used by the worker:\nasync def advanced_worker_config():\n    config = {\n        'nats': {\n            'url': 'nats://localhost:4222',\n            'max_reconnect_attempts': 10,\n            'reconnect_delay': 2.0,\n            'connection_timeout': 30.0\n        },\n        'jobs': {\n            'default_timeout': 3600,\n            'result_ttl': 604800,\n            'cleanup_interval': 3600  # Clean up old results every hour\n        },\n        'events': {\n            'enabled': True,\n            'batch_size': 100,\n            'flush_interval': 1.0,\n            'max_buffer_size': 10000\n        }\n    }\n    \n    async with ServiceManager(config) as services:\n        worker = Worker(\n            queues=['production'],\n            services=services,\n            concurrency=20,\n            worker_name='production-worker-1'\n        )\n        \n        await worker.run()\n\n\nSharing ServiceManager Across Workers\nMultiple workers can share the same ServiceManager for efficient resource usage:\nasync def shared_worker_services():\n    config = {'nats': {'url': 'nats://localhost:4222'}}\n    \n    async with ServiceManager(config) as services:\n        # Create multiple workers sharing the same services\n        worker1 = Worker(\n            queues=['high_priority'],\n            services=services,\n            concurrency=5,\n            worker_name='high-priority-worker'\n        )\n        \n        worker2 = Worker(\n            queues=['low_priority'],\n            services=services,\n            concurrency=10,\n            worker_name='low-priority-worker'\n        )\n        \n        # Run workers concurrently\n        worker1_task = asyncio.create_task(worker1.run())\n        worker2_task = asyncio.create_task(worker2.run())\n        \n        # Wait for both workers (in a real scenario, you might run them indefinitely)\n        await asyncio.sleep(10)\n        \n        # Cancel worker tasks\n        worker1_task.cancel()\n        worker2_task.cancel()\n        \n        try:\n            await worker1_task\n            await worker2_task\n        except asyncio.CancelledError:\n            pass\n\n\nWorker Event Integration\nWhen using the service layer, workers automatically integrate with the event system:\nasync def worker_with_events():\n    config = {\n        'nats': {'url': 'nats://localhost:4222'},\n        'events': {\n            'enabled': True,\n            'batch_size': 50,\n            'flush_interval': 0.5\n        }\n    }\n    \n    async with ServiceManager(config) as services:\n        worker = Worker(\n            queues=['example'],\n            services=services,\n            concurrency=3\n        )\n        \n        # The worker will automatically log:\n        # - STARTED events when jobs begin processing\n        # - COMPLETED events when jobs finish successfully\n        # - FAILED events when jobs encounter errors\n        # - STATUS_CHANGED events for job state transitions\n        \n        await worker.run()\n\n\nCustom Worker with Services\nFor advanced use cases, you can create custom worker classes that leverage the service layer:\nfrom naq.worker import Worker\nfrom naq.services import ServiceManager, JobService, EventService\n\nclass CustomWorker(Worker):\n    \"\"\"Custom worker with enhanced service integration.\"\"\"\n    \n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._job_service = None\n        self._event_service = None\n    \n    async def _setup_services(self):\n        \"\"\"Set up service references.\"\"\"\n        if self._services:\n            self._job_service = await self._services.get_service(JobService)\n            self._event_service = await self._services.get_service(EventService)\n    \n    async def process_job(self, job):\n        \"\"\"Override to add custom processing with services.\"\"\"\n        await self._setup_services()\n        \n        # Log custom event before processing\n        if self._event_service:\n            from naq.models import JobEvent, JobEventType\n            custom_event = JobEvent(\n                job_id=job.job_id,\n                event_type=\"custom_processing_started\",\n                queue_name=job.queue_name,\n                worker_id=self.worker_name,\n                details={\"custom_data\": \"pre-processing\"}\n            )\n            await self._event_service.log_event(custom_event)\n        \n        # Process the job using the job service\n        result = await self._job_service.execute_job(job)\n        \n        # Log custom event after processing\n        if self._event_service:\n            custom_event = JobEvent(\n                job_id=job.job_id,\n                event_type=\"custom_processing_completed\",\n                queue_name=job.queue_name,\n                worker_id=self.worker_name,\n                details={\"duration_ms\": result.duration_ms}\n            )\n            await self._event_service.log_event(custom_event)\n        \n        return result\n\n# Using the custom worker\nasync def custom_worker_example():\n    config = {'nats': {'url': 'nats://localhost:4222'}}\n    \n    async with ServiceManager(config) as services:\n        worker = CustomWorker(\n            queues=['example'],\n            services=services,\n            concurrency=2\n        )\n        \n        await worker.run()\nThe service layer integration provides a robust foundation for building efficient, maintainable worker applications with naq, eliminating connection duplication and providing clean resource management throughout the system."
  },
  {
    "objectID": "installation.html",
    "href": "installation.html",
    "title": "Installation",
    "section": "",
    "text": "You can install naq directly from PyPI using pip. A Python version of 3.12 or higher is required.\npip install naq\n\n\nFor faster installation, you can also use modern package managers like uv or pixi:\n# Using uv\nuv pip install naq\n\n# Using pixi\npixi add naq\n\n\n\nnaq comes with an optional web-based dashboard for real-time monitoring. To install the necessary dependencies (including Sanic and Datastar), use the dashboard extra:\npip install naq[dashboard]"
  },
  {
    "objectID": "installation.html#installing-naq",
    "href": "installation.html#installing-naq",
    "title": "Installation",
    "section": "",
    "text": "You can install naq directly from PyPI using pip. A Python version of 3.12 or higher is required.\npip install naq\n\n\nFor faster installation, you can also use modern package managers like uv or pixi:\n# Using uv\nuv pip install naq\n\n# Using pixi\npixi add naq\n\n\n\nnaq comes with an optional web-based dashboard for real-time monitoring. To install the necessary dependencies (including Sanic and Datastar), use the dashboard extra:\npip install naq[dashboard]"
  },
  {
    "objectID": "installation.html#setting-up-nats",
    "href": "installation.html#setting-up-nats",
    "title": "Installation",
    "section": "Setting Up NATS",
    "text": "Setting Up NATS\nnaq requires a running NATS server with JetStream enabled to function. JetStream provides the persistence layer for jobs and results.\n\nUsing Docker (Recommended)\nThe easiest way to get a NATS server running for development is by using the provided Docker Compose file.\n\nNavigate to the docker directory in the project root: bash     cd /path/to/naq/docker\nStart the NATS server in detached mode: bash     docker-compose up -d\n\nThis will start a NATS server on localhost:4222 with JetStream enabled and ready to use.\n\n\nManual Setup\nIf you prefer to run a NATS server manually, ensure that you start it with the -js flag to enable JetStream:\nnats-server -js\nRefer to the official NATS documentation for detailed installation instructions for your operating system."
  },
  {
    "objectID": "installation.html#troubleshooting",
    "href": "installation.html#troubleshooting",
    "title": "Installation",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\n\n\n\n\n\nNote\n\n\n\nConnection Issues?\n\nEnsure your NATS server is running and accessible from where you are running your application and workers.\nVerify that JetStream is enabled. You can check the server logs for a line confirming “JetStream is enabled.”\nBy default, naq attempts to connect to nats://localhost:4222. If your server is elsewhere, set the NATS_URL environment variable."
  },
  {
    "objectID": "architecture.html#utils-package-architecture",
    "href": "architecture.html#utils-package-architecture",
    "title": "Architecture Overview",
    "section": "Utils Package Architecture",
    "text": "Utils Package Architecture\nThe utils package is a comprehensive collection of utilities extracted from various modules throughout the codebase as part of Task 08. It provides common functionality used across all components of naq, eliminating code duplication and providing a consistent set of tools for common operations.\n\nPackage Structure\nThe utils package is organized into specialized modules, each focusing on a specific area of functionality:\nsrc/naq/utils/\n├── __init__.py\n├── async_helpers.py      # Async/sync conversion utilities\n├── config.py            # Configuration management\n├── context_managers.py  # Common context managers\n├── decorators.py        # Reusable decorators\n├── error_handling.py    # Centralized error handling\n├── logging.py           # Logging utilities\n├── nats_helpers.py      # NATS-specific utilities\n├── retry.py             # Retry mechanisms\n├── serialization.py     # Serialization helpers\n├── timing.py            # Timing and benchmarking\n├── types.py             # Common type definitions\n└── validation.py        # Validation utilities\n\n\nCore Utilities\n\n1. Async Helpers (async_helpers.py)\nProvides utilities for bridging synchronous and asynchronous code:\n\nrun_async(): Execute async functions from sync code\nrun_sync(): Execute sync functions from async code\nasync_to_sync(): Convert async functions to sync functions\nsync_to_async(): Convert sync functions to async functions\nasyncify(): Decorator to make sync functions async-compatible\n\n\n\n2. Configuration Management (config.py)\nCentralized configuration system with multiple source support:\n\nConfigManager: Manages configuration from multiple sources with priority-based merging\nConfigSource: Base class for configuration sources\nEnvironmentConfigSource`: Reads from environment variables\nFileConfigSource: Reads from YAML/JSON files\nDictConfigSource: Uses provided dictionaries\nConfiguration dataclasses: Predefined configuration structures for different components\n\n\n\n3. Context Managers (context_managers.py)\nCommon context managers for resource management:\n\nmanaged_resource(): Generic resource management with cleanup\ntimeout_context(): Timeout management for operations\nretry_context(): Retry logic with context management\nbenchmark_context(): Performance benchmarking\n\n\n\n4. Decorators (decorators.py)\nReusable decorators for common patterns:\n\nretry_decorator(): Automatic retry logic with configurable strategies\ntimeout_decorator(): Timeout enforcement for functions\nbenchmark_decorator(): Performance measurement\nlog_errors(): Automatic error logging\nvalidate_args(): Argument validation\n\n\n\n5. Error Handling (error_handling.py)\nCentralized error handling and reporting:\n\nErrorContext: Rich error context with metadata\nErrorCategory: Categorization of errors\nErrorHandler: Centralized error handling strategies\nErrorReporter: Structured error reporting\nRecovery strategies: Configurable recovery mechanisms\n\n\n\n6. Logging (logging.py)\nComprehensive logging utilities:\n\nStructured logging: Context-aware logging with rich metadata\nPerformance logging: Specialized logging for performance metrics\nLog handler management: Centralized log handler configuration\nLog correlation: Request/operation ID tracking\n\n\n\n7. NATS Helpers (nats_helpers.py)\nNATS-specific utilities extracted from connection modules:\n\nConnection management: Context managers for NATS connections\nStream operations: Stream creation and management utilities\nKV store operations: Key-value store operations with retry logic\nSubject utilities: Subject building and parsing\nMessage handling: Message publishing and subscription utilities\nConnection monitoring: Connection metrics and health monitoring\n\n\n\n8. Retry Mechanisms (retry.py)\nComprehensive retry system:\n\nRetryConfig: Configurable retry strategies\nretry_async(): Async retry with exponential backoff\nretry(): Sync retry with exponential backoff\nRetry strategies: Linear, exponential, custom strategies\nJitter support: Avoid thundering herd problems\n\n\n\n9. Serialization (serialization.py)\nSerialization utilities for jobs and results:\n\nJobSerializer: Job serialization with multiple formats\nResultSerializer: Result serialization with TTL support\nSecureSerializer: Security-focused serialization\nSerializer protocol: Extensible serialization interface\n\n\n\n10. Timing and Benchmarking (timing.py)\nPerformance measurement and timing utilities:\n\nTimer: High-resolution timing for operations\nAsyncTimer: Async-compatible timing\nBenchmark: Comprehensive benchmarking tools\nScheduler: Time-based scheduling utilities\nPerformance tracking: Long-term performance monitoring\n\n\n\n11. Type Definitions (types.py)\nComprehensive type definitions for the entire system:\n\nBasic type aliases: Common types used throughout the codebase\nProtocol definitions: Interface definitions for common patterns\nGeneric types: Reusable generic types\nType validation helpers: Runtime type checking utilities\nModel types: Type definitions for core models\nConfiguration types: Type definitions for configuration\n\n\n\n12. Validation (validation.py)\nComprehensive validation utilities:\n\nType validation: Runtime type checking\nRange validation: Numeric range checking\nChoice validation: Enum/value choice validation\nString validation: String format validation\nURL/email validation: Specialized string validation\nDictionary validation: Nested dictionary validation\nDataclass validation: Automatic dataclass validation\n\n\n\n\nIntegration with Core Components\nThe utils package integrates seamlessly with all core components of naq:\n\nQueue Integration\nfrom naq.utils import retry_async, ErrorContext, ErrorCategory\nfrom naq.utils.nats_helpers import nats_jetstream_context\n\nclass Queue:\n    async def enqueue(self, func, *args, **kwargs):\n        async with nats_jetstream_context() as (conn, js):\n            return await retry_async(\n                self._enqueue_impl,\n                func, args, kwargs, js\n            )\n\n\nWorker Integration\nfrom naq.utils import Timer, ErrorContext\nfrom naq.utils.logging import get_structured_logger\n\nclass Worker:\n    async def process_job(self, job):\n        with Timer() as timer:\n            logger = get_structured_logger(\"worker\")\n            try:\n                result = await job.execute()\n                logger.info(\"Job completed\", job_id=job.id, duration=timer.duration)\n                return result\n            except Exception as e:\n                error_context = ErrorContext(\n                    operation=\"job_execution\",\n                    exception=e,\n                    job_id=job.id\n                )\n                error_context.category = ErrorCategory.EXECUTION\n                logger.error(\"Job failed\", error_context=error_context)\n                raise\n\n\nService Layer Integration\nfrom naq.utils.config import ConfigManager, load_naq_config\nfrom naq.utils.types import Result, Error\n\nclass ServiceManager:\n    def __init__(self, config=None):\n        self.config = config or load_naq_config()\n        self._config_manager = ConfigManager([DictConfigSource(self.config)])\n\n\n\nBenefits of the Utils Package\n\n1. Code Consolidation\n\nEliminates duplication of common patterns across 44+ locations\nProvides single source of truth for common functionality\nReduces maintenance burden and bug potential\n\n\n\n2. Consistency\n\nUnified error handling across all components\nConsistent logging formats and correlation\nStandardized retry and timeout behavior\n\n\n\n3. Testability\n\nAll utilities are thoroughly tested\nMockable interfaces for easy unit testing\nClear separation of concerns\n\n\n\n4. Extensibility\n\nProtocol-based design allows easy extension\nPlugin architecture for custom serializers, validators, etc.\nConfigurable behavior through settings\n\n\n\n5. Performance\n\nOptimized implementations for critical paths\nMinimal overhead for common operations\nEfficient resource management\n\n\n\n\nMigration Path\nThe utils package provides a clear migration path from the old scattered utilities:\n\nBefore: Scattered Utilities\n# In queue.py\ndef _get_nats_connection(self):\n    # Custom connection logic\n    pass\n\n# In worker.py\ndef _get_nats_connection(self):\n    # Similar but slightly different connection logic\n    pass\n\n# In scheduler.py\ndef _get_nats_connection(self):\n    # Another variation of connection logic\n    pass\n\n\nAfter: Centralized Utils\n# In all components\nfrom naq.utils.nats_helpers import nats_connection_context\n\nasync def _get_nats_connection(self):\n    async with nats_connection_context() as conn:\n        return conn\n\n\n\nBest Practices\n\n1. Using Utils in Components\n# Import specific utilities\nfrom naq.utils import retry_async, Timer, ErrorContext\nfrom naq.utils.nats_helpers import nats_jetstream_context\nfrom naq.utils.logging import get_structured_logger\n\n# Use in component methods\nasync def process_request(self, request):\n    with Timer() as timer:\n        logger = get_structured_logger(\"component\")\n        \n        try:\n            async with nats_jetstream_context() as (conn, js):\n                result = await retry_async(\n                    self._process_impl,\n                    request, js\n                )\n                \n                logger.info(\n                    \"Request processed\",\n                    request_id=request.id,\n                    duration=timer.duration\n                )\n                return result\n                \n        except Exception as e:\n            error_context = ErrorContext(\n                operation=\"request_processing\",\n                exception=e,\n                request_id=request.id\n            )\n            logger.error(\"Request failed\", error_context=error_context)\n            raise\n\n\n2. Configuration Management\nfrom naq.utils.config import ConfigManager, EnvironmentConfigSource, FileConfigSource\n\n# Create configuration manager with multiple sources\nconfig_manager = ConfigManager([\n    EnvironmentConfigSource(prefix=\"MYAPP_\"),\n    FileConfigSource(\"config.yaml\"),\n    DictConfigSource({\"default\": \"values\"})\n])\n\n# Use typed configuration\nmax_retries = config_manager.get_typed(\"max_retries\", int, 3)\ntimeout = config_manager.get_required_typed(\"timeout\", float)\n\n\n3. Error Handling\nfrom naq.utils import ErrorContext, ErrorCategory, ErrorReporter\n\nasync def risky_operation(self):\n    try:\n        return await self._do_risky_thing()\n    except Exception as e:\n        error_context = ErrorContext(\n            operation=\"risky_operation\",\n            exception=e,\n            component=self.__class__.__name__\n        )\n        error_context.category = ErrorCategory.EXECUTION\n        error_context.add_metadata({\"attempt\": self._attempt})\n        \n        reporter = ErrorReporter()\n        await reporter.report_error(error_context)\n        raise\n\n\n4. Performance Monitoring\nfrom naq.utils import Timer, Benchmark\nfrom naq.utils.timing import Scheduler\n\n# Time individual operations\nasync def process_item(self, item):\n    with Timer() as timer:\n        result = await self._process_item_impl(item)\n        \n        # Log timing metrics\n        self.metrics.observe(\"process_item_duration\", timer.duration)\n        return result\n\n# Benchmark operations\nasync def benchmark_processing(self):\n    benchmark = Benchmark()\n    \n    async with benchmark:\n        for i in range(1000):\n            await self.process_item(f\"item_{i}\")\n    \n    print(f\"Processed 1000 items in {benchmark.total_time:.2f}s\")\n    print(f\"Average time: {benchmark.average_time:.4f}s\")\n    print(f\"Min time: {benchmark.min_time:.4f}s\")\n    print(f\"Max time: {benchmark.max_time:.4f}s\")\nThe utils package represents a significant architectural improvement for naq, providing a solid foundation of reusable, well-tested utilities that eliminate code duplication and ensure consistency across all components of the system."
  },
  {
    "objectID": "api/index.html#utils-package",
    "href": "api/index.html#utils-package",
    "title": "API Reference",
    "section": "Utils Package",
    "text": "Utils Package\nThe utils package provides a comprehensive collection of utilities extracted from various modules throughout the codebase. It offers common functionality used across all components of naq, eliminating code duplication and providing consistent tools for common operations.\n\nutils Package: Core utilities including async helpers, configuration management, error handling, logging, NATS helpers, retry mechanisms, serialization, timing, types, and validation.\n\n\nCore Utils Modules\n\nasync_helpers: Utilities for bridging synchronous and asynchronous code, including run_async(), run_sync(), and conversion decorators.\nconfig: Configuration management system with multiple source support, including ConfigManager, environment variables, file-based config, and dataclass loading.\ncontext_managers: Common context managers for resource management, including managed_resource(), timeout_context(), and retry_context().\ndecorators: Reusable decorators for common patterns, including retry_decorator(), timeout_decorator(), benchmark_decorator(), and log_errors().\nerror_handling: Centralized error handling and reporting, including ErrorContext, ErrorCategory, ErrorHandler, and recovery strategies.\nlogging: Comprehensive logging utilities, including structured logging, performance logging, log handler management, and log correlation.\nnats_helpers: NATS-specific utilities extracted from connection modules, including connection management, stream operations, KV store operations, subject utilities, and message handling.\nretry: Comprehensive retry system with RetryConfig, retry_async(), retry(), various retry strategies, and jitter support.\nserialization: Serialization utilities for jobs and results, including JobSerializer, ResultSerializer, SecureSerializer, and extensible serializer protocols.\ntiming: Performance measurement and timing utilities, including Timer, AsyncTimer, Benchmark, Scheduler, and performance tracking.\ntypes: Comprehensive type definitions for the entire system, including basic type aliases, protocol definitions, generic types, validation helpers, and model types.\nvalidation: Comprehensive validation utilities, including type validation, range validation, choice validation, string validation, URL/email validation, dictionary validation, and dataclass validation.\n\n\n\nUtils Usage Examples\nThe utils package is designed to be used throughout all components of naq. Here are some common usage patterns:\n\nConfiguration Management\nfrom naq.utils.config import ConfigManager, EnvironmentConfigSource, load_naq_config\n\n# Load configuration with multiple sources\nconfig_manager = ConfigManager([\n    EnvironmentConfigSource(prefix=\"NAQ_\"),\n    FileConfigSource(\"naq.yaml\")\n])\n\n# Get typed configuration values\nmax_retries = config_manager.get_typed(\"job.max_retries\", int, 3)\ntimeout = config_manager.get_required_typed(\"connection.timeout\", float)\n\n# Load predefined configuration structures\nnaq_config = load_naq_config(config_manager)\n\n\nError Handling\nfrom naq.utils import ErrorContext, ErrorCategory, ErrorReporter\n\nasync def process_job(job):\n    try:\n        result = await job.execute()\n        return result\n    except Exception as e:\n        error_context = ErrorContext(\n            operation=\"job_execution\",\n            exception=e,\n            job_id=job.id\n        )\n        error_context.category = ErrorCategory.EXECUTION\n        \n        reporter = ErrorReporter()\n        await reporter.report_error(error_context)\n        raise\n\n\nNATS Operations\nfrom naq.utils.nats_helpers import nats_jetstream_context, create_stream_with_retry\n\nasync def setup_stream():\n    async with nats_jetstream_context() as (conn, js):\n        stream_info = await create_stream_with_retry(\n            js,\n            StreamConfigHelper(\n                name=\"MY_STREAM\",\n                subjects=[\"my.subject.*\"]\n            )\n        )\n        return stream_info\n\n\nPerformance Monitoring\nfrom naq.utils import Timer, Benchmark\nfrom naq.utils.logging import get_structured_logger\n\nasync def process_with_monitoring(item):\n    logger = get_structured_logger(\"processor\")\n    \n    with Timer() as timer:\n        result = await self._process_item(item)\n        \n        logger.info(\n            \"Item processed\",\n            item_id=item.id,\n            duration=timer.duration\n        )\n        \n        return result\n\ndef benchmark_processing():\n    benchmark = Benchmark()\n    \n    with benchmark:\n        for i in range(1000):\n            process_with_monitoring(f\"item_{i}\")\n    \n    print(f\"Average time: {benchmark.average_time:.4f}s\")\n\n\nRetry Logic\nfrom naq.utils.retry import RetryConfig, retry_async\n\nasync def unreliable_operation():\n    # This might fail\n    pass\n\nasync def robust_operation():\n    retry_config = RetryConfig(\n        max_attempts=3,\n        base_delay=1.0,\n        max_delay=10.0,\n        retryable_exceptions=(ConnectionError, TimeoutError)\n    )\n    \n    return await retry_async(unreliable_operation, config=retry_config)\nFor more detailed information about the utils package architecture and comprehensive API documentation, see the Utils Package Architecture section in the architecture documentation."
  }
]