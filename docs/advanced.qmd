---
title: "Advanced Usage"
---

This section covers advanced features and configuration options for optimizing `naq` in production environments.

## Efficient Connection Handling & Batching

When enqueuing many jobs in a tight loop, creating a new NATS connection for each job is inefficient. `naq` provides several ways to manage connections for high-throughput scenarios.

### Using a `Queue` Instance with ServiceManager (Async)

For asynchronous applications, the most efficient way to enqueue jobs is to instantiate a `Queue` object with a ServiceManager. The `Queue` instance achieves its efficiency by leveraging the ServiceManager for connection pooling and reuse, ensuring optimal resource utilization.

```python
# async_batch_enqueue.py
import asyncio
from naq.queue import Queue
from naq.services import ServiceManager

async def my_task(i):
    return f"Processed item {i}"

async def main():
    # Create a ServiceManager and Queue instance for the 'high_volume' queue
    async with ServiceManager() as services:
        queue = Queue(name="high_volume", services=services)

        print("Enqueuing 1,000 jobs using connection pooling...")
        tasks = []
        for i in range(1000):
            task = queue.enqueue(my_task, i)
            tasks.append(task)

        await asyncio.gather(*tasks)
        print("All jobs enqueued.")

        # The ServiceManager handles connection pooling and cleanup automatically
        # No need to manually close connections

if __name__ == "__main__":
    asyncio.run(main())
```

### Thread-Local Connections (Sync)

The synchronous helper functions (`enqueue_sync`, `enqueue_at_sync`, etc.) are optimized for batching out of the box. They automatically use a **thread-local NATS connection**. This means that all calls to these functions from the same thread will reuse the same connection, avoiding the overhead of reconnecting each time.

```python
# sync_batch_enqueue.py
from naq import enqueue_sync, close_sync_connections

def my_task(i):
    return f"Processed item {i}"

def main():
    print("Enqueuing 1,000 jobs using a thread-local connection...")
    for i in range(1000):
        enqueue_sync(my_task, i)
    print("All jobs enqueued.")

    # Optionally, you can explicitly close the thread-local connection
    # when you are done with a batch. This is not required, as connections
    # are also closed on process exit.
    close_sync_connections()
    print("Thread-local connection closed.")

if __name__ == "__main__":
    main()
```

## Configuration via Environment Variables

Many of `naq`'s settings can be configured using environment variables, which is ideal for production and containerized deployments.

| Variable                      | Default                        | Description                                                                                             |
| ----------------------------- | ------------------------------ | ------------------------------------------------------------------------------------------------------- |
| `NAQ_NATS_URL`                | `nats://localhost:4222`        | The URL of the NATS server.                                                                             |
| `NAQ_DEFAULT_QUEUE`           | `naq_default_queue`            | The default queue name used when none is specified.                                                     |
| `NAQ_JOB_SERIALIZER`          | `pickle`                       | The serializer for jobs. Can be `pickle` or `json`. **See security note below.**                        |
| `NAQ_DEFAULT_RESULT_TTL`      | `604800` (7 days)              | Default time-to-live (in seconds) for job results stored in NATS.                                       |
| `NAQ_SCHEDULER_LOCK_TTL`      | `30`                           | TTL (in seconds) for the scheduler's high-availability leader lock.                                       |
| `NAQ_WORKER_TTL`              | `60`                           | TTL (in seconds) for a worker's heartbeat. If a worker is silent for this long, it's considered dead.    |
| `NAQ_WORKER_HEARTBEAT_INTERVAL` | `15`                           | How often (in seconds) a worker sends a heartbeat to NATS.                                              |
| `NAQ_LOG_LEVEL`               | `CRITICAL`                     | The logging level for `naq` components. Can be `DEBUG`, `INFO`, `WARNING`, `ERROR`.                       |

## Job Serialization (`pickle` vs. `json`)

`naq` uses a serializer to convert job data (the function and its arguments) into a format that can be stored in NATS. You can choose between two built-in serializers.

### `pickle` (Default)

-   **Pros**: Can serialize almost any Python object, including complex custom classes, lambdas, and functions defined in a REPL.
-   **Cons**: **Not secure**. A malicious actor who can enqueue jobs could craft a `pickle` payload that executes arbitrary code on your workers.

### `json` (Recommended for Production)

-   **Pros**: **Secure**. Only serializes basic data types (strings, numbers, lists, dicts). Functions are referenced by their import path (e.g., `my_app.tasks.process_data`), not serialized directly. This prevents arbitrary code execution.
-   **Cons**: Less flexible. Cannot serialize complex Python objects that don't have a natural JSON representation.

To use the `json` serializer, set the following environment variable:

```bash
export NAQ_JOB_SERIALIZER=json
```

::: {.callout-warning}
**Security Warning**

It is **strongly recommended** to use the `json` serializer in any environment where the job producer is not fully trusted.
:::

## Event Logging & Monitoring

`naq` includes a powerful, built-in event logging and monitoring system that provides deep visibility into your job queue. This system captures all key events throughout a job's lifecycle—from enqueuing to completion or failure—and stores them durably in NATS JetStream.

### Key Concepts

The event system is composed of a few core components, with the EventService serving as the central point for event logging and consumption:

-   **`EventService`**: The central service that manages all event-related operations, including logging, storage, and processing. It provides a unified interface for event management across the system.
-   **`JobEvent`**: A structured data object representing a single event in a job's lifecycle (e.g., `ENQUEUED`, `STARTED`, `FAILED`).
-   **`AsyncJobEventLogger`**: A high-performance, non-blocking logger that buffers events and flushes them to NATS in batches. This is integrated into the core `naq` components (`Queue`, `Worker`, `Scheduler`) through the EventService.
-   **`NATSJobEventStorage`**: The storage backend that persists events to a dedicated NATS JetStream stream, accessed through the EventService.
-   **`AsyncJobEventProcessor`**: A real-time event processor that allows you to subscribe to event streams and react to events as they happen, using the EventService for event retrieval.
-   **`SharedEventLoggerManager`**: A singleton manager that provides centralized event logging across all components, coordinated through the EventService.

### Enabling Event Logging

Event logging is controlled by the `NAQ_EVENTS_ENABLED` environment variable. By default, it is disabled.

```bash
# Enable event logging
export NAQ_EVENTS_ENABLED=true
```

Once enabled, `naq` workers and queues will automatically start logging events without any code changes.

### Configuration

The event system is highly configurable through environment variables:

| Variable                      | Default                        | Description                                                                                             |
| ----------------------------- | ------------------------------ | ------------------------------------------------------------------------------------------------------- |
| `NAQ_EVENTS_ENABLED`          | `False`                        | Whether to enable the event logging system.                                                             |
| `NAQ_EVENT_STORAGE_TYPE`      | `nats`                         | The type of storage backend (currently only `nats` is supported).                                       |
| `NAQ_EVENT_STORAGE_URL`       | `nats://localhost:4222`        | The URL of the NATS server for event storage.                                                           |
| `NAQ_EVENT_STREAM_NAME`       | `NAQ_JOB_EVENTS`               | The name of the JetStream stream where events are stored.                                               |
| `NAQ_EVENT_SUBJECT_PREFIX`    | `naq.jobs.events`              | The base subject prefix for publishing events.                                                           |

### Monitoring Events in Real-Time

`naq` provides a built-in CLI command to monitor events in real-time. This is incredibly useful for debugging and observing the behavior of your queues during development and in production.

To start monitoring, simply run:

```bash
naq events
```

This command will start an `AsyncJobEventProcessor` with a default handler that prints a formatted, colorized representation of each event to your console. You will see output like this:

```
12:34:56.789 [ENQUEUED]   job-abc-123 on queue 'default'
12:34:56.790 [STARTED]    job-abc-123 by worker 'worker-1'
12:35:01.123 [COMPLETED]  job-abc-123 by worker 'worker-1' (duration: 4333ms)
```

### Building Reactive Applications with Event Handlers

The real power of the event system lies in its ability to let you build reactive, event-driven applications. You can register custom handlers to react to specific job events through the EventService, which serves as the central point for event consumption.

Here's an example of how to set up a custom event processor to send a Slack notification whenever a job fails, using the EventService:

```python
# custom_handler.py
import asyncio
from naq.services import ServiceManager
from naq.events import AsyncJobEventProcessor, JobEventType

async def send_failure_alert(event: naq.events.JobEvent):
    """Sends a Slack alert when a job fails."""
    print(f"ALERT: Job {event.job_id} failed!")
    print(f"  - Error: {event.error_message}")
    print(f"  - Queue: {event.queue_name}")
    print(f"  - Worker: {event.worker_id}")
    # In a real application, you would integrate with the Slack API here.
    # await slack_client.post_message(channel="#alerts", text=f"Job {event.job_id} failed!")

async def main():
    # Create ServiceManager
    async with ServiceManager() as services:
        # Get EventService from the service manager
        event_service = await services.get_service(EventService)
        
        # Create event processor using the EventService
        processor = AsyncJobEventProcessor(event_service.get_event_storage())

        # Register our custom handler for FAILED events
        processor.add_handler(JobEventType.FAILED, send_failure_alert)

        # Start the processor
        await processor.start()

        try:
            # Keep the processor running indefinitely
            while True:
                await asyncio.sleep(1)
        finally:
            # Ensure the processor is stopped cleanly
            await processor.stop()

if __name__ == "__main__":
    asyncio.run(main())
```

You would run this script alongside your workers and other `naq` components. It will connect to the same NATS server and receive a copy of all `FAILED` events, allowing you to trigger custom logic.

### Event Data Structure

Each `JobEvent` contains rich contextual information. Here's a breakdown of the data available for a `COMPLETED` event:

```json
{
  "job_id": "job-xyz-789",
  "event_type": "completed",
  "timestamp": 1678886400.123,
  "worker_id": "worker-2",
  "queue_name": "high_priority",
  "message": "Job completed successfully",
  "details": {
    "result_type": "str"
  },
  "error_type": null,
  "error_message": null,
  "duration_ms": 2500,
  "nats_subject": "naq.jobs.events.job-xyz-789.worker.worker-2.completed",
  "nats_sequence": 12345
}
```

This structured data makes it easy to build complex monitoring, analytics, and alerting systems on top of `naq`'s event stream.

## Building Event-Driven Applications with naq

The event-driven architecture of naq enables you to build reactive applications that respond to job lifecycle events in real-time. This section covers how to leverage the event system for building sophisticated, event-driven applications.

### Core Principles

Event-driven applications built with naq follow these principles:

1. **Reactive Processing**: Your application reacts to events as they occur rather than polling for state changes.
2. **Decoupled Components**: Event producers and consumers are completely decoupled, communicating only through the event stream.
3. **Event Correlation**: All events for a job can be correlated using the job ID, enabling complete lifecycle tracking.
4. **Real-time Processing**: Events are processed as they arrive, enabling immediate responses to state changes.

### Setting Up an Event-Driven Application

```python
# event_driven_app.py
import asyncio
from naq.events import (
    AsyncJobEventProcessor,
    NATSJobEventStorage,
    JobEventType,
    configure_shared_logger
)

async def handle_job_events():
    """Main event processing function."""
    # Configure event logging
    configure_shared_logger(enabled=True)
    
    # Create storage and processor
    storage = NATSJobEventStorage()
    processor = AsyncJobEventProcessor(storage)
    
    # Register event handlers
    processor.add_handler(JobEventType.COMPLETED, handle_job_completed)
    processor.add_handler(JobEventType.FAILED, handle_job_failed)
    processor.add_handler(JobEventType.STATUS_CHANGED, handle_status_change)
    processor.add_global_handler(log_all_events)
    
    # Start processing events
    await processor.start()
    
    try:
        # Keep the application running
        while True:
            await asyncio.sleep(1)
    finally:
        await processor.stop()

async def handle_job_completed(event):
    """Handle job completion events."""
    print(f"Job {event.job_id} completed successfully")
    
    # Update application state
    await update_job_status(event.job_id, "completed")
    
    # Trigger post-processing tasks
    if event.details.get("requires_post_processing"):
        await enqueue_post_processing(event.job_id)

async def handle_job_failed(event):
    """Handle job failure events."""
    print(f"Job {event.job_id} failed: {event.error_message}")
    
    # Update application state
    await update_job_status(event.job_id, "failed")
    
    # Send alert if critical
    if event.queue_name == "critical":
        await send_failure_alert(event)
    
    # Log failure for analysis
    await log_failure_for_analysis(event)

async def handle_status_change(event):
    """Handle job status change events."""
    old_status = event.details.get("old_status")
    new_status = event.details.get("new_status")
    
    print(f"Job {event.job_id} status changed: {old_status} -> {new_status}")
    
    # Update dashboards
    await update_dashboard_metrics(event)
    
    # Check for specific transitions
    if old_status == "running" and new_status == "failed":
        await handle_runtime_failure(event)

async def log_all_events(event):
    """Log all events for auditing."""
    await persist_event_to_database(event)

if __name__ == "__main__":
    asyncio.run(handle_job_events())
```

### Best Practices for Event Logging and Monitoring

#### 1. Use Structured Event Details

Always include structured data in event details to enable better filtering and processing:

```python
# Good - structured details
event.details = {
    "user_id": "user-123",
    "priority": "high",
    "retry_count": 2,
    "custom_metadata": {
        "source": "api",
        "version": "1.0"
    }
}

# Avoid - unstructured details
event.details = "Job failed, will retry"
```

#### 2. Implement Proper Error Handling

Event handlers should be resilient to errors:

```python
async def robust_event_handler(event):
    """Robust event handler with proper error handling."""
    try:
        # Process the event
        await process_event(event)
    except Exception as e:
        # Log the error but don't crash
        print(f"Error processing event {event.job_id}: {e}")
        
        # Optionally send to dead-letter queue
        await send_to_dead_letter_queue(event, e)
```

#### 3. Use Selective Event Processing

Only process the events you need to avoid unnecessary overhead:

```python
# Good - specific event types
processor.add_handler(JobEventType.COMPLETED, handle_completion)
processor.add_handler(JobEventType.FAILED, handle_failure)

# Avoid - processing all events when not needed
processor.add_global_handler(handle_all_events)  # Only if truly needed
```

#### 4. Implement Event Batching for High Volume

For high-volume event processing, implement batching:

```python
class BatchEventProcessor:
    def __init__(self, batch_size=100, flush_interval=5.0):
        self.batch = []
        self.batch_size = batch_size
        self.flush_interval = flush_interval
        self.last_flush = time.time()
    
    async def process_event(self, event):
        """Process events in batches."""
        self.batch.append(event)
        
        # Flush if batch is full or interval elapsed
        if (len(self.batch) >= self.batch_size or
            time.time() - self.last_flush >= self.flush_interval):
            await self.flush_batch()
    
    async def flush_batch(self):
        """Flush the current batch of events."""
        if not self.batch:
            return
            
        # Process the batch
        await process_event_batch(self.batch)
        
        # Reset batch
        self.batch = []
        self.last_flush = time.time()
```

### Extending the Event System for Custom Use Cases

#### Custom Event Types

You can extend the event system with custom event types:

```python
from naq.models import JobEvent, JobEventType

# Add custom event type
class CustomJobEventType(str, Enum):
    CUSTOM_EVENT = "custom_event"
    BUSINESS_LOGIC_COMPLETED = "business_logic_completed"

# Create custom event
custom_event = JobEvent(
    job_id="job-123",
    event_type=CustomJobEventType.CUSTOM_EVENT,
    queue_name="custom_queue",
    details={
        "custom_field": "custom_value",
        "business_metrics": {"metric1": 100, "metric2": 200}
    }
)

# Log the custom event
logger = get_shared_sync_logger()
if logger:
    logger.log_event(custom_event)
```

#### Custom Event Storage Backends

You can implement custom storage backends by extending `BaseEventStorage`:

```python
from naq.events.storage import BaseEventStorage

class CustomEventStorage(BaseEventStorage):
    """Custom event storage backend."""
    
    async def store_event(self, event: JobEvent) -> None:
        """Store event in custom backend."""
        # Implement custom storage logic
        await self._store_in_custom_backend(event)
    
    async def get_events(self, job_id: str) -> List[JobEvent]:
        """Retrieve events for a job."""
        # Implement custom retrieval logic
        return await self._get_from_custom_backend(job_id)
    
    async def stream_events(self, job_id: str, **kwargs) -> AsyncIterator[JobEvent]:
        """Stream events in real-time."""
        # Implement custom streaming logic
        async for event in self._stream_from_custom_backend(job_id, **kwargs):
            yield event
```

#### Custom Event Processors

You can create specialized event processors for specific use cases:

```python
class AnalyticsEventProcessor:
    """Event processor for analytics and metrics."""
    
    def __init__(self, storage):
        self.storage = storage
        self.metrics = {}
    
    async def start(self):
        """Start processing events for analytics."""
        async for event in self.storage.stream_events(job_id="*"):
            await self.update_metrics(event)
    
    async def update_metrics(self, event):
        """Update analytics metrics based on events."""
        # Update queue metrics
        queue = event.queue_name or "unknown"
        if queue not in self.metrics:
            self.metrics[queue] = {
                "total": 0,
                "completed": 0,
                "failed": 0,
                "avg_duration": 0
            }
        
        self.metrics[queue]["total"] += 1
        
        if event.event_type == JobEventType.COMPLETED:
            self.metrics[queue]["completed"] += 1
            if event.duration_ms:
                # Update average duration
                current_avg = self.metrics[queue]["avg_duration"]
                completed = self.metrics[queue]["completed"]
                self.metrics[queue]["avg_duration"] = (
                    (current_avg * (completed - 1) + event.duration_ms) / completed
                )
        elif event.event_type == JobEventType.FAILED:
            self.metrics[queue]["failed"] += 1
```

### Performance Considerations for Event Logging

#### 1. Buffering and Batching

The shared event logger automatically handles buffering and batching, but you can optimize it for your use case:

```python
# Configure for high-throughput scenarios
configure_shared_logger(
    enabled=True,
    batch_size=500,  # Larger batch size
    flush_interval=1.0,  # More frequent flushes
    max_buffer_size=50000  # Larger buffer
)

# Configure for low-latency scenarios
configure_shared_logger(
    enabled=True,
    batch_size=10,  # Smaller batch size
    flush_interval=0.1,  # Very frequent flushes
    max_buffer_size=1000  # Smaller buffer
)
```

#### 2. Connection Management

Always use the shared event logger to avoid connection overhead:

```python
# Good - using shared logger
logger = get_shared_sync_logger()
if logger:
    logger.log_job_completed(job_id="job-123", worker_id="worker-1", duration_ms=1000)

# Avoid - creating new loggers
from naq.events import JobEventLogger
storage = NATSJobEventStorage()
logger = JobEventLogger(storage)  # Creates new connection
```

#### 3. Event Filtering

Filter events at the source to reduce processing overhead:

```python
# Good - filter by job_id
async for event in storage.stream_events(job_id="specific-job"):
    await process_event(event)

# Good - filter by event type
async for event in storage.stream_events(job_id="*", event_type=JobEventType.COMPLETED):
    await process_event(event)

# Avoid - processing all events and filtering manually
async for event in storage.stream_events(job_id="*"):
    if event.event_type == JobEventType.COMPLETED:
        await process_event(event)
```

#### 4. Memory Management

Be mindful of memory usage when processing large volumes of events:

```python
class MemoryEfficientProcessor:
    def __init__(self):
        self.recent_events = {}
        self.max_events_per_job = 100
    
    async def process_event(self, event):
        """Process events with memory constraints."""
        job_id = event.job_id
        
        # Initialize job event list if needed
        if job_id not in self.recent_events:
            self.recent_events[job_id] = []
        
        # Add event
        self.recent_events[job_id].append(event)
        
        # Prune old events if limit exceeded
        if len(self.recent_events[job_id]) > self.max_events_per_job:
            self.recent_events[job_id] = self.recent_events[job_id][-self.max_events_per_job:]
        
        # Process the event
        await self._process_single_event(event)
```

By following these best practices and patterns, you can build efficient, scalable event-driven applications with naq that leverage the full power of the event system.

## Event-Driven Architecture Patterns

The event-driven architecture of naq enables the implementation of sophisticated patterns for workflow orchestration, monitoring, and system integration. This section explores advanced patterns and techniques for building robust event-driven systems.

### Event-Driven Architecture Patterns

#### 1. Event Sourcing Pattern

Event sourcing is a pattern where state changes are captured as a sequence of events. naq's event system naturally supports this pattern:

```python
class JobEventSourcing:
    """Event sourcing implementation for job state management."""
    
    def __init__(self, storage):
        self.storage = storage
    
    async def get_job_state(self, job_id: str) -> dict:
        """Reconstruct job state from events."""
        events = await self.storage.get_events(job_id)
        
        state = {
            "job_id": job_id,
            "status": "unknown",
            "start_time": None,
            "end_time": None,
            "error": None,
            "worker_id": None,
            "queue_name": None
        }
        
        for event in events:
            self._apply_event_to_state(state, event)
        
        return state
    
    def _apply_event_to_state(self, state: dict, event: JobEvent):
        """Apply a single event to the state."""
        if event.event_type == JobEventType.ENQUEUED:
            state["status"] = "enqueued"
            state["queue_name"] = event.queue_name
        elif event.event_type == JobEventType.STARTED:
            state["status"] = "running"
            state["worker_id"] = event.worker_id
            state["start_time"] = event.timestamp
        elif event.event_type == JobEventType.COMPLETED:
            state["status"] = "completed"
            state["end_time"] = event.timestamp
        elif event.event_type == JobEventType.FAILED:
            state["status"] = "failed"
            state["error"] = event.error_message
            state["end_time"] = event.timestamp
        elif event.event_type == JobEventType.CANCELLED:
            state["status"] = "cancelled"
            state["end_time"] = event.timestamp
```

#### 2. CQRS (Command Query Responsibility Segregation)

CQRS separates read and write operations, using events to synchronize between them:

```python
class JobReadModel:
    """Read model for job queries."""
    
    def __init__(self):
        self.jobs = {}  # job_id -> job_data
    
    async def update_from_event(self, event: JobEvent):
        """Update read model from events."""
        if event.job_id not in self.jobs:
            self.jobs[event.job_id] = {
                "id": event.job_id,
                "status": "unknown",
                "created_at": event.timestamp,
                "updated_at": event.timestamp,
                "events": []
            }
        
        job = self.jobs[event.job_id]
        job["events"].append(event)
        job["updated_at"] = event.timestamp
        
        # Update status based on event type
        if event.event_type == JobEventType.ENQUEUED:
            job["status"] = "enqueued"
            job["queue_name"] = event.queue_name
        elif event.event_type == JobEventType.STARTED:
            job["status"] = "running"
            job["worker_id"] = event.worker_id
        elif event.event_type == JobEventType.COMPLETED:
            job["status"] = "completed"
            job["duration_ms"] = event.duration_ms
        elif event.event_type == JobEventType.FAILED:
            job["status"] = "failed"
            job["error"] = event.error_message
    
    def get_job(self, job_id: str) -> Optional[dict]:
        """Get job data from read model."""
        return self.jobs.get(job_id)
    
    def get_jobs_by_status(self, status: str) -> List[dict]:
        """Get jobs by status."""
        return [job for job in self.jobs.values() if job["status"] == status]
```

### Event-Driven Workflow Orchestration

Use events to orchestrate complex workflows with multiple steps:

```python
class WorkflowOrchestrator:
    """Orchestrates complex workflows using events."""
    
    def __init__(self, storage, queue):
        self.storage = storage
        self.queue = queue
        self.workflows = {}  # workflow_id -> workflow_state
    
    async def start_workflow(self, workflow_id: str, initial_job_data: dict):
        """Start a new workflow."""
        workflow_state = {
            "id": workflow_id,
            "status": "started",
            "current_step": 0,
            "steps": [
                {"name": "data_processing", "status": "pending"},
                {"name": "validation", "status": "pending"},
                {"name": "enrichment", "status": "pending"},
                {"name": "notification", "status": "pending"}
            ],
            "job_ids": {}
        }
        
        self.workflows[workflow_id] = workflow_state
        
        # Enqueue first step
        await self._enqueue_workflow_step(workflow_id, 0, initial_job_data)
    
    async def handle_job_event(self, event: JobEvent):
        """Handle job events to advance workflow."""
        # Find workflow that contains this job
        for workflow_id, workflow in self.workflows.items():
            if event.job_id in workflow["job_ids"].values():
                await self._advance_workflow(workflow_id, event)
                break
    
    async def _advance_workflow(self, workflow_id: str, event: JobEvent):
        """Advance workflow based on job completion."""
        workflow = self.workflows[workflow_id]
        
        if event.event_type == JobEventType.COMPLETED:
            # Find which step this job belongs to
            for i, step in enumerate(workflow["steps"]):
                if step.get("job_id") == event.job_id:
                    step["status"] = "completed"
                    
                    # Enqueue next step
                    if i + 1 < len(workflow["steps"]):
                        await self._enqueue_workflow_step(
                            workflow_id,
                            i + 1,
                            {"previous_result": event.details}
                        )
                    else:
                        # Workflow completed
                        workflow["status"] = "completed"
                    break
        
        elif event.event_type == JobEventType.FAILED:
            # Handle workflow failure
            workflow["status"] = "failed"
            workflow["error"] = event.error_message
    
    async def _enqueue_workflow_step(self, workflow_id: str, step_index: int, data: dict):
        """Enqueue a specific workflow step."""
        workflow = self.workflows[workflow_id]
        step = workflow["steps"][step_index]
        
        # Enqueue job for this step
        job = await self.queue.enqueue(
            workflow_step_function,
            step_name=step["name"],
            workflow_id=workflow_id,
            data=data
        )
        
        step["status"] = "running"
        step["job_id"] = job.job_id
        workflow["job_ids"][step["name"]] = job.job_id
```

### Event Correlation and Tracing

#### 1. Distributed Tracing

Implement distributed tracing to track requests across multiple services:

```python
class EventTracer:
    """Distributed tracing for job events."""
    
    def __init__(self):
        self.traces = {}  # trace_id -> trace_data
    
    def start_trace(self, trace_id: str, job_id: str, operation: str) -> str:
        """Start a new trace."""
        span_id = f"{trace_id}-{uuid.uuid4().hex[:8]}"
        
        if trace_id not in self.traces:
            self.traces[trace_id] = {
                "id": trace_id,
                "root_job_id": job_id,
                "spans": [],
                "start_time": time.time()
            }
        
        self.traces[trace_id]["spans"].append({
            "id": span_id,
            "job_id": job_id,
            "operation": operation,
            "start_time": time.time(),
            "status": "running"
        })
        
        return span_id
    
    def finish_span(self, trace_id: str, span_id: str, status: str = "completed"):
        """Finish a span."""
        if trace_id in self.traces:
            for span in self.traces[trace_id]["spans"]:
                if span["id"] == span_id:
                    span["end_time"] = time.time()
                    span["duration_ms"] = (span["end_time"] - span["start_time"]) * 1000
                    span["status"] = status
                    break
    
    def get_trace(self, trace_id: str) -> Optional[dict]:
        """Get trace data."""
        return self.traces.get(trace_id)
    
    async def trace_job_event(self, event: JobEvent):
        """Trace job events with correlation."""
        # Extract trace context from event details
        trace_id = event.details.get("trace_id")
        span_id = event.details.get("span_id")
        
        if trace_id and span_id:
            if event.event_type in [JobEventType.STARTED]:
                self.start_trace(trace_id, event.job_id, event.queue_name or "unknown")
            elif event.event_type in [JobEventType.COMPLETED, JobEventType.FAILED]:
                status = "completed" if event.event_type == JobEventType.COMPLETED else "failed"
                self.finish_span(trace_id, span_id, status)
```

#### 2. Event Correlation

Correlate related events across different jobs and workflows:

```python
class EventCorrelator:
    """Correlates related events across jobs and workflows."""
    
    def __init__(self):
        self.correlations = {}  # correlation_id -> related_events
        self.event_index = {}   # job_id -> correlation_ids
    
    def create_correlation(self, correlation_id: str, job_ids: List[str]):
        """Create a correlation between multiple jobs."""
        self.correlations[correlation_id] = {
            "id": correlation_id,
            "job_ids": job_ids,
            "events": [],
            "created_at": time.time()
        }
        
        # Index jobs by correlation
        for job_id in job_ids:
            if job_id not in self.event_index:
                self.event_index[job_id] = []
            self.event_index[job_id].append(correlation_id)
    
    async def add_event(self, event: JobEvent):
        """Add an event to relevant correlations."""
        if event.job_id in self.event_index:
            for correlation_id in self.event_index[event.job_id]:
                if correlation_id in self.correlations:
                    self.correlations[correlation_id]["events"].append(event)
    
    def get_correlation(self, correlation_id: str) -> Optional[dict]:
        """Get correlation data."""
        return self.correlations.get(correlation_id)
    
    def get_correlated_jobs(self, job_id: str) -> List[str]:
        """Get all jobs correlated with the given job."""
        correlated_jobs = []
        
        if job_id in self.event_index:
            for correlation_id in self.event_index[job_id]:
                correlation = self.correlations.get(correlation_id)
                if correlation:
                    correlated_jobs.extend(correlation["job_ids"])
        
        # Remove the original job from the list
        return [job for job in correlated_jobs if job != job_id]
```

### Building Monitoring and Alerting on Events

#### 1. Real-time Dashboard

Build a real-time dashboard that updates based on events:

```python
class DashboardMetrics:
    """Real-time dashboard metrics based on events."""
    
    def __init__(self):
        self.metrics = {
            "total_jobs": 0,
            "completed_jobs": 0,
            "failed_jobs": 0,
            "running_jobs": 0,
            "queue_sizes": {},
            "average_duration": {},
            "error_rates": {}
        }
        self.recent_events = []
    
    async def update_from_event(self, event: JobEvent):
        """Update metrics based on events."""
        self.recent_events.append(event)
        
        # Keep only recent events (last 1000)
        if len(self.recent_events) > 1000:
            self.recent_events = self.recent_events[-1000:]
        
        # Update queue metrics
        queue = event.queue_name or "unknown"
        if queue not in self.metrics["queue_sizes"]:
            self.metrics["queue_sizes"][queue] = 0
            self.metrics["average_duration"][queue] = 0
            self.metrics["error_rates"][queue] = 0
        
        # Update based on event type
        if event.event_type == JobEventType.ENQUEUED:
            self.metrics["total_jobs"] += 1
            self.metrics["queue_sizes"][queue] += 1
        elif event.event_type == JobEventType.STARTED:
            self.metrics["running_jobs"] += 1
        elif event.event_type == JobEventType.COMPLETED:
            self.metrics["completed_jobs"] += 1
            self.metrics["running_jobs"] -= 1
            self.metrics["queue_sizes"][queue] -= 1
            
            # Update average duration
            if event.duration_ms:
                current_avg = self.metrics["average_duration"][queue]
                completed_count = self.metrics["completed_jobs"]
                self.metrics["average_duration"][queue] = (
                    (current_avg * (completed_count - 1) + event.duration_ms) / completed_count
                )
        elif event.event_type == JobEventType.FAILED:
            self.metrics["failed_jobs"] += 1
            self.metrics["running_jobs"] -= 1
            self.metrics["queue_sizes"][queue] -= 1
            
            # Update error rate
            total_jobs = self.metrics["total_jobs"]
            if total_jobs > 0:
                self.metrics["error_rates"][queue] = (
                    self.metrics["failed_jobs"] / total_jobs
                ) * 100
    
    def get_dashboard_data(self) -> dict:
        """Get current dashboard data."""
        return {
            "metrics": self.metrics,
            "recent_events": self.recent_events[-50:],  # Last 50 events
            "timestamp": time.time()
        }
```

#### 2. Alerting System

Implement an alerting system based on event patterns:

```python
class AlertingSystem:
    """Alerting system based on event patterns."""
    
    def __init__(self):
        self.alerts = []
        self.rules = []
        self.event_history = []
    
    def add_alert_rule(self, rule: dict):
        """Add an alert rule."""
        self.rules.append(rule)
    
    async def process_event(self, event: JobEvent):
        """Process event against alert rules."""
        self.event_history.append(event)
        
        # Keep history manageable
        if len(self.event_history) > 10000:
            self.event_history = self.event_history[-5000:]
        
        # Check each rule
        for rule in self.rules:
            if await self._evaluate_rule(rule, event):
                await self._trigger_alert(rule, event)
    
    async def _evaluate_rule(self, rule: dict, event: JobEvent) -> bool:
        """Evaluate if an alert rule is triggered."""
        rule_type = rule.get("type")
        
        if rule_type == "threshold":
            return await self._evaluate_threshold_rule(rule, event)
        elif rule_type == "pattern":
            return await self._evaluate_pattern_rule(rule, event)
        elif rule_type == "rate":
            return await self._evaluate_rate_rule(rule, event)
        
        return False
    
    async def _evaluate_threshold_rule(self, rule: dict, event: JobEvent) -> bool:
        """Evaluate threshold-based alert rule."""
        metric = rule.get("metric")
        threshold = rule.get("threshold")
        operator = rule.get("operator", ">")
        
        if metric == "failure_rate":
            # Calculate failure rate for the queue
            queue_events = [e for e in self.event_history
                           if e.queue_name == event.queue_name]
            if len(queue_events) > 0:
                failures = len([e for e in queue_events
                              if e.event_type == JobEventType.FAILED])
                rate = (failures / len(queue_events)) * 100
                
                if operator == ">" and rate > threshold:
                    return True
                elif operator == "<" and rate < threshold:
                    return True
        
        return False
    
    async def _evaluate_pattern_rule(self, rule: dict, event: JobEvent) -> bool:
        """Evaluate pattern-based alert rule."""
        pattern = rule.get("pattern")
        time_window = rule.get("time_window", 300)  # 5 minutes
        
        # Look for patterns in recent events
        recent_events = [e for e in self.event_history
                        if time.time() - e.timestamp <= time_window]
        
        if pattern == "consecutive_failures":
            # Check for consecutive failures
            consecutive_failures = 0
            for e in reversed(recent_events):
                if e.event_type == JobEventType.FAILED:
                    consecutive_failures += 1
                    if consecutive_failures >= rule.get("count", 3):
                        return True
                else:
                    break
        
        return False
    
    async def _evaluate_rate_rule(self, rule: dict, event: JobEvent) -> bool:
        """Evaluate rate-based alert rule."""
        metric = rule.get("metric")
        threshold = rule.get("threshold")
        time_window = rule.get("time_window", 60)  # 1 minute
        
        # Count events in time window
        recent_events = [e for e in self.event_history
                        if time.time() - e.timestamp <= time_window]
        
        if metric == "events_per_minute":
            rate = len(recent_events) / (time_window / 60)
            return rate > threshold
        
        return False
    
    async def _trigger_alert(self, rule: dict, event: JobEvent):
        """Trigger an alert."""
        alert = {
            "id": f"alert-{uuid.uuid4().hex[:8]}",
            "rule_id": rule.get("id"),
            "rule_name": rule.get("name"),
            "severity": rule.get("severity", "medium"),
            "message": rule.get("message", "Alert triggered"),
            "triggered_by": event.job_id,
            "timestamp": time.time(),
            "status": "active"
        }
        
        self.alerts.append(alert)
        
        # Send notification
        await self._send_notification(alert)
    
    async def _send_notification(self, alert: dict):
        """Send alert notification."""
        # Implement notification logic (email, Slack, PagerDuty, etc.)
        print(f"ALERT: {alert['message']} (Severity: {alert['severity']})")
        
        # In a real implementation, you would integrate with notification services
        # await send_slack_notification(alert)
        # await send_email_notification(alert)
        # await send_pagerduty_notification(alert)
```

#### 3. Event-Driven Auto-scaling

Implement auto-scaling based on event metrics:

```python
class AutoScaler:
    """Auto-scaling based on event metrics."""
    
    def __init__(self, min_workers=1, max_workers=10):
        self.min_workers = min_workers
        self.max_workers = max_workers
        self.current_workers = min_workers
        self.metrics_history = []
    
    async def process_event(self, event: JobEvent):
        """Process events for auto-scaling decisions."""
        # Update metrics
        self._update_metrics(event)
        
        # Check scaling conditions every 10 events
        if len(self.metrics_history) % 10 == 0:
            await self._evaluate_scaling()
    
    def _update_metrics(self, event: JobEvent):
        """Update scaling metrics."""
        self.metrics_history.append({
            "timestamp": time.time(),
            "event_type": event.event_type,
            "queue_name": event.queue_name,
            "duration_ms": event.duration_ms
        })
        
        # Keep only recent metrics (last hour)
        one_hour_ago = time.time() - 3600
        self.metrics_history = [
            m for m in self.metrics_history
            if m["timestamp"] > one_hour_ago
        ]
    
    async def _evaluate_scaling(self):
        """Evaluate if scaling is needed."""
        if len(self.metrics_history) < 10:
            return
        
        # Calculate current metrics
        recent_metrics = [m for m in self.metrics_history
                         if time.time() - m["timestamp"] <= 300]  # Last 5 minutes
        
        # Calculate queue depth
        enqueued = len([m for m in recent_metrics if m["event_type"] == JobEventType.ENQUEUED])
        processed = len([m for m in recent_metrics
                        if m["event_type"] in [JobEventType.COMPLETED, JobEventType.FAILED]])
        queue_depth = enqueued - processed
        
        # Calculate average processing time
        completed_events = [m for m in recent_metrics
                           if m["event_type"] == JobEventType.COMPLETED and m["duration_ms"]]
        avg_duration = sum(m["duration_ms"] for m in completed_events) / len(completed_events) if completed_events else 0
        
        # Scaling decisions
        if queue_depth > 50 and avg_duration > 5000:  # Scale up
            await self._scale_up()
        elif queue_depth < 10 and avg_duration < 1000:  # Scale down
            await self._scale_down()
    
    async def _scale_up(self):
        """Scale up worker count."""
        if self.current_workers < self.max_workers:
            self.current_workers += 1
            await self._start_worker()
            print(f"Scaled up to {self.current_workers} workers")
    
    async def _scale_down(self):
        """Scale down worker count."""
        if self.current_workers > self.min_workers:
            self.current_workers -= 1
            await self._stop_worker()
            print(f"Scaled down to {self.current_workers} workers")
    
    async def _start_worker(self):
        """Start a new worker."""
        # Implement worker startup logic
        pass
    
    async def _stop_worker(self):
        """Stop a worker."""
        # Implement worker shutdown logic
        pass
```

These patterns and techniques demonstrate the power and flexibility of naq's event-driven architecture. By combining these patterns, you can build sophisticated, resilient, and scalable systems that can handle complex business requirements while maintaining excellent performance and reliability.

## Advanced Service Layer Usage

The service layer architecture in `naq` provides a powerful foundation for building advanced applications with efficient resource management and clean separation of concerns. This section explores advanced patterns and techniques for leveraging the service layer in production environments.

### Service Inter-Communication Patterns

Services in `naq` are designed to work together through well-defined communication patterns. Understanding these patterns is key to building robust, maintainable applications.

#### 1. Service Dependency Injection

Services can depend on other services, with dependencies automatically managed by the ServiceManager:

```python
from naq.services import ServiceManager, ConnectionService, EventService

class JobService:
    def __init__(self, config, connection_service: ConnectionService, event_service: EventService):
        self.config = config
        self.connection_service = connection_service
        self.event_service = event_service
    
    async def enqueue_job(self, job):
        # Use connection_service and event_service
        conn = await self.connection_service.get_connection()
        await self.event_service.log_job_enqueued(job)
        # ... implementation
```

#### 2. Service Event Broadcasting

Services can broadcast events that other services can subscribe to:

```python
class MonitoringService:
    def __init__(self, event_service: EventService):
        self.event_service = event_service
        self.metrics = {}
    
    async def start_monitoring(self):
        # Subscribe to job events from other services
        await self.event_service.subscribe_to_events(
            event_types=['JOB_ENQUEUED', 'JOB_COMPLETED', 'JOB_FAILED'],
            callback=self._handle_job_event
        )
    
    async def _handle_job_event(self, event):
        # Update metrics based on job events
        self._update_metrics(event)
```

#### 3. Service Health Monitoring

Services can monitor the health of dependent services:

```python
class HealthMonitorService:
    def __init__(self, service_manager: ServiceManager):
        self.service_manager = service_manager
        self.health_status = {}
    
    async def check_all_services(self):
        # Check health of all registered services
        for service_type in [ConnectionService, EventService, JobService]:
            try:
                service = await self.service_manager.get_service(service_type)
                health = await service.check_health()
                self.health_status[service_type.__name__] = health
            except Exception as e:
                self.health_status[service_type.__name__] = {'status': 'error', 'error': str(e)}
        
        return self.health_status
```

### Service Decorators for Enhanced Functionality

Decorators can enhance service functionality with common patterns like retry logic and metrics collection.

#### Retry Decorator

```python
from functools import wraps
from typing import Callable, Any
import asyncio

def service_retry(max_retries: int = 3, delay: float = 1.0, backoff_factor: float = 2.0):
    """Decorator to add retry logic to service methods."""
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        async def wrapper(self, *args, **kwargs):
            last_error = None
            
            for attempt in range(max_retries):
                try:
                    return await func(self, *args, **kwargs)
                except Exception as e:
                    last_error = e
                    if attempt < max_retries - 1:
                        wait_time = delay * (backoff_factor ** attempt)
                        await asyncio.sleep(wait_time)
                    else:
                        break
            
            # All retries failed
            raise ServiceError(f"Service operation failed after {max_retries} retries: {last_error}") from last_error
        
        return wrapper
    return decorator
```

#### Metrics Collection Decorator

```python
import time
from functools import wraps
from typing import Callable, Any

def service_metrics(metric_name: str):
    """Decorator to collect metrics for service operations."""
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        async def wrapper(self, *args, **kwargs):
            start_time = time.time()
            
            try:
                result = await func(self, *args, **kwargs)
                duration_ms = (time.time() - start_time) * 1000
                
                # Record success metric
                if hasattr(self, '_metrics_collector'):
                    await self._metrics_collector.record_success(
                        metric_name, duration_ms
                    )
                
                return result
            except Exception as e:
                duration_ms = (time.time() - start_time) * 1000
                
                # Record failure metric
                if hasattr(self, '_metrics_collector'):
                    await self._metrics_collector.record_failure(
                        metric_name, duration_ms, str(e)
                    )
                
                raise
        
        return wrapper
    return decorator
```

#### Usage Example

```python
class EnhancedJobService(JobService):
    """Job service with enhanced functionality."""
    
    @service_retry(max_retries=5, delay=2.0)
    @service_metrics('enqueue_job')
    async def enqueue_job(self, job):
        """Enqueue a job with retry and metrics."""
        # Implementation
        await self.connection_service.get_connection()
        await self.event_service.log_job_enqueued(job)
        # ... rest of implementation
```

### Service Testing Strategies

Testing services effectively requires specialized approaches to isolate dependencies and verify behavior.

#### Mocking Services

```python
import pytest
from unittest.mock import AsyncMock, MagicMock

class MockConnectionService:
    """Mock connection service for testing."""
    
    def __init__(self, config):
        self.config = config
        self._connections = {}
        self._should_fail = False
    
    async def _do_initialize(self):
        pass
    
    def set_failure_mode(self, should_fail: bool):
        self._should_fail = should_fail
    
    async def get_connection(self, url=None):
        if self._should_fail:
            raise NaqConnectionError("Mock connection failure")
        
        target_url = url or self.config.get('nats', {}).get('url', 'nats://localhost:4222')
        if target_url not in self._connections:
            self._connections[target_url] = {'mock': True, 'url': target_url}
        
        return self._connections[target_url]

@pytest.fixture
async def mock_services():
    """Fixture providing mock services."""
    mock_connection = MockConnectionService({'nats': {'url': 'nats://test:4222'}})
    await mock_connection.initialize()
    
    return {
        'connection': mock_connection,
        # Add other mock services as needed
    }

async def test_job_service_with_mocks(mock_services):
    """Test JobService with mocked dependencies."""
    config = {'nats': {'url': 'nats://test:4222'}}
    
    # Create service with mocked dependencies
    job_service = JobService(config, mock_services['connection'])
    await job_service.initialize()
    
    # Test success case
    result = await job_service.enqueue_job(test_job)
    assert result is not None
    
    # Test failure case
    mock_services['connection'].set_failure_mode(True)
    try:
        await job_service.enqueue_job(test_job)
        assert False, "Should have raised an exception"
    except NaqConnectionError:
        pass  # Expected
    
    await job_service.cleanup()
```

#### Integration Testing

```python
import pytest

class ServiceIntegrationTest:
    """Integration tests for service layer."""
    
    @pytest.mark.asyncio
    async def test_service_lifecycle(self):
        """Test service initialization and cleanup."""
        config = {'nats': {'url': 'nats://localhost:4222'}}
        
        async with ServiceManager(config) as services:
            # Get services
            connection_service = await services.get_service(ConnectionService)
            event_service = await services.get_service(EventService)
            
            # Verify services are initialized
            assert connection_service._initialized is True
            assert event_service._initialized is True
            
            # Use services
            conn = await connection_service.get_connection()
            assert conn is not None
            
            # Test service interaction
            await event_service.log_event(test_event)
        
        # Verify services are cleaned up
        assert connection_service._initialized is False
        assert event_service._initialized is False
    
    @pytest.mark.asyncio
    async def test_service_dependency_injection(self):
        """Test that service dependencies are properly injected."""
        config = {'nats': {'url': 'nats://localhost:4222'}}
        
        async with ServiceManager(config) as services:
            # Get EventService which depends on ConnectionService
            event_service = await services.get_service(EventService)
            
            # Verify dependency is injected
            assert event_service._connection_service is not None
            assert isinstance(event_service._connection_service, ConnectionService)
            
            # Verify both services share the same connection
            conn_service = await services.get_service(ConnectionService)
            assert event_service._connection_service is conn_service
```

### Performance Optimization with Services

Services can be optimized for performance through careful configuration and implementation patterns.

#### Connection Pool Tuning

```python
class TunedConnectionService(ConnectionService):
    """Connection service with performance tuning."""
    
    def __init__(self, config):
        super().__init__(config)
        self._connection_pool_size = config.get('pool_size', 10)
        self._connection_timeout = config.get('connection_timeout', 30.0)
        self._connection_idle_timeout = config.get('idle_timeout', 300.0)
        self._last_used = {}
    
    async def _do_initialize(self):
        await super()._do_initialize()
        
        # Start connection cleanup task
        asyncio.create_task(self._cleanup_idle_connections())
    
    async def _cleanup_idle_connections(self):
        """Clean up idle connections."""
        while True:
            try:
                await asyncio.sleep(60)  # Check every minute
                
                current_time = time.time()
                idle_connections = []
                
                async with self._lock:
                    for url, last_used in self._last_used.items():
                        if current_time - last_used > self._connection_idle_timeout:
                            idle_connections.append(url)
                    
                    # Close idle connections
                    for url in idle_connections:
                        if url in self._connections:
                            try:
                                await self._connections[url].close()
                                del self._connections[url]
                                del self._last_used[url]
                                logger.info(f"Closed idle connection to {url}")
                            except Exception as e:
                                logger.error(f"Error closing idle connection to {url}: {e}")
            
            except Exception as e:
                logger.error(f"Error in connection cleanup task: {e}")
    
    async def get_connection(self, url=None):
        """Get connection with performance tracking."""
        target_url = url if url is not None else self._default_url
        
        # Update last used time
        self._last_used[target_url] = time.time()
        
        return await super().get_connection(target_url)
```

#### Batch Processing with Services

```python
class BatchJobService(JobService):
    """Job service optimized for batch processing."""
    
    def __init__(self, config):
        super().__init__(config)
        self._batch_size = config.get('batch_size', 100)
        self._batch_timeout = config.get('batch_timeout', 5.0)
        self._pending_jobs = []
        self._batch_event = asyncio.Event()
    
    async def _do_initialize(self):
        await super()._do_initialize()
        
        # Start batch processing task
        asyncio.create_task(self._process_batches())
    
    async def _process_batches(self):
        """Process jobs in batches."""
        while True:
            try:
                # Wait for batch to fill or timeout
                await asyncio.wait_for(
                    self._batch_event.wait(),
                    timeout=self._batch_timeout
                )
                
                # Process batch
                async with self._lock:
                    if not self._pending_jobs:
                        self._batch_event.clear()
                        continue
                    
                    batch = self._pending_jobs[:self._batch_size]
                    self._pending_jobs = self._pending_jobs[self._batch_size:]
                    self._batch_event.clear()
                
                # Execute batch
                await self._execute_batch(batch)
                
            except asyncio.TimeoutError:
                # Timeout is normal, process any pending jobs
                async with self._lock:
                    if self._pending_jobs:
                        batch = self._pending_jobs[:]
                        self._pending_jobs = []
                        await self._execute_batch(batch)
            except Exception as e:
                logger.error(f"Error in batch processing: {e}")
    
    async def _execute_batch(self, batch):
        """Execute a batch of jobs."""
        logger.info(f"Executing batch of {len(batch)} jobs")
        
        # Execute jobs concurrently
        tasks = [self.execute_job(job) for job in batch]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Process results
        for job, result in zip(batch, results):
            if isinstance(result, Exception):
                logger.error(f"Job {job.job_id} failed: {result}")
            else:
                logger.debug(f"Job {job.job_id} completed successfully")
    
    async def execute_job(self, job):
        """Execute a single job (overridden for batch processing)."""
        # For batch processing, add to pending queue
        async with self._lock:
            self._pending_jobs.append(job)
            self._batch_event.set()
        
        # Return a placeholder result
        return JobResult(
            job_id=job.job_id,
            status="pending",
            result=None,
            error=None,
            start_time=time.time(),
            end_time=time.time()
        )
```

### Custom Service Implementation

You can extend the service layer by implementing custom services that integrate with the existing service ecosystem.

#### Creating a Custom Service

```python
from typing import Dict, Any, Optional
from naq.services.base import BaseService
from naq.services import ServiceManager

class AnalyticsService(BaseService):
    """Custom service for collecting and analyzing job metrics."""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self._metrics = {}
        self._event_service = None
        self._job_service = None
    
    async def _do_initialize(self) -> None:
        """Initialize the analytics service."""
        # Initialize metrics storage
        self._metrics = {
            'queue_stats': {},
            'job_timings': {},
            'error_rates': {},
            'throughput': {}
        }
        
        # Get required services from ServiceManager
        # Note: In a real implementation, these would be injected
        logger.info("AnalyticsService initialized")
    
    async def cleanup(self) -> None:
        """Cleanup analytics service resources."""
        await super().cleanup()
        # Persist metrics if needed
        logger.info("AnalyticsService cleaned up")
    
    async def record_job_timing(self, queue_name: str, duration_ms: float) -> None:
        """Record job execution timing."""
        if queue_name not in self._metrics['job_timings']:
            self._metrics['job_timings'][queue_name] = []
        
        self._metrics['job_timings'][queue_name].append(duration_ms)
        
        # Keep only recent timings (last 1000)
        if len(self._metrics['job_timings'][queue_name]) > 1000:
            self._metrics['job_timings'][queue_name] = self._metrics['job_timings'][queue_name][-1000:]
    
    async def get_queue_stats(self, queue_name: str) -> Dict[str, Any]:
        """Get statistics for a specific queue."""
        timings = self._metrics['job_timings'].get(queue_name, [])
        
        if not timings:
            return {
                'queue_name': queue_name,
                'job_count': 0,
                'avg_duration_ms': 0,
                'min_duration_ms': 0,
                'max_duration_ms': 0
            }
        
        return {
            'queue_name': queue_name,
            'job_count': len(timings),
            'avg_duration_ms': sum(timings) / len(timings),
            'min_duration_ms': min(timings),
            'max_duration_ms': max(timings)
        }
    
    async def get_system_stats(self) -> Dict[str, Any]:
        """Get system-wide statistics."""
        all_stats = {
            'total_queues': len(self._metrics['job_timings']),
            'total_jobs': sum(len(timings) for timings in self._metrics['job_timings'].values()),
            'queue_stats': {}
        }
        
        # Collect stats for each queue
        for queue_name in self._metrics['job_timings']:
            all_stats['queue_stats'][queue_name] = await self.get_queue_stats(queue_name)
        
        return all_stats
```

#### Registering Custom Services

```python
from naq.services import ServiceManager

async def custom_service_example():
    """Example of using a custom service with ServiceManager."""
    
    # Create configuration
    config = {
        'nats': {
            'url': 'nats://localhost:4222',
            'max_reconnect_attempts': 5,
            'reconnect_delay': 1.0
        },
        'analytics': {
            'retention_period_hours': 24,
            'metrics_batch_size': 100
        }
    }
    
    async with ServiceManager(config) as services:
        # Manually create and register custom service
        analytics_service = AnalyticsService(config.get('analytics', {}))
        await analytics_service.initialize()
        
        # Use the service
        await analytics_service.record_job_timing('high_priority', 150.5)
        await analytics_service.record_job_timing('high_priority', 120.3)
        
        stats = await analytics_service.get_queue_stats('high_priority')
        print(f"Queue stats: {stats}")
        
        # Cleanup
        await analytics_service.cleanup()
```

### Service Configuration Patterns

#### Environment-Based Configuration

```python
import os
from typing import Dict, Any

def get_service_config() -> Dict[str, Any]:
    """Build service configuration from environment variables."""
    config = {
        'nats': {
            'url': os.getenv('NAQ_NATS_URL', 'nats://localhost:4222'),
            'max_reconnect_attempts': int(os.getenv('NAQ_MAX_RECONNECT', '5')),
            'reconnect_delay': float(os.getenv('NAQ_RECONNECT_DELAY', '1.0')),
            'connection_timeout': float(os.getenv('NAQ_CONNECTION_TIMEOUT', '10.0')),
            'ping_interval': int(os.getenv('NAQ_PING_INTERVAL', '60')),
            'max_outstanding_pings': int(os.getenv('NAQ_MAX_OUTSTANDING_PINGS', '2'))
        },
        'events': {
            'enabled': os.getenv('NAQ_EVENTS_ENABLED', 'false').lower() == 'true',
            'batch_size': int(os.getenv('NAQ_EVENT_BATCH_SIZE', '100')),
            'flush_interval': float(os.getenv('NAQ_EVENT_FLUSH_INTERVAL', '1.0')),
            'max_buffer_size': int(os.getenv('NAQ_EVENT_MAX_BUFFER', '10000'))
        },
        'jobs': {
            'default_timeout': int(os.getenv('NAQ_DEFAULT_TIMEOUT', '3600')),
            'result_ttl': int(os.getenv('NAQ_RESULT_TTL', '604800'))
        }
    }
    
    return config

async def environment_configured_example():
    """Example using environment-based configuration."""
    config = get_service_config()
    
    async with ServiceManager(config) as services:
        queue = Queue(name='production', services=services)
        # Queue will use environment-configured services
        job = await queue.enqueue(process_data, data_item)
```

#### Multi-Environment Configuration

```python
def get_environment_config(env: str) -> Dict[str, Any]:
    """Get configuration for different environments."""
    configs = {
        'development': {
            'nats': {
                'url': 'nats://localhost:4222',
                'max_reconnect_attempts': 3,
                'reconnect_delay': 1.0
            },
            'events': {
                'enabled': True,
                'batch_size': 10,
                'flush_interval': 0.5
            }
        },
        'staging': {
            'nats': {
                'url': 'nats://staging.example.com:4222',
                'max_reconnect_attempts': 5,
                'reconnect_delay': 2.0
            },
            'events': {
                'enabled': True,
                'batch_size': 50,
                'flush_interval': 1.0
            }
        },
        'production': {
            'nats': {
                'url': 'nats://prod.example.com:4222',
                'max_reconnect_attempts': 10,
                'reconnect_delay': 5.0
            },
            'events': {
                'enabled': True,
                'batch_size': 100,
                'flush_interval': 2.0
            }
        }
    }
    
    return configs.get(env, configs['development'])

async def multi_environment_example():
    """Example of multi-environment configuration."""
    env = os.getenv('NAQ_ENVIRONMENT', 'development')
    config = get_environment_config(env)
    
    async with ServiceManager(config) as services:
        # Services configured for the specific environment
        queue = Queue(name='app_queue', services=services)
        # ...
```

### Advanced Service Patterns

#### Service Inter-Communication

```python
class MonitoringService(BaseService):
    """Service that monitors other services."""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self._connection_service = None
        self._event_service = None
        self._health_checks = {}
    
    async def _do_initialize(self) -> None:
        """Initialize monitoring service."""
        # Set up health checks for other services
        self._health_checks = {
            'connection': self._check_connection_health,
            'events': self._check_event_health,
            'jobs': self._check_job_health
        }
        
        logger.info("MonitoringService initialized")
    
    async def _check_connection_health(self) -> Dict[str, Any]:
        """Check connection service health."""
        if not self._connection_service:
            return {'status': 'error', 'message': 'Connection service not available'}
        
        try:
            status = self._connection_service.get_connection_status()
            healthy = all(conn.get('connected', False) for conn in status.values())
            return {
                'status': 'healthy' if healthy else 'unhealthy',
                'connections': status
            }
        except Exception as e:
            return {'status': 'error', 'message': str(e)}
    
    async def _check_event_health(self) -> Dict[str, Any]:
        """Check event service health."""
        if not self._event_service:
            return {'status': 'error', 'message': 'Event service not available'}
        
        try:
            stats = self._event_service.get_logger_stats()
            buffer_size = self._event_service.get_buffer_size()
            return {
                'status': 'healthy',
                'stats': stats,
                'buffer_size': buffer_size
            }
        except Exception as e:
            return {'status': 'error', 'message': str(e)}
    
    async def _check_job_health(self) -> Dict[str, Any]:
        """Check job service health."""
        # Implementation depends on job service metrics
        return {'status': 'healthy', 'message': 'Job service operational'}
    
    async def get_system_health(self) -> Dict[str, Any]:
        """Get overall system health."""
        health_report = {
            'timestamp': time.time(),
            'services': {}
        }
        
        for service_name, check_func in self._health_checks.items():
            try:
                health_report['services'][service_name] = await check_func()
            except Exception as e:
                health_report['services'][service_name] = {
                    'status': 'error',
                    'message': str(e)
                }
        
        # Determine overall system status
        all_healthy = all(
            service.get('status') == 'healthy'
            for service in health_report['services'].values()
        )
        
        health_report['overall_status'] = 'healthy' if all_healthy else 'unhealthy'
        
        return health_report
```

#### Service Decorators for Enhanced Functionality

```python
from functools import wraps
from typing import Callable, Any

def with_service_retry(max_retries: int = 3, delay: float = 1.0):
    """Decorator to add retry logic to service methods."""
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        async def wrapper(self, *args, **kwargs):
            last_error = None
            
            for attempt in range(max_retries):
                try:
                    return await func(self, *args, **kwargs)
                except Exception as e:
                    last_error = e
                    if attempt < max_retries - 1:
                        await asyncio.sleep(delay * (2 ** attempt))  # Exponential backoff
                    else:
                        break
            
            # All retries failed
            raise ServiceError(f"Service operation failed after {max_retries} retries: {last_error}") from last_error
        
        return wrapper
    return decorator

def with_service_metrics(metric_name: str):
    """Decorator to collect metrics for service operations."""
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        async def wrapper(self, *args, **kwargs):
            start_time = time.time()
            
            try:
                result = await func(self, *args, **kwargs)
                duration_ms = (time.time() - start_time) * 1000
                
                # Record success metric
                if hasattr(self, '_metrics_collector'):
                    await self._metrics_collector.record_success(
                        metric_name, duration_ms
                    )
                
                return result
            except Exception as e:
                duration_ms = (time.time() - start_time) * 1000
                
                # Record failure metric
                if hasattr(self, '_metrics_collector'):
                    await self._metrics_collector.record_failure(
                        metric_name, duration_ms, str(e)
                    )
                
                raise
        
        return wrapper
    return decorator

# Usage example
class EnhancedConnectionService(ConnectionService):
    """Connection service with enhanced functionality."""
    
    @with_service_retry(max_retries=5, delay=2.0)
    @with_service_metrics('get_connection')
    async def get_connection(self, url: Optional[str] = None) -> NATSClient:
        """Get connection with retry and metrics."""
        return await super().get_connection(url)
```

### Service Testing Strategies

#### Mock Services for Testing

```python
class MockConnectionService(BaseService):
    """Mock connection service for testing."""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self._connections = {}
        self._should_fail = False
    
    async def _do_initialize(self) -> None:
        """Initialize mock service."""
        pass
    
    def set_failure_mode(self, should_fail: bool):
        """Set whether the service should fail."""
        self._should_fail = should_fail
    
    async def get_connection(self, url: Optional[str] = None) -> Any:
        """Mock connection method."""
        if self._should_fail:
            raise NaqConnectionError("Mock connection failure")
        
        target_url = url or self.config.get('nats', {}).get('url', 'nats://localhost:4222')
        if target_url not in self._connections:
            self._connections[target_url] = {'mock': True, 'url': target_url}
        
        return self._connections[target_url]

async def test_with_mock_services():
    """Example of testing with mock services."""
    config = {'nats': {'url': 'nats://test:4222'}}
    
    # Create mock service
    mock_connection = MockConnectionService(config)
    await mock_connection.initialize()
    
    # Test success case
    conn = await mock_connection.get_connection()
    assert conn['mock'] is True
    
    # Test failure case
    mock_connection.set_failure_mode(True)
    try:
        await mock_connection.get_connection()
        assert False, "Should have raised an exception"
    except NaqConnectionError:
        pass  # Expected
    
    await mock_connection.cleanup()
```

#### Service Integration Testing

```python
import pytest

class ServiceIntegrationTest:
    """Integration tests for service layer."""
    
    @pytest.mark.asyncio
    async def test_service_lifecycle(self):
        """Test service initialization and cleanup."""
        config = {'nats': {'url': 'nats://localhost:4222'}}
        
        async with ServiceManager(config) as services:
            # Get services
            connection_service = await services.get_service(ConnectionService)
            stream_service = await services.get_service(StreamService)
            
            # Verify services are initialized
            assert connection_service._initialized is True
            assert stream_service._initialized is True
            
            # Use services
            js = await connection_service.get_jetstream()
            assert js is not None
        
        # Verify services are cleaned up
        assert connection_service._initialized is False
        assert stream_service._initialized is False
    
    @pytest.mark.asyncio
    async def test_service_dependency_injection(self):
        """Test that service dependencies are properly injected."""
        config = {'nats': {'url': 'nats://localhost:4222'}}
        
        async with ServiceManager(config) as services:
            # Get StreamService which depends on ConnectionService
            stream_service = await services.get_service(StreamService)
            
            # Verify dependency is injected
            assert stream_service._connection_service is not None
            assert isinstance(stream_service._connection_service, ConnectionService)
            
            # Verify both services share the same connection
            conn_service = await services.get_service(ConnectionService)
            assert stream_service._connection_service is conn_service
```

### Performance Optimization with Services

#### Connection Pool Tuning

```python
class TunedConnectionService(ConnectionService):
    """Connection service with performance tuning."""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self._connection_pool_size = config.get('pool_size', 10)
        self._connection_timeout = config.get('connection_timeout', 30.0)
        self._connection_idle_timeout = config.get('idle_timeout', 300.0)
        self._last_used = {}
    
    async def _do_initialize(self) -> None:
        """Initialize with performance tuning."""
        await super()._do_initialize()
        
        # Start connection cleanup task
        asyncio.create_task(self._cleanup_idle_connections())
    
    async def _cleanup_idle_connections(self):
        """Clean up idle connections."""
        while True:
            try:
                await asyncio.sleep(60)  # Check every minute
                
                current_time = time.time()
                idle_connections = []
                
                async with self._lock:
                    for url, last_used in self._last_used.items():
                        if current_time - last_used > self._connection_idle_timeout:
                            idle_connections.append(url)
                    
                    # Close idle connections
                    for url in idle_connections:
                        if url in self._connections:
                            try:
                                await self._connections[url].close()
                                del self._connections[url]
                                del self._last_used[url]
                                logger.info(f"Closed idle connection to {url}")
                            except Exception as e:
                                logger.error(f"Error closing idle connection to {url}: {e}")
            
            except Exception as e:
                logger.error(f"Error in connection cleanup task: {e}")
    
    async def get_connection(self, url: Optional[str] = None) -> NATSClient:
        """Get connection with performance tracking."""
        target_url = url if url is not None else self._default_url
        
        # Update last used time
        self._last_used[target_url] = time.time()
        
        return await super().get_connection(target_url)
```

#### Batch Processing with Services

```python
class BatchJobService(JobService):
    """Job service optimized for batch processing."""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self._batch_size = config.get('batch_size', 100)
        self._batch_timeout = config.get('batch_timeout', 5.0)
        self._pending_jobs = []
        self._batch_event = asyncio.Event()
    
    async def _do_initialize(self) -> None:
        """Initialize batch processing."""
        await super()._do_initialize()
        
        # Start batch processing task
        asyncio.create_task(self._process_batches())
    
    async def _process_batches(self):
        """Process jobs in batches."""
        while True:
            try:
                # Wait for batch to fill or timeout
                await asyncio.wait_for(
                    self._batch_event.wait(),
                    timeout=self._batch_timeout
                )
                
                # Process batch
                async with self._lock:
                    if not self._pending_jobs:
                        self._batch_event.clear()
                        continue
                    
                    batch = self._pending_jobs[:self._batch_size]
                    self._pending_jobs = self._pending_jobs[self._batch_size:]
                    self._batch_event.clear()
                
                # Execute batch
                await self._execute_batch(batch)
                
            except asyncio.TimeoutError:
                # Timeout is normal, process any pending jobs
                async with self._lock:
                    if self._pending_jobs:
                        batch = self._pending_jobs[:]
                        self._pending_jobs = []
                        await self._execute_batch(batch)
            except Exception as e:
                logger.error(f"Error in batch processing: {e}")
    
    async def _execute_batch(self, batch: List[Job]):
        """Execute a batch of jobs."""
        logger.info(f"Executing batch of {len(batch)} jobs")
        
        # Execute jobs concurrently
        tasks = [self.execute_job(job) for job in batch]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Process results
        for job, result in zip(batch, results):
            if isinstance(result, Exception):
                logger.error(f"Job {job.job_id} failed: {result}")
            else:
                logger.debug(f"Job {job.job_id} completed successfully")
    
    async def execute_job(self, job: Job) -> JobResult:
        """Execute a single job (overridden for batch processing)."""
        # For batch processing, add to pending queue
        async with self._lock:
            self._pending_jobs.append(job)
            self._batch_event.set()
        
        # Return a placeholder result
        return JobResult(
            job_id=job.job_id,
            status="pending",
            result=None,
            error=None,
            start_time=time.time(),
            end_time=time.time()
        )
```

These advanced patterns demonstrate the flexibility and power of the service layer architecture in `naq`. By leveraging these techniques, you can build sophisticated, high-performance applications that make the most of the service layer's capabilities while maintaining clean, maintainable code.