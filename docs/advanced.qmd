---
title: "Advanced Usage"
---

This section covers advanced features and configuration options for optimizing `naq` in production environments.

## Utility Patterns for Robust Applications

NAQ provides a comprehensive set of utility patterns that help you build robust, maintainable applications. These utilities handle common patterns like error handling, retries, logging, and performance monitoring.

### Error Handling and Retry Patterns

Use NAQ's retry decorators and error handling utilities to make your job functions more resilient:

```python
from naq.utils import retry, log_errors, async_error_handler_context
from naq.utils.error_handling import get_global_error_handler
from naq import enqueue_async

# Configure global error handler
error_handler = get_global_error_handler()
error_handler.register_handler(
    ConnectionError,
    lambda err, ctx, extra: print(f"Connection failed in {ctx}: {err}")
)

@retry(max_attempts=3, delay=2.0, backoff="exponential")
@log_errors(reraise=True)
async def unreliable_api_call(user_id: int):
    """Job function with automatic retry and error logging."""
    async with async_error_handler_context(
        error_handler, f"api_call_user_{user_id}"
    ):
        # Simulate unreliable API call
        if random.random() < 0.3:  # 30% failure rate
            raise ConnectionError("API temporarily unavailable")
        return f"Data for user {user_id}"

# Enqueue the job - retries happen automatically
await enqueue_async(unreliable_api_call, 123)
```

### Performance Monitoring and Structured Logging

Track performance and use structured logging for better observability:

```python
from naq.utils import timing, StructuredLogger, performance_context
from naq import enqueue_async

# Create structured logger with context
logger = StructuredLogger("data_processor", {
    "service": "background_jobs",
    "version": "1.0.0"
})

@timing(threshold_ms=5000)  # Log if takes more than 5 seconds
async def process_large_dataset(dataset_id: str, batch_size: int = 1000):
    """Process large dataset with performance monitoring."""
    
    async with performance_context("dataset_processing", logger) as perf:
        # Add custom metrics to performance context
        perf['dataset_id'] = dataset_id
        perf['batch_size'] = batch_size
        
        with logger.operation_context("load_dataset", dataset_id=dataset_id):
            dataset = await load_dataset(dataset_id)
            logger.info(f"Loaded dataset with {len(dataset)} records")
        
        with logger.operation_context("process_batches", batch_count=len(dataset) // batch_size):
            results = []
            for i in range(0, len(dataset), batch_size):
                batch = dataset[i:i + batch_size]
                result = await process_batch(batch)
                results.extend(result)
                
                # Log progress
                logger.info(
                    f"Processed batch {i // batch_size + 1}",
                    batch_index=i // batch_size + 1,
                    records_processed=len(results)
                )
        
        return results

# Enqueue with structured context
logger.info("Enqueueing dataset processing job", dataset_id="dataset_123")
await enqueue_async(process_large_dataset, "dataset_123", batch_size=500)
```

### Async Utilities and Concurrency Control

Control concurrency and manage async operations efficiently:

```python
from naq.utils.async_helpers import gather_with_concurrency, async_map
from naq import enqueue_async

async def fetch_user_data(user_id: int):
    """Fetch data for a single user."""
    # Simulate API call
    await asyncio.sleep(0.1)
    return f"Data for user {user_id}"

async def bulk_user_processing(user_ids: list[int]):
    """Process multiple users with controlled concurrency."""
    
    # Process users with max 10 concurrent operations
    tasks = [fetch_user_data(uid) for uid in user_ids]
    results = await gather_with_concurrency(tasks, concurrency=10)
    
    # Alternative: use async_map for the same result
    # results = await async_map(fetch_user_data, user_ids, concurrency=10)
    
    return results

# Enqueue bulk processing job
user_ids = list(range(1, 101))  # 100 users
await enqueue_async(bulk_user_processing, user_ids)
```

### Context Managers for Resource Management

Use context managers for safe resource handling:

```python
from naq.utils.context_managers import timeout_context, managed_resource
from naq import enqueue_async

async def acquire_database():
    """Acquire database connection."""
    return await database.connect()

async def release_database(conn):
    """Release database connection."""
    await conn.close()

async def database_heavy_job(query: str):
    """Job that requires database access with timeout."""
    
    async with timeout_context(30.0, "Database operation timeout"):
        async with managed_resource(acquire_database, release_database) as db:
            # Database operations are automatically timed out and cleaned up
            result = await db.execute(query)
            return result.fetchall()

# Enqueue database job with automatic resource management
await enqueue_async(database_heavy_job, "SELECT * FROM large_table")
```

## Efficient Connection Handling & Batching

When enqueuing many jobs in a tight loop, creating a new NATS connection for each job is inefficient. `naq` provides several ways to manage connections for high-throughput scenarios.

### Using a `Queue` Instance (Async)

For asynchronous applications, the most efficient way to enqueue jobs is to instantiate a `Queue` object and reuse it. The `Queue` instance manages a persistent connection to NATS.

```python
# async_batch_enqueue.py
import asyncio
from naq.queue import Queue

async def my_task(i):
    return f"Processed item {i}"

async def main():
    # Create a single Queue instance for the 'high_volume' queue
    queue = Queue(name="high_volume")

    print("Enqueuing 1,000 jobs using a single connection...")
    tasks = []
    for i in range(1000):
        task = queue.enqueue(my_task, i)
        tasks.append(task)

    await asyncio.gather(*tasks)
    print("All jobs enqueued.")

    # The connection remains open until the Queue object is no longer in use
    # or explicitly closed.
    await queue.close()

if __name__ == "__main__":
    asyncio.run(main())
```

### Thread-Local Connections (Sync)

The synchronous helper functions (`enqueue_sync`, `enqueue_at_sync`, etc.) are optimized for batching out of the box. They automatically use a **thread-local NATS connection**. This means that all calls to these functions from the same thread will reuse the same connection, avoiding the overhead of reconnecting each time.

```python
# sync_batch_enqueue.py
from naq import enqueue_sync, close_sync_connections

def my_task(i):
    return f"Processed item {i}"

def main():
    print("Enqueuing 1,000 jobs using a thread-local connection...")
    for i in range(1000):
        enqueue_sync(my_task, i)
    print("All jobs enqueued.")

    # Optionally, you can explicitly close the thread-local connection
    # when you are done with a batch. This is not required, as connections
    # are also closed on process exit.
    close_sync_connections()
    print("Thread-local connection closed.")

if __name__ == "__main__":
    main()
```

## Service Layer Architecture

NAQ features a comprehensive service layer that provides centralized connection management, dependency injection, and resource lifecycle management. This architecture improves performance, reliability, and maintainability.

### Core Service Components

The service layer consists of several key components:

- **ServiceManager**: Central coordinator for all services with dependency injection
- **ConnectionService**: Manages NATS connections with pooling and failover
- **StreamService**: Handles JetStream operations for job queues
- **KVStoreService**: Manages KeyValue store operations for results and metadata
- **JobService**: Orchestrates job execution and result handling
- **EventService**: Manages comprehensive event logging and processing

### Using the Service Layer

```python
from naq.services import create_service_manager_from_config, BaseService
from naq.config import load_config

# Create service manager with typed configuration
service_manager = create_service_manager_from_config("./config.yaml")

# Use services in your application
async def process_jobs():
    async with service_manager as services:
        # Get job service (automatically gets other dependencies)
        job_service = await services.get_service(JobService)
        
        # Process jobs with managed connections and resources
        result = await job_service.execute_job(my_job)
        return result

# Run with proper resource management
await process_jobs()
```

### Connection Management Benefits

The centralized connection management provides several advantages:

- **Connection Pooling**: Efficient reuse of NATS connections across services
- **Automatic Failover**: Transparent failover to backup NATS servers
- **Resource Cleanup**: Automatic cleanup of connections and streams
- **Configuration Integration**: Direct integration with YAML configuration system
- **Type Safety**: Full type safety with IDE support

### Custom Services

You can create custom services that integrate with the NAQ service layer:

```python
from naq.services import BaseService
from naq.config.types import NAQConfig

class CustomAnalyticsService(BaseService):
    """Custom service for analytics processing."""
    
    def __init__(self, config: NAQConfig):
        super().__init__(config)
        self.analytics_enabled = config.get_nested("custom.analytics.enabled", True)
        
    async def _do_initialize(self):
        """Initialize analytics connections."""
        # Access NATS config with type safety
        nats_servers = self.nats_config.servers
        self.connection = await self._create_analytics_connection(nats_servers)
        
    async def _do_cleanup(self):
        """Clean up analytics resources."""
        if hasattr(self, 'connection'):
            await self.connection.close()
    
    async def track_job_metrics(self, job_id: str, metrics: dict):
        """Track job performance metrics."""
        if not self.analytics_enabled:
            return
            
        await self.connection.publish(
            f"analytics.job.{job_id}",
            json.dumps(metrics).encode()
        )

# Register and use custom service
service_manager.register_service(CustomAnalyticsService)
analytics = await service_manager.get_service(CustomAnalyticsService)
await analytics.track_job_metrics("job-123", {"duration_ms": 1500})
```

## Configuration via YAML and Environment Variables

NAQ supports comprehensive configuration through YAML files with full backward compatibility for environment variables. The new configuration system provides better structure, validation, and type safety while maintaining all existing functionality.

### YAML Configuration (Recommended)

For new projects, use YAML configuration files for better structure and maintainability:

```yaml
# naq.yaml
nats:
  servers: ["nats://localhost:4222"]
  client_name: "my-app"
  max_reconnect_attempts: 5
  
workers:
  concurrency: 10
  heartbeat_interval: 15
  pools:
    high_priority:
      concurrency: 20
      queues: ["urgent", "critical"]
      
events:
  enabled: true
  batch_size: 100
  
logging:
  level: "INFO"
  format: "json"
```

**See the complete [Configuration Guide](configuration.qmd) for detailed YAML configuration options.**

### Environment Variables (Legacy Support)

All existing environment variables continue to work and will override YAML settings:

| Variable                      | YAML Path | Default                        | Description                                                                                             |
| ----------------------------- | --------- | ------------------------------ | ------------------------------------------------------------------------------------------------------- |
| `NAQ_NATS_URL`                | `nats.servers[0]` | `nats://localhost:4222`        | The URL of the NATS server.                                                                             |
| `NAQ_DEFAULT_QUEUE`           | `queues.default` | `naq_default_queue`            | The default queue name used when none is specified.                                                     |
| `NAQ_JOB_SERIALIZER`          | `serialization.job_serializer` | `pickle`                       | The serializer for jobs. Can be `pickle` or `json`. **See security note below.**                        |
| `NAQ_DEFAULT_RESULT_TTL`      | `results.ttl` | `604800` (7 days)              | Default time-to-live (in seconds) for job results stored in NATS.                                       |
| `NAQ_SCHEDULER_LOCK_TTL`      | `scheduler.lock_ttl` | `30`                           | TTL (in seconds) for the scheduler's high-availability leader lock.                                       |
| `NAQ_WORKER_TTL`              | `workers.ttl` | `60`                           | TTL (in seconds) for a worker's heartbeat. If a worker is silent for this long, it's considered dead.    |
| `NAQ_WORKER_HEARTBEAT_INTERVAL` | `workers.heartbeat_interval` | `15`                           | How often (in seconds) a worker sends a heartbeat to NATS.                                              |
| `NAQ_LOG_LEVEL`               | `logging.level` | `CRITICAL`                     | The logging level for `naq` components. Can be `DEBUG`, `INFO`, `WARNING`, `ERROR`.                       |
| `NAQ_EVENTS_ENABLED`          | `events.enabled` | `true`                        | Enable/disable event logging system globally |
| `NAQ_EVENT_LOGGER_BATCH_SIZE` | `events.batch_size` | `100`                         | Number of events to batch before flushing |
| `NAQ_CONCURRENCY`             | `workers.concurrency` | `10`                          | Default worker concurrency level |

### Configuration Priority

Configuration values are loaded with the following priority (highest to lowest):

1. **Command-line config file** (`--config ./my-config.yaml`)
2. **Local config file** (`./naq.yaml` or `./naq.yml`)
3. **User config file** (`~/.naq/config.yaml`)
4. **System config file** (`/etc/naq/config.yaml`)
5. **Environment variables** (`NAQ_*` variables)
6. **Built-in defaults**

### Environment Variable Interpolation

YAML configuration supports environment variable interpolation:

```yaml
nats:
  servers: ["${NAQ_NATS_URL:nats://localhost:4222}"]
  auth:
    username: "${NATS_USER}"
    password: "${NATS_PASS}"
    
workers:
  concurrency: "${NAQ_CONCURRENCY:10}"
```

## Job Serialization (`pickle` vs. `json`)

`naq` uses a serializer to convert job data (the function and its arguments) into a format that can be stored in NATS. You can choose between two built-in serializers.

### `pickle` (Default)

-   **Pros**: Can serialize almost any Python object, including complex custom classes, lambdas, and functions defined in a REPL.
-   **Cons**: **Not secure**. A malicious actor who can enqueue jobs could craft a `pickle` payload that executes arbitrary code on your workers.

### `json` (Recommended for Production)

-   **Pros**: **Secure**. Only serializes basic data types (strings, numbers, lists, dicts). Functions are referenced by their import path (e.g., `my_app.tasks.process_data`), not serialized directly. This prevents arbitrary code execution.
-   **Cons**: Less flexible. Cannot serialize complex Python objects that don't have a natural JSON representation.

To use the `json` serializer, set the following environment variable:

```bash
export NAQ_JOB_SERIALIZER=json
```

::: {.callout-warning}
**Security Warning**

It is **strongly recommended** to use the `json` serializer in any environment where the job producer is not fully trusted.
:::

## Event System Configuration

NAQ's event-driven state management system is highly configurable for different deployment scenarios and performance requirements.

### Core Event Logging Settings

| Environment Variable | Default Value | Description |
|---------------------|---------------|-------------|
| `NAQ_EVENT_LOGGING_ENABLED` | `true` | Enable/disable event logging system globally |
| `NAQ_EVENT_STREAM_NAME` | `NAQ_JOB_EVENTS` | Name of the NATS JetStream stream for events |
| `NAQ_EVENT_SUBJECT_PREFIX` | `naq.jobs.events` | Subject prefix for event messages |
| `NAQ_EVENT_STREAM_MAX_AGE` | `168h` | Maximum age for events (7 days) |
| `NAQ_EVENT_STREAM_MAX_BYTES` | `1GB` | Maximum storage for event stream |

### Event Logger Performance

| Environment Variable | Default Value | Description |
|---------------------|---------------|-------------|
| `NAQ_EVENT_LOGGER_BATCH_SIZE` | `100` | Number of events to batch before flushing |
| `NAQ_EVENT_LOGGER_FLUSH_INTERVAL` | `5.0` | Maximum seconds between flushes |
| `NAQ_EVENT_LOGGER_MAX_PENDING` | `1000` | Maximum pending events in memory |

### Example Production Configuration

```bash
# Enable event logging with optimized settings
export NAQ_EVENT_LOGGING_ENABLED=true
export NAQ_EVENT_STREAM_MAX_AGE=720h    # 30 days retention
export NAQ_EVENT_STREAM_MAX_BYTES=5GB   # Larger storage limit

# Performance tuning for high-throughput scenarios
export NAQ_EVENT_LOGGER_BATCH_SIZE=250  # Larger batches
export NAQ_EVENT_LOGGER_FLUSH_INTERVAL=2.0  # More frequent flushes
export NAQ_EVENT_LOGGER_MAX_PENDING=2000    # Higher memory buffer

# Reduce worker heartbeat noise
export NAQ_WORKER_HEARTBEAT_INTERVAL=30  # Less frequent heartbeats
```

## Custom Event Processing

### Building Custom Event Processors

Create specialized event processors for your specific needs:

```python
# custom_processor.py
import asyncio
from naq.events import AsyncJobEventProcessor, JobEventType
from datetime import datetime, timedelta

class PerformanceAnalyzer:
    """Custom processor for performance analysis."""
    
    def __init__(self, alert_threshold_ms=10000):
        self.alert_threshold = alert_threshold_ms
        self.slow_jobs = []
        self.queue_stats = {}
    
    async def start_monitoring(self):
        """Start the performance monitoring system."""
        processor = AsyncJobEventProcessor()
        
        # Register handlers
        processor.add_handler(JobEventType.COMPLETED, self._analyze_completion)
        processor.add_handler(JobEventType.FAILED, self._analyze_failure)
        
        await processor.start()
        
        try:
            # Process events continuously
            async for event in processor.stream_job_events():
                pass
        finally:
            await processor.stop()
    
    async def _analyze_completion(self, event):
        """Analyze completed job performance."""
        if event.duration_ms and event.duration_ms > self.alert_threshold:
            self.slow_jobs.append({
                'job_id': event.job_id,
                'duration': event.duration_ms,
                'queue': event.queue_name,
                'worker': event.worker_id,
                'timestamp': event.timestamp
            })
            
            # Send alert
            await self._send_performance_alert(event)
        
        # Update queue statistics
        queue = event.queue_name or 'default'
        if queue not in self.queue_stats:
            self.queue_stats[queue] = {
                'completed': 0,
                'total_duration': 0,
                'avg_duration': 0
            }
        
        stats = self.queue_stats[queue]
        stats['completed'] += 1
        stats['total_duration'] += (event.duration_ms or 0)
        stats['avg_duration'] = stats['total_duration'] / stats['completed']
    
    async def _analyze_failure(self, event):
        """Analyze job failures."""
        print(f"🚨 Job failure analysis: {event.job_id}")
        print(f"   Error: {event.error_message}")
        print(f"   Queue: {event.queue_name}")
        print(f"   Worker: {event.worker_id}")
    
    async def _send_performance_alert(self, event):
        """Send performance alert."""
        print(f"⚠️  SLOW JOB ALERT: {event.job_id}")
        print(f"   Duration: {event.duration_ms}ms (threshold: {self.alert_threshold}ms)")
        print(f"   Queue: {event.queue_name}")
        
        # In production: send to Slack, PagerDuty, etc.
    
    def get_performance_report(self):
        """Generate performance report."""
        return {
            'slow_jobs_count': len(self.slow_jobs),
            'queue_performance': self.queue_stats,
            'recent_slow_jobs': self.slow_jobs[-10:]
        }

# Usage
analyzer = PerformanceAnalyzer(alert_threshold_ms=5000)
asyncio.run(analyzer.start_monitoring())
```

### Integration with Monitoring Systems

#### Prometheus Metrics

```python
# prometheus_integration.py
import time
from prometheus_client import Counter, Histogram, Gauge, start_http_server
from naq.events import AsyncJobEventProcessor, JobEventType

# Define metrics
job_total = Counter('naq_jobs_total', 'Total jobs processed', ['queue', 'status'])
job_duration = Histogram('naq_job_duration_seconds', 'Job duration', ['queue'])
active_workers = Gauge('naq_active_workers', 'Number of active workers')

class PrometheusExporter:
    """Export NAQ metrics to Prometheus."""
    
    def __init__(self, port=8000):
        self.port = port
        self.active_worker_count = 0
    
    async def start_exporter(self):
        """Start the Prometheus metrics exporter."""
        # Start HTTP server for metrics
        start_http_server(self.port)
        print(f"📊 Prometheus metrics available at http://localhost:{self.port}/metrics")
        
        # Start event processing
        processor = AsyncJobEventProcessor()
        
        processor.add_handler(JobEventType.COMPLETED, self._handle_completed)
        processor.add_handler(JobEventType.FAILED, self._handle_failed)
        processor.add_handler(JobEventType.ENQUEUED, self._handle_enqueued)
        processor.add_global_handler(self._handle_worker_events)
        
        await processor.start()
        
        try:
            async for event in processor.stream_job_events():
                pass
        finally:
            await processor.stop()
    
    async def _handle_completed(self, event):
        """Handle completed job metrics."""
        queue = event.queue_name or 'default'
        job_total.labels(queue=queue, status='completed').inc()
        
        if event.duration_ms:
            duration_seconds = event.duration_ms / 1000.0
            job_duration.labels(queue=queue).observe(duration_seconds)
    
    async def _handle_failed(self, event):
        """Handle failed job metrics."""
        queue = event.queue_name or 'default'
        job_total.labels(queue=queue, status='failed').inc()
    
    async def _handle_enqueued(self, event):
        """Handle enqueued job metrics."""
        queue = event.queue_name or 'default'
        job_total.labels(queue=queue, status='enqueued').inc()
    
    async def _handle_worker_events(self, event):
        """Handle worker events for active count."""
        if hasattr(event, 'details') and event.details:
            if event.details.get('event_category') == 'worker':
                if 'worker_started' in event.event_type.value:
                    self.active_worker_count += 1
                elif 'worker_stopped' in event.event_type.value:
                    self.active_worker_count -= 1
                
                active_workers.set(self.active_worker_count)

# Usage
exporter = PrometheusExporter(port=8000)
asyncio.run(exporter.start_exporter())
```

## Performance Optimization

### High-Volume Event Logging

For high-throughput systems, optimize event logging performance:

```python
# high_performance_config.py
import os

# Optimize for high volume
os.environ.update({
    'NAQ_EVENT_LOGGER_BATCH_SIZE': '500',        # Larger batches
    'NAQ_EVENT_LOGGER_FLUSH_INTERVAL': '1.0',    # Faster flushes
    'NAQ_EVENT_LOGGER_MAX_PENDING': '5000',      # Higher memory buffer
    
    # Reduce event noise
    'NAQ_WORKER_HEARTBEAT_INTERVAL': '60',       # Less frequent heartbeats
    
    # Optimize NATS JetStream
    'NAQ_EVENT_STREAM_MAX_BYTES': '10GB',        # Larger stream
    'NAQ_EVENT_STREAM_MAX_AGE': '48h',           # Shorter retention
})
```

### Memory Management

Monitor and control memory usage:

```python
# memory_monitoring.py
import asyncio
import psutil
from naq.events import AsyncJobEventLogger

class MemoryAwareLogger:
    """Event logger with memory monitoring."""
    
    def __init__(self, memory_threshold_mb=1000):
        self.threshold = memory_threshold_mb
        self.logger = AsyncJobEventLogger()
        self._monitoring = True
    
    async def start_with_monitoring(self):
        """Start logger with memory monitoring."""
        await self.logger.start()
        
        # Start memory monitoring task
        monitor_task = asyncio.create_task(self._monitor_memory())
        
        try:
            # Your application logic here
            pass
        finally:
            self._monitoring = False
            monitor_task.cancel()
            await self.logger.stop()
    
    async def _monitor_memory(self):
        """Monitor memory usage and adjust batch size."""
        while self._monitoring:
            process = psutil.Process()
            memory_mb = process.memory_info().rss / 1024 / 1024
            
            if memory_mb > self.threshold:
                print(f"⚠️  High memory usage: {memory_mb:.1f}MB")
                # Force flush to reduce memory
                await self.logger.flush()
            
            await asyncio.sleep(30)  # Check every 30 seconds

# Usage
memory_logger = MemoryAwareLogger(memory_threshold_mb=800)
asyncio.run(memory_logger.start_with_monitoring())
```

## Troubleshooting

### Common Event System Issues

**Events not appearing in stream:**

1. Check if event logging is enabled:
   ```bash
   echo $NAQ_EVENT_LOGGING_ENABLED
   ```

2. Verify NATS connectivity:
   ```bash
   nats stream info NAQ_JOB_EVENTS
   ```

3. Check event logger lifecycle:
   ```python
   logger = AsyncJobEventLogger()
   await logger.start()  # Must call start()
   # ... log events
   await logger.stop()   # Must call stop()
   ```

**High memory usage:**

1. Reduce batch size:
   ```bash
   export NAQ_EVENT_LOGGER_BATCH_SIZE=50
   ```

2. Increase flush frequency:
   ```bash
   export NAQ_EVENT_LOGGER_FLUSH_INTERVAL=2.0
   ```

3. Monitor pending events:
   ```python
   # In your application
   if logger._pending_events_count > 500:
       await logger.flush()
   ```

**Slow event processing:**

1. Increase batch size for better throughput:
   ```bash
   export NAQ_EVENT_LOGGER_BATCH_SIZE=300
   ```

2. Use dedicated NATS cluster for events
3. Consider event sampling for very high-volume scenarios

### Debug Commands

```bash
# Check event stream status
nats stream info NAQ_JOB_EVENTS

# Monitor event rate
naq events --format raw | pv -l

# Test event logging
python -c "
import asyncio
from naq.events import AsyncJobEventLogger

async def test():
    logger = AsyncJobEventLogger()
    await logger.start()
    await logger.log_job_enqueued('test-job', 'test-queue')
    await logger.flush()
    await logger.stop()
    print('Event logged successfully')

asyncio.run(test())
"
```