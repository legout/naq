---
title: "Events API"
---

The `naq.events` module now primarily exposes the `EventService` as the public API for managing job events. This service provides a comprehensive event logging and monitoring system for `naq`, allowing you to track the entire lifecycle of your jobs, from enqueuing to completion or failure, and react to these events in real-time.

Underneath, the `EventService` leverages NATS JetStream for durable event storage and efficient processing. The internal components like `NATSJobEventStorage`, `AsyncJobEventLogger`, and `SharedEventLoggerManager` are now primarily used by the `EventService` and are not typically interacted with directly by users.

## Core Components

### `naq.events.JobEventType`

An enumeration of all possible event types in a job's lifecycle.

```python
class JobEventType(str, Enum):
    ENQUEUED = "enqueued"
    STARTED = "started"
    COMPLETED = "completed"
    FAILED = "failed"
    RETRY_SCHEDULED = "retry_scheduled"
    SCHEDULED = "scheduled"
    SCHEDULE_TRIGGERED = "schedule_triggered"
    CANCELLED = "cancelled"
    STATUS_CHANGED = "status_changed"
```

#### Event Type Descriptions

| Event Type | Description | Triggered By |
|------------|-------------|--------------|
| `ENQUEUED` | Job has been added to a queue | Client/Queue |
| `STARTED` | Job has begun execution by a worker | Worker |
| `COMPLETED` | Job finished successfully | Worker |
| `FAILED` | Job failed during execution | Worker |
| `RETRY_SCHEDULED` | Job failed and will be retried | Worker |
| `SCHEDULED` | Job has been scheduled for future execution | Scheduler |
| `SCHEDULE_TRIGGERED` | Scheduled job is now being enqueued | Scheduler |
| `CANCELLED` | Job was cancelled before or during execution | Worker/Client |
| `STATUS_CHANGED` | Job status has changed (any transition) | All Components |

### `naq.events.JobEvent`

A `msgspec.Struct` representing a single event in a job's lifecycle. This is the primary data structure for all event information.

#### Properties

| Property          | Type                               | Description                                                                 |
| ----------------- | ---------------------------------- | --------------------------------------------------------------------------- |
| `job_id`          | `str`                              | The unique identifier of the job this event belongs to.                      |
| `event_type`      | `JobEventType`                     | The type of event that occurred.                                            |
| `timestamp`       | `float`                            | The UTC timestamp when the event was generated.                             |
| `worker_id`       | `str \| None`                      | The ID of the worker that processed the job (if applicable).                |
| `queue_name`      | `str \| None`                      | The name of the queue the job was in.                                       |
| `message`         | `str \| None`                      | A human-readable message describing the event.                              |
| `details`         | `dict \| None`                     | Additional structured details about the event.                              |
| `error_type`      | `str \| None`                      | The type of error if the event represents a failure.                       |
| `error_message`   | `str \| None`                      | The error message if the event represents a failure.                       |
| `duration_ms`     | `int \| None`                      | The duration of the job execution in milliseconds (if applicable).          |
| `nats_subject`    | `str \| None`                      | The NATS subject the event was published to.                                |
| `nats_sequence`   | `int \| None`                      | The NATS stream sequence number of the event.                               |

#### Convenience Methods

The `JobEvent` class provides several class methods for easily creating specific event types:

-   `JobEvent.enqueued(job_id: str, queue_name: str, **kwargs) -> JobEvent`
-   `JobEvent.started(job_id: str, worker_id: str, queue_name: str, **kwargs) -> JobEvent`
-   `JobEvent.completed(job_id: str, worker_id: str, queue_name: str, duration_ms: int, **kwargs) -> JobEvent`
-   `JobEvent.failed(job_id: str, worker_id: str, queue_name: str, error_type: str, error_message: str, **kwargs) -> JobEvent`
-   `JobEvent.retry_scheduled(job_id: str, worker_id: str, queue_name: str, **kwargs) -> JobEvent`
-   `JobEvent.scheduled(job_id: str, **kwargs) -> JobEvent`
-   `JobEvent.schedule_triggered(job_id: str, **kwargs) -> JobEvent`
-   `JobEvent.cancelled(job_id: str, queue_name: str, **kwargs) -> JobEvent`
-   `JobEvent.status_changed(job_id: str, queue_name: str, old_status: str, new_status: str, **kwargs) -> JobEvent`

### Internal Components (Used by `EventService`)

The following components are internal to the `EventService` and are generally not intended for direct user interaction. They are documented here for completeness and advanced understanding of the event system's architecture.

#### `naq.events.NATSJobEventStorage`

A storage backend for job events using NATS JetStream. This class is responsible for persisting and retrieving events. You typically won't need to interact with this class directly unless you are building custom tools.

#### Initialization

```python
async def __init__(
    self,
    nats_url: str = DEFAULT_NATS_URL,
    stream_name: str = "NAQ_JOB_EVENTS",
    subject_prefix: str = "naq.jobs.events",
    stream_config: Optional[StreamConfig] = None
)
```

| Parameter        | Type                      | Description                                                              |
| ---------------- | ------------------------- | ------------------------------------------------------------------------ |
| `nats_url`       | `str`                     | The URL of the NATS server.                                               |
| `stream_name`    | `str`                     | The name of the JetStream stream to use for events.                       |
| `subject_prefix` | `str`                     | The prefix for NATS subjects used for events.                             |
| `stream_config`  | `Optional[StreamConfig]` | Custom JetStream stream configuration. If `None`, defaults are used.      |

#### Key Methods

-   `async def store_event(self, event: JobEvent) -> None`: Stores a single event.
-   `async def get_events(self, job_id: str) -> List[JobEvent]`: Retrieves all events for a given job ID.
-   `async def stream_events(self, **kwargs) -> AsyncIterator[JobEvent]`: Streams new events in real-time. Accepts filters like `context`, `event_type`, etc.

#### `naq.events.AsyncJobEventLogger`

A high-performance, non-blocking logger that buffers events in memory and flushes them periodically to the `NATSJobEventStorage`. This is the primary interface for logging events from your application.

#### Initialization

```python
async def __init__(
    self,
    storage: NATSJobEventStorage,
    batch_size: int = 100,
    flush_interval: float = 5.0,
    max_buffer_size: int = 10000
)
```

| Parameter         | Type                 | Description                                                                |
| ----------------- | -------------------- | -------------------------------------------------------------------------- |
| `storage`         | `NATSJobEventStorage`| The storage backend instance to use.                                        |
| `batch_size`      | `int`                | The number of events to buffer before flushing.                            |
| `flush_interval`  | `float`              | The interval in seconds to flush buffered events.                          |
| `max_buffer_size` | `int`                | The maximum number of events to hold in the buffer before dropping new ones.|

#### Key Methods

-   `async def start(self) -> None`: Starts the background flush task.
-   `async def stop(self) -> None`: Stops the background flush task and flushes any remaining events.
-   `async def log_event(self, event: JobEvent) -> None`: Logs a generic `JobEvent`.
-   `async def log_job_enqueued(self, job_id: str, queue_name: str, **kwargs) -> None`: Logs an `ENQUEUED` event.
-   `async def log_job_started(self, job_id: str, worker_id: str, queue_name: str, **kwargs) -> None`: Logs a `STARTED` event.
-   `async def log_job_completed(self, job_id: str, worker_id: str, queue_name: str, duration_ms: int, **kwargs) -> None`: Logs a `COMPLETED` event.
-   `async def log_job_failed(self, job_id: str, worker_id: str, queue_name: str, error_type: str, error_message: str, **kwargs) -> None`: Logs a `FAILED` event.
-   `async def log_job_retry_scheduled(self, job_id: str, worker_id: str, queue_name: str, **kwargs) -> None`: Logs a `RETRY_SCHEDULED` event.
-   `async def log_job_scheduled(self, job_id: str, **kwargs) -> None`: Logs a `SCHEDULED` event.
-   `async def log_job_schedule_triggered(self, job_id: str, **kwargs) -> None`: Logs a `SCHEDULE_TRIGGERED` event.
-   `async def log_job_cancelled(self, job_id: str, queue_name: str, **kwargs) -> None`: Logs a `CANCELLED` event.
-   `async def log_job_status_changed(self, job_id: str, queue_name: str, old_status: str, new_status: str, **kwargs) -> None`: Logs a `STATUS_CHANGED` event.

#### `naq.events.JobEventLogger`

A synchronous wrapper around `AsyncJobEventLogger`, following the `_sync` pattern used elsewhere in `naq`. It provides the same interface but can be used in synchronous contexts.

#### Initialization

```python
def __init__(
    self,
    nats_url: str = DEFAULT_NATS_URL,
    **logger_kwargs
)
```

| Parameter       | Type  | Description                                                      |
| --------------- | ----- | ---------------------------------------------------------------- |
| `nats_url`      | `str` | The URL of the NATS server.                                       |
| `logger_kwargs` | `dict`| Keyword arguments to pass to the `AsyncJobEventLogger` constructor. |

The methods are the same as `AsyncJobEventLogger` but without the `async`/`await` keywords.

#### `naq.events.SharedEventLoggerManager`

A singleton manager for shared event logger instances that provides a centralized way to create, manage, and share event logger instances across different components in the NAQ system.

#### Key Features

- **Singleton Pattern**: Ensures a single instance across the application
- **Thread-safe**: Safe for use in multi-threaded environments
- **Dual Support**: Manages both synchronous and asynchronous loggers
- **Centralized Configuration**: Configure event logging once for all components

#### Initialization

The `SharedEventLoggerManager` is a singleton and should be accessed through the global instance:

```python
from naq.events import get_shared_event_logger_manager

# Get the global instance
manager = get_shared_event_logger_manager()
```

#### Configuration

```python
def configure(
    self,
    enabled: Optional[bool] = None,
    storage_type: Optional[str] = None,
    storage_url: Optional[str] = None,
    stream_name: Optional[str] = None,
    subject_prefix: Optional[str] = None,
    **kwargs: Any
) -> None
```

| Parameter | Type | Description |
|-----------|------|-------------|
| `enabled` | `Optional[bool]` | Whether event logging is enabled |
| `storage_type` | `Optional[str]` | Type of storage backend ('nats', etc.) |
| `storage_url` | `Optional[str]` | URL for the storage backend |
| `stream_name` | `Optional[str]` | Name of the stream for events |
| `subject_prefix` | `Optional[str]` | Base subject for events |
| `**kwargs` | `Any` | Additional configuration options |

#### Key Methods

-   `get_sync_logger() -> Optional[JobEventLogger]`: Get the shared synchronous event logger instance.
-   `async get_async_logger() -> Optional[AsyncJobEventLogger]`: Get the shared asynchronous event logger instance.
-   `configure(**kwargs) -> None`: Configure the shared event logger manager.
-   `reset() -> None`: Reset the shared event logger manager (synchronous).
-   `async async_reset() -> None`: Reset the shared event logger manager (asynchronous).
-   `get_config() -> Dict[str, Any]`: Get the current configuration.
-   `is_enabled() -> bool`: Check if event logging is enabled.

#### Convenience Functions

For easier access, the module provides global convenience functions:

```python
from naq.events import (
    get_shared_sync_logger,
    get_shared_async_logger,
    configure_shared_logger,
)

# Configure the shared logger
configure_shared_logger(enabled=True, storage_url="nats://localhost:4222")

# Get loggers
sync_logger = get_shared_sync_logger()
async_logger = await get_shared_async_logger()
```

### `EventService`

The `EventService` is the primary public API for interacting with NAQ's event system. It provides methods for logging job events and consuming events in real-time or retrieving historical event data.

#### API Reference

```python
class EventService(BaseService):
    """Service for managing event logging and processing."""
    
    async def log_event(self, event: JobEvent) -> None:
        """Log a generic job event."""
        
    async def log_job_enqueued(
        self,
        job_id: str,
        queue_name: str,
        **kwargs: Any
    ) -> None:
        """Log a job enqueued event."""
        
    async def log_job_started(
        self,
        job_id: str,
        worker_id: str,
        queue_name: str,
        **kwargs: Any
    ) -> None:
        """Log a job started event."""
        
    async def log_job_completed(
        self,
        job_id: str,
        worker_id: str,
        queue_name: str,
        duration_ms: int,
        **kwargs: Any
    ) -> None:
        """Log a job completed event."""
        
    async def log_job_failed(
        self,
        job_id: str,
        worker_id: str,
        queue_name: str,
        error_type: str,
        error_message: str,
        **kwargs: Any
    ) -> None:
        """Log a job failed event."""
        
    async def log_job_retry_scheduled(
        self,
        job_id: str,
        worker_id: str,
        queue_name: str,
        **kwargs: Any
    ) -> None:
        """Log a job retry scheduled event."""
        
    async def log_job_scheduled(self, job_id: str, **kwargs: Any) -> None:
        """Log a job scheduled event."""
        
    async def log_job_schedule_triggered(self, job_id: str, **kwargs: Any) -> None:
        """Log a job schedule triggered event."""
        
    async def log_job_cancelled(
        self,
        job_id: str,
        queue_name: str,
        **kwargs: Any
    ) -> None:
        """Log a job cancelled event."""
        
    async def log_job_status_changed(
        self,
        job_id: str,
        queue_name: str,
        old_status: str,
        new_status: str,
        **kwargs: Any
    ) -> None:
        """Log a job status changed event."""
        
    async def stream_events(
        self,
        job_id: Optional[str] = None,
        event_type: Optional[JobEventType] = None,
        timeout: float = 0.0,
        batch_size: int = 1,
        **kwargs: Any
    ) -> AsyncIterator[JobEvent]:
        """
        Stream job events in real-time.
        
        Args:
            job_id: Optional job ID to filter events.
            event_type: Optional event type to filter events.
            timeout: Maximum time to wait for events (0 for indefinite).
            batch_size: Number of events to fetch in a single batch.
        """
        
    async def get_event_history(self, job_id: str) -> List[JobEvent]:
        """Retrieve all historical events for a given job ID."""
        
    async def flush_events(self) -> None:
        """Manually flush any buffered events to the storage backend."""
        
    def get_logger_stats(self) -> Dict[str, Any]:
        """Get statistics about the event logger's buffer."""
```

#### Key Methods

-   `async def start(self) -> None`: Starts the event processing loop.
-   `async def stop(self) -> None`: Stops the event processing loop.
-   `def add_handler(self, event_type: JobEventType, handler: Callable[[JobEvent], Any]) -> None`: Registers a handler function for a specific event type.
-   `def add_global_handler(self, handler: Callable[[JobEvent], Any]) -> None`: Registers a handler function that will be called for all events.

Handlers can be either synchronous or asynchronous functions that accept a single `JobEvent` argument. Synchronous handlers are automatically executed in a thread pool to avoid blocking the event loop.

#### Usage Examples

The `EventService` provides a unified interface for logging and consuming events. The following examples demonstrate common usage patterns.

##### Logging Events

###### Basic Event Logging

```python
from naq.services import ServiceManager, EventService
from naq.models import JobEvent, JobEventType
from naq.config import NAQConfig

async def log_example():
    config = NAQConfig.load_from_dict({'nats': {'url': 'nats://localhost:4222'}})
    
    async with ServiceManager(config) as services:
        event_service = await services.get_service(EventService)
        
        # Log a job enqueued event
        await event_service.log_job_enqueued(job_id="job-abc", queue_name="default_queue")
        
        # Log a job started event
        await event_service.log_job_started(job_id="job-abc", worker_id="worker-1", queue_name="default_queue")
        
        # Log a custom event
        custom_event = JobEvent(
            job_id="job-xyz",
            event_type=JobEventType.STATUS_CHANGED,
            queue_name="my_queue",
            details={"old_status": "pending", "new_status": "processing"}
        )
        await event_service.log_event(custom_event)
        
        print("Events logged successfully.")

# To run this example:
# import asyncio
# asyncio.run(log_example())
```

###### Streaming Real-time Events

```python
import asyncio
from naq.services import ServiceManager, EventService
from naq.models import JobEventType
from naq.config import NAQConfig

async def stream_events_example():
    config = NAQConfig.load_from_dict({'nats': {'url': 'nats://localhost:4222'}})
    
    async with ServiceManager(config) as services:
        event_service = await services.get_service(EventService)
        
        print("Streaming all job events...")
        # Stream events for all jobs, optionally filtering by type or other criteria
        async for event in event_service.stream_events(event_type=JobEventType.STARTED):
            print(f"Received streamed event: Job {event.job_id} - Type: {event.event_type} - Worker: {event.worker_id}")
            # Process the event, e.g., update a dashboard, trigger a notification

        # You would typically run this in a long-running process
        # await asyncio.sleep(3600) # Keep streaming for an hour or indefinitely

# To run this example:
# import asyncio
# asyncio.run(stream_events_example())
```

###### Retrieving Historical Events

```python
from naq.services import ServiceManager, EventService
from naq.config import NAQConfig

async def get_history_example():
    config = NAQConfig.load_from_dict({'nats': {'url': 'nats://localhost:4222'}})
    
    async with ServiceManager(config) as services:
        event_service = await services.get_service(EventService)
        
        job_id_to_check = "job-abc" # Replace with an actual job ID
        history = await event_service.get_event_history(job_id_to_check)
        
        if history:
            print(f"Event history for Job ID: {job_id_to_check}")
            for event in history:
                print(f"- Type: {event.event_type}, Timestamp: {event.timestamp}, Details: {event.details}")
        else:
            print(f"No event history found for Job ID: {job_id_to_check}")

# To run this example:
# import asyncio
# asyncio.run(get_history_example())
```

### Performance Considerations

When working with events, consider these performance best practices:

1. **Use Shared Loggers**: Always use the shared event logger to avoid creating multiple connections.
2. **Batch Processing**: The shared logger automatically batches events for better performance.
3. **Async Handlers**: Use async handlers for event processing to avoid blocking.
4. **Selective Subscriptions**: Only subscribe to events you need to process.
5. **Error Handling**: Implement proper error handling in your event handlers to prevent processing failures.