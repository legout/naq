---
title: "Queue API"
---

The `queue` module provides the primary interface for adding jobs to `naq`.

## `Queue` Class

The `Queue` class represents a job queue and is the main entry point for enqueuing tasks.

::: {.callout-note}
For simple, one-off enqueueing, you might prefer the helper functions like [`enqueue`](#enqueue-functions) which manage the `Queue` instance for you.
:::

### `naq.queue.Queue(name, config=None, services=None)`

The `Queue` class constructor now emphasizes the `config` parameter for centralized configuration and the `services` parameter for dependency injection. The `nats_url`, `default_timeout`, and `prefer_thread_local` parameters are now typically managed via the `config` object or internally by the service layer.

| Parameter            | Type                               | Description                                                               |
| -------------------- | ---------------------------------- | ------------------------------------------------------------------------- |
| `name`               | `str`                              | The name of the queue. Defaults to `naq_default_queue`.                   |
| `config`             | `NAQConfig` &#124; `dict` &#124; `None` | Optional NAQConfig object or dictionary for queue and service configuration. |
| `services`           | `ServiceManager` &#124; `None`     | Optional ServiceManager instance for dependency injection.                 |


### Methods

#### `enqueue()`

Enqueues a job for immediate execution. This method internally leverages the `JobService` to manage job lifecycle and results.

```python
async def enqueue(
    self,
    func: Callable,
    *args: Any,
    max_retries: Optional[int] = 0,
    retry_delay: RetryDelayType = 0,
    depends_on: Optional[Union[str, List[str], Job, List[Job]]] = None,
    timeout: Optional[int] = None,
    **kwargs: Any
) -> Job
```

#### `enqueue_at()`

Schedules a job to be enqueued at a specific `datetime`. This method internally uses the `SchedulerService` for job scheduling.

```python
async def enqueue_at(
    self,
    dt: datetime.datetime,
    func: Callable,
    *args: Any,
    ...
) -> Job
```

#### `enqueue_in()`

Schedules a job to be enqueued after a `timedelta`. This method internally uses the `SchedulerService` for job scheduling.

```python
async def enqueue_in(
    self,
    delta: timedelta,
    func: Callable,
    *args: Any,
    ...
) -> Job
```

#### `schedule()`

Schedules a job to run on a recurring basis. This method internally uses the `SchedulerService` for managing recurring jobs.

```python
async def schedule(
    self,
    func: Callable,
    *args: Any,
    cron: Optional[str] = None,
    interval: Optional[Union[timedelta, float, int]] = None,
    repeat: Optional[int] = None,
    ...
) -> Job
```

| Parameter  | Type                                | Description                                                              |
| ---------- | ----------------------------------- | ------------------------------------------------------------------------ |
| `cron`     | `str`                               | A cron string (e.g., `'*/5 * * * *'`) for the schedule.                   |
| `interval` | `timedelta` &#124; `float` &#124; `int` | The interval in seconds or as a `timedelta` between job runs.            |
| `repeat`   | `int` &#124; `None`                  | The number of times to repeat the job. `None` for indefinitely.          |

#### `purge()`

Removes all jobs from the queue. This operation typically interacts with the underlying NATS JetStream stream managed by the `StreamService`.

```python
async def purge(self) -> int
```

Returns the number of jobs purged.

#### `cancel_scheduled_job()`

Cancels a scheduled or recurring job. This method internally uses the `SchedulerService` to manage scheduled jobs.

```python
async def cancel_scheduled_job(self, job_id: str) -> bool
```

Returns `True` if the job was found and canceled.

## Service Layer Integration

The `Queue` class integrates with the service layer architecture to provide efficient resource management and connection pooling. When using the service layer, the queue automatically leverages centralized services for NATS connections, stream management, and other resources.

### Queue Class Definition and Service Layer Integration

The `Queue` class is designed to seamlessly integrate with NAQ's service layer, allowing it to leverage shared resources and centralized configuration.

```python
from naq.config import NAQConfig
from naq.services import ServiceManager, JobService, SchedulerService, StreamService
from naq.queue.core import Queue as CoreQueue
from typing import Callable, Any, Optional, Union, List
from datetime import datetime, timedelta

class Queue(CoreQueue):
    def __init__(
        self,
        name: str = "naq_default_queue",
        config: Optional[Union[NAQConfig, dict]] = None,
        services: Optional[ServiceManager] = None
    ):
        # The constructor now primarily takes a config object and a ServiceManager.
        # It internally uses these to get the necessary services.
        super().__init__(name=name, config=config, services=services)
        self._job_service: Optional[JobService] = None
        self._scheduler_service: Optional[SchedulerService] = None
        self._stream_service: Optional[StreamService] = None

    async def _get_job_service(self) -> JobService:
        if self._job_service is None:
            self._job_service = await self._services.get_service(JobService)
        return self._job_service

    async def _get_scheduler_service(self) -> SchedulerService:
        if self._scheduler_service is None:
            self._scheduler_service = await self._services.get_service(SchedulerService)
        return self._scheduler_service

    async def _get_stream_service(self) -> StreamService:
        if self._stream_service is None:
            self._stream_service = await self._services.get_service(StreamService)
        return self._stream_service

    async def enqueue(
        self,
        func: Callable,
        *args: Any,
        max_retries: Optional[int] = 0,
        retry_delay: Union[int, float, Callable[[int], Union[int, float]]] = 0,
        depends_on: Optional[Union[str, List[str]]] = None,
        timeout: Optional[int] = None,
        **kwargs: Any
    ):
        job = self._create_job(func, args, max_retries, retry_delay, depends_on, timeout, **kwargs)
        job_service = await self._get_job_service()
        return await job_service.execute_job(job)

    async def enqueue_at(
        self,
        dt: datetime,
        func: Callable,
        *args: Any,
        max_retries: Optional[int] = 0,
        retry_delay: Union[int, float, Callable[[int], Union[int, float]]] = 0,
        depends_on: Optional[Union[str, List[str]]] = None,
        timeout: Optional[int] = None,
        **kwargs: Any
    ):
        job = self._create_job(func, args, max_retries, retry_delay, depends_on, timeout, **kwargs)
        scheduler_service = await self._get_scheduler_service()
        return await scheduler_service.schedule_job(job, dt)

    async def enqueue_in(
        self,
        delta: timedelta,
        func: Callable,
        *args: Any,
        max_retries: Optional[int] = 0,
        retry_delay: Union[int, float, Callable[[int], Union[int, float]]] = 0,
        depends_on: Optional[Union[str, List[str]]] = None,
        timeout: Optional[int] = None,
        **kwargs: Any
    ):
        job = self._create_job(func, args, max_retries, retry_delay, depends_on, timeout, **kwargs)
        scheduler_service = await self._get_scheduler_service()
        return await scheduler_service.schedule_job(job, delta)

    async def schedule(
        self,
        func: Callable,
        *args: Any,
        cron: Optional[str] = None,
        interval: Optional[Union[timedelta, float, int]] = None,
        repeat: Optional[int] = None,
        max_retries: Optional[int] = 0,
        retry_delay: Union[int, float, Callable[[int], Union[int, float]]] = 0,
        timeout: Optional[int] = None,
        **kwargs: Any
    ):
        job = self._create_job(func, args, max_retries, retry_delay, None, timeout, **kwargs)
        scheduler_service = await self._get_scheduler_service()
        return await scheduler_service.schedule_job(job, cron=cron, interval=interval, repeat=repeat)

    async def purge(self) -> int:
        stream_service = await self._get_stream_service()
        return await stream_service.purge_stream(self.name) # Assuming queue name maps to stream name

    async def cancel_scheduled_job(self, job_id: str) -> bool:
        scheduler_service = await self._get_scheduler_service()
        return await scheduler_service.cancel_scheduled_job(job_id)

```

#### Usage Pattern with `ServiceManager`

The recommended way to instantiate and use the `Queue` class is by providing a `ServiceManager` instance. This allows the `Queue` to share and leverage the common services managed by the `ServiceManager`.


```python
import asyncio
from naq.queue import Queue
from naq.services import ServiceManager

async def queue_with_services():
    # Create service configuration
    config = {
        'nats': {
            'url': 'nats://localhost:4222',
            'max_reconnect_attempts': 5,
            'reconnect_delay': 1.0
        }
    }
    
    # Use ServiceManager for lifecycle management
    async with ServiceManager(config) as services:
        # Create Queue with ServiceManager
        queue = Queue(name='example', services=services)
        
        # Use the queue - it will automatically use the services
        job = await queue.enqueue(my_function, arg1, arg2)
        
        # Services are automatically managed and cleaned up
```

### Benefits of Service Layer Integration

1. **Connection Pooling**: Multiple queue instances can share the same NATS connection, reducing resource usage.

2. **Centralized Configuration**: Connection parameters, retry logic, and timeouts are configured in one place.

3. **Automatic Resource Management**: Services are automatically initialized and cleaned up by the ServiceManager.

4. **Dependency Injection**: Services can depend on each other, creating a clean dependency graph.

### Migration from Direct Connection Management

#### Before (Direct Connection Management)

```python
# Old approach - direct connection management
async def old_queue_usage():
    queue = Queue(name='example', nats_url='nats://localhost:4222')
    
    try:
        job = await queue.enqueue(my_function, arg1, arg2)
        # Use the queue
    finally:
        await queue.close()  # Manual cleanup
```

#### After (Service Layer)

```python
# New approach - service layer
async def new_queue_usage():
    config = {'nats': {'url': 'nats://localhost:4222'}}
    
    async with ServiceManager(config) as services:
        queue = Queue(name='example', services=services)
        
        job = await queue.enqueue(my_function, arg1, arg2)
        # No need to manually close - services are automatically managed
```

### Advanced Service Configuration

You can provide detailed configuration for the services used by the queue:

```python
async def advanced_service_config():
    config = {
        'nats': {
            'url': 'nats://localhost:4222',
            'max_reconnect_attempts': 10,
            'reconnect_delay': 2.0,
            'connection_timeout': 30.0,
            'ping_interval': 60,
            'max_outstanding_pings': 3
        },
        'events': {
            'enabled': True,
            'batch_size': 100,
            'flush_interval': 1.0,
            'max_buffer_size': 10000
        }
    }
    
    async with ServiceManager(config) as services:
        queue = Queue(name='production', services=services)
        # Queue will use the configured services
```

### Sharing ServiceManager Across Queues

Multiple queues can share the same ServiceManager for efficient resource usage:

```python
async def shared_services_example():
    config = {'nats': {'url': 'nats://localhost:4222'}}
    
    async with ServiceManager(config) as services:
        # Create multiple queues sharing the same services
        high_priority_queue = Queue(name='high_priority', services=services)
        low_priority_queue = Queue(name='low_priority', services=services)
        
        # Both queues share the same connection and other services
        job1 = await high_priority_queue.enqueue(urgent_task)
        job2 = await low_priority_queue.enqueue(background_task)
```

## Enqueue Functions

These helper functions provide a simpler way to enqueue jobs without needing to manage a `Queue` instance yourself. They are available in both async and sync versions.

### Async Helpers

-   `naq.enqueue()`
-   `naq.enqueue_at()`
-   `naq.enqueue_in()`
-   `naq.schedule()`
-   `naq.purge_queue()`
-   `naq.cancel_scheduled_job()`

### Sync Helpers

For use in synchronous code, `naq` provides sync versions of the enqueue functions. These functions automatically manage an event loop and use a thread-local connection for efficiency.

-   `naq.enqueue_sync()`
-   `naq.enqueue_at_sync()`
-   `naq.enqueue_in_sync()`
-   `naq.schedule_sync()`
-   `naq.purge_queue_sync()`
-   `naq.cancel_scheduled_job_sync()`
-   `naq.close_sync_connections()`