---
title: "Usage Examples"
---

This page provides practical examples for some of `naq`'s key features.

## Example 1: Scheduled and Recurring Jobs

`naq` allows you to schedule jobs to run at a specific time in the future or on a recurring basis using cron expressions.

To run these examples, you need both a worker and the scheduler process running:

```bash
# Terminal 1: Start the scheduler
naq scheduler

# Terminal 2: Start a worker
naq worker scheduled_queue
```

### One-Time Scheduled Job

You can enqueue a job to run after a specific delay or at a precise time.

```python
# schedule_task.py
import asyncio
from datetime import datetime, timedelta
from naq import enqueue_at, enqueue_in

async def send_reminder(user_id, message):
    print(f"Sending reminder to {user_id}: {message}")
    # Add logic to send an email or push notification
    return f"Reminder sent to {user_id}"

async def main():
    # Schedule a job to run in 5 minutes
    run_in_5_min = await enqueue_in(
        send_reminder,
        delay=timedelta(minutes=5),
        user_id="user123",
        message="Your meeting starts in 5 minutes.",
        queue_name="scheduled_queue"
    )
    print(f"Job {run_in_5_min.job_id} scheduled to run in 5 minutes.")

    # Schedule a job to run at a specific time (UTC)
    run_at_time = datetime.utcnow() + timedelta(hours=1)
    run_at = await enqueue_at(
        send_reminder,
        run_at=run_at_time,
        user_id="user456",
        message="Don't forget your 1-hour follow-up.",
        queue_name="scheduled_queue"
    )
    print(f"Job {run_at.job_id} scheduled to run at {run_at_time.isoformat()}.")

if __name__ == "__main__":
    asyncio.run(main())
```

### Recurring Job (Cron)

For tasks that need to run on a regular schedule (e.g., nightly reports, weekly cleanups), you can use the `schedule` function with a cron string.

```python
# recurring_task.py
import asyncio
from naq import schedule

async def generate_nightly_report():
    print("Generating the nightly sales report...")
    # Logic to aggregate data and create a report
    print("Nightly report complete.")
    return "Report generated successfully."

async def main():
    # Schedule the report to run every day at 2:00 AM UTC
    cron_schedule = await schedule(
        generate_nightly_report,
        cron="0 2 * * *",  # Standard cron format
        schedule_id="nightly-sales-report",
        queue_name="scheduled_queue"
    )
    print(f"Cron job '{cron_schedule.schedule_id}' is now active.")

if __name__ == "__main__":
    asyncio.run(main())
```

## Example 2: Automatic Job Retries

`naq` can automatically retry failed jobs with configurable strategies. This is useful for tasks that might fail due to transient issues, like network hiccups.

```python
# retry_task.py
import asyncio
import random
from naq import enqueue

async def flaky_api_call(request_id):
    """
    This function simulates an API call that sometimes fails.
    """
    print(f"Attempting to call API for request {request_id}...")
    if random.random() > 0.5:
        print("API call successful!")
        return "Success"
    else:
        print("API call failed. Will retry...")
        raise ConnectionError("Could not connect to the API")

async def main():
    # Enqueue the job with a retry policy
    job = await enqueue(
        flaky_api_call,
        request_id="abc-123",
        queue_name="default",
        max_retries=3,          # Attempt the job up to 3 more times
        retry_delay=5,          # Wait 5 seconds between retries
        retry_strategy="linear" # Use a fixed delay
    )
    print(f"Enqueued job {job.job_id} with 3 linear retries.")

if __name__ == "__main__":
    asyncio.run(main())
```

## Example 5: Using JobService Directly

The `JobService` provides a centralized way to execute jobs, manage results, and handle job lifecycle events. This is useful when you need more control over job execution beyond simple enqueueing.

```python
# job_service_example.py
import asyncio
from naq.services.base import ServiceConfig
from naq.services.connection import ConnectionService, ConnectionServiceConfig
from naq.services.kv_stores import KVStoreService, KVStoreServiceConfig
from naq.services.events import EventService, EventServiceConfig
from naq.services.jobs import JobService, JobServiceConfig
from naq.models.jobs import Job

async def process_data(data_id):
    """Example job function that processes data."""
    print(f"Processing data with ID: {data_id}")
    await asyncio.sleep(1)  # Simulate processing time
    return f"Processed data {data_id}"

async def main():
    # Initialize services
    connection_config = ConnectionServiceConfig()
    connection_service = ConnectionService(config=ServiceConfig(custom_settings=connection_config.as_dict()))
    await connection_service.initialize()
    
    kv_store_config = KVStoreServiceConfig()
    kv_store_service = KVStoreService(config=ServiceConfig(custom_settings=kv_store_config.as_dict()))
    await kv_store_service.initialize()
    
    event_config = EventServiceConfig()
    event_service = EventService(config=ServiceConfig(custom_settings=event_config.as_dict()), kv_store_service=kv_store_service)
    await event_service.initialize()
    
    # Create JobService with all dependencies
    job_config = JobServiceConfig(
        enable_job_execution=True,
        enable_result_storage=True,
        enable_event_logging=True,
        max_job_execution_time=30
    )
    job_service = JobService(
        config=ServiceConfig(custom_settings=job_config.as_dict()),
        connection_service=connection_service,
        kv_store_service=kv_store_service,
        event_service=event_service
    )
    await job_service.initialize()
    
    # Create a job
    job = Job(
        function=process_data,
        args=("data-123",),
        queue_name="processing",
        max_retries=2,
        retry_delay=1.0
    )
    
    # Execute the job
    print("Executing job...")
    result = await job_service.execute_job(job, "worker-1")
    print(f"Job completed with result: {result.result}")
    print(f"Job status: {result.status}")
    print(f"Execution time: {result.execution_time:.2f} seconds")
    
    # Retrieve the stored result
    stored_result = await job_service.get_result(job.job_id)
    print(f"Retrieved result: {stored_result.result}")
    
    # Get job events
    job_events = await event_service.get_job_events(job.job_id)
    print(f"Job events: {len(job_events)} events logged")
    for event in job_events:
        print(f"  - {event.event_type.value} at {event.timestamp}")
    
    # Cleanup
    await job_service.cleanup()
    await event_service.cleanup()
    await kv_store_service.cleanup()
    await connection_service.cleanup()

if __name__ == "__main__":
    asyncio.run(main())
```

### Handling Job Failures with JobService

The `JobService` provides robust failure handling with automatic retries and detailed error logging.

```python
# job_failure_example.py
import asyncio
from naq.services.base import ServiceConfig
from naq.services.connection import ConnectionService, ConnectionServiceConfig
from naq.services.kv_stores import KVStoreService, KVStoreServiceConfig
from naq.services.events import EventService, EventServiceConfig
from naq.services.jobs import JobService, JobServiceConfig
from naq.models.jobs import Job

async def unreliable_api_call():
    """Example function that might fail."""
    import random
    if random.random() < 0.7:  # 70% chance of failure
        raise ConnectionError("API temporarily unavailable")
    return "API call successful"

async def main():
    # Initialize services (simplified for brevity)
    connection_config = ConnectionServiceConfig()
    connection_service = ConnectionService(config=ServiceConfig(custom_settings=connection_config.as_dict()))
    await connection_service.initialize()
    
    kv_store_config = KVStoreServiceConfig()
    kv_store_service = KVStoreService(config=ServiceConfig(custom_settings=kv_store_config.as_dict()))
    await kv_store_service.initialize()
    
    event_config = EventServiceConfig()
    event_service = EventService(config=ServiceConfig(custom_settings=event_config.as_dict()), kv_store_service=kv_store_service)
    await event_service.initialize()
    
    # Create JobService with retry configuration
    job_config = JobServiceConfig(
        enable_job_execution=True,
        enable_result_storage=True,
        enable_event_logging=True
    )
    job_service = JobService(
        config=ServiceConfig(custom_settings=job_config.as_dict()),
        connection_service=connection_service,
        kv_store_service=kv_store_service,
        event_service=event_service
    )
    await job_service.initialize()
    
    # Create a job with retry configuration
    job = Job(
        function=unreliable_api_call,
        queue_name="api_calls",
        max_retries=3,  # Retry up to 3 times
        retry_delay=2.0  # Wait 2 seconds between retries
    )
    
    # Execute the job
    try:
        result = await job_service.execute_job(job, "worker-1")
        print(f"Job succeeded: {result.result}")
    except Exception as e:
        print(f"Job failed after all retries: {e}")
    
    # Check the stored result regardless of success/failure
    stored_result = await job_service.get_result(job.job_id)
    if stored_result:
        print(f"Final job status: {stored_result.status}")
        if stored_result.status == "failed":
            print(f"Error: {stored_result.error}")
            print(f"Traceback: {stored_result.traceback}")
    
    # Get failure events
    job_events = await event_service.get_job_events(job.job_id)
    failure_events = [e for e in job_events if e.event_type.value == "failed"]
    retry_events = [e for e in job_events if e.event_type.value == "retry_scheduled"]
    
    print(f"Failure events: {len(failure_events)}")
    print(f"Retry events: {len(retry_events)}")
    
    # Cleanup
    await job_service.cleanup()
    await event_service.cleanup()
    await kv_store_service.cleanup()
    await connection_service.cleanup()

if __name__ == "__main__":
    asyncio.run(main())
```

### Custom JobService Configuration

You can customize the `JobService` behavior through its configuration.

```python
# custom_job_config.py
import asyncio
from naq.services.base import ServiceConfig
from naq.services.connection import ConnectionService, ConnectionServiceConfig
from naq.services.kv_stores import KVStoreService, KVStoreServiceConfig
from naq.services.events import EventService, EventServiceConfig
from naq.services.jobs import JobService, JobServiceConfig
from naq.models.jobs import Job

async def long_running_task():
    """Example long-running task."""
    await asyncio.sleep(5)  # Simulate 5 seconds of work
    return "Task completed"

async def main():
    # Initialize services (simplified for brevity)
    connection_config = ConnectionServiceConfig()
    connection_service = ConnectionService(config=ServiceConfig(custom_settings=connection_config.as_dict()))
    await connection_service.initialize()
    
    kv_store_config = KVStoreServiceConfig()
    kv_store_service = KVStoreService(config=ServiceConfig(custom_settings=kv_store_config.as_dict()))
    await kv_store_service.initialize()
    
    event_config = EventServiceConfig()
    event_service = EventService(config=ServiceConfig(custom_settings=event_config.as_dict()), kv_store_service=kv_store_service)
    await event_service.initialize()
    
    # Create JobService with custom configuration
    job_config = JobServiceConfig(
        enable_job_execution=True,
        enable_result_storage=True,
        enable_event_logging=True,
        max_job_execution_time=10,  # Maximum 10 seconds per job
        default_result_ttl=3600,    # Store results for 1 hour
        results_bucket_name="custom_results",
        auto_create_buckets=True
    )
    job_service = JobService(
        config=ServiceConfig(custom_settings=job_config.as_dict()),
        connection_service=connection_service,
        kv_store_service=kv_store_service,
        event_service=event_service
    )
    await job_service.initialize()
    
    # Check configuration
    print(f"Job execution enabled: {job_service.is_job_execution_enabled}")
    print(f"Result storage enabled: {job_service.is_result_storage_enabled}")
    print(f"Event logging enabled: {job_service.is_event_logging_enabled}")
    print(f"Max execution time: {job_service.job_config.max_job_execution_time} seconds")
    
    # Create and execute a job
    job = Job(
        function=long_running_task,
        queue_name="long_tasks",
        max_retries=1
    )
    
    try:
        result = await job_service.execute_job(job, "worker-1")
        print(f"Job completed: {result.result}")
    except Exception as e:
        print(f"Job failed: {e}")
    
    # Cleanup
    await job_service.cleanup()
    await event_service.cleanup()
    await kv_store_service.cleanup()
    await connection_service.cleanup()

if __name__ == "__main__":
    asyncio.run(main())
```

### Using JobService with Minimal Dependencies

If you only need basic job execution without all services, you can initialize `JobService` with minimal dependencies.

```python
# minimal_job_service.py
import asyncio
from naq.services.base import ServiceConfig
from naq.services.connection import ConnectionService, ConnectionServiceConfig
from naq.services.jobs import JobService, JobServiceConfig
from naq.models.jobs import Job

async def simple_task(x, y):
    """Simple task that adds two numbers."""
    return x + y

async def main():
    # Initialize only the connection service
    connection_config = ConnectionServiceConfig()
    connection_service = ConnectionService(config=ServiceConfig(custom_settings=connection_config.as_dict()))
    await connection_service.initialize()
    
    # Create JobService with minimal configuration
    job_config = JobServiceConfig(
        enable_job_execution=True,
        enable_result_storage=False,  # Disable result storage
        enable_event_logging=False    # Disable event logging
    )
    job_service = JobService(
        config=ServiceConfig(custom_settings=job_config.as_dict()),
        connection_service=connection_service
        # KVStoreService and EventService will be created automatically
    )
    await job_service.initialize()
    
    # Create and execute a job
    job = Job(
        function=simple_task,
        args=(5, 7),
        queue_name="simple_tasks"
    )
    
    result = await job_service.execute_job(job, "worker-1")
    print(f"Job result: {result.result}")
    
    # Result storage is disabled, so get_result will return None
    stored_result = await job_service.get_result(job.job_id)
    print(f"Stored result: {stored_result}")  # Will be None
    
    # Cleanup
    await job_service.cleanup()
    await connection_service.cleanup()

if __name__ == "__main__":
    asyncio.run(main())
```

::: {.callout-note}
**Retry Strategies**

`naq` supports both `linear` (fixed delay) and `exponential` backoff strategies for retries. You can also provide a list of integers to `retry_delay` for a custom delay sequence.
:::

## Example 3: Job Dependencies

You can create workflows by making jobs dependent on the successful completion of others. The dependent job will only run after its dependencies have finished.

```python
# dependency_workflow.py
import asyncio
from naq import enqueue

async def download_data(source_url):
    print(f"Downloading data from {source_url}...")
    await asyncio.sleep(2)  # Simulate download
    file_path = f"/tmp/{source_url.split('/')[-1]}.csv"
    print(f"Data downloaded to {file_path}")
    return file_path

async def process_data(file_path):
    print(f"Processing data from {file_path}...")
    await asyncio.sleep(3)  # Simulate processing
    result_path = f"{file_path}.processed"
    print(f"Data processed and saved to {result_path}")
    return result_path

async def upload_results(result_path):
    print(f"Uploading {result_path} to cloud storage...")
    await asyncio.sleep(1)  # Simulate upload
    print("Upload complete.")
    return "Workflow finished successfully."

async def main():
    # Step 1: Download data
    download_job = await enqueue(download_data, source_url="http://example.com/data")

    # Step 2: Process data (depends on download)
    process_job = await enqueue(
        process_data,
        file_path=download_job, # Pass the result of the dependency
        depends_on=[download_job]
    )

    # Step 3: Upload results (depends on processing)
    upload_job = await enqueue(
        upload_results,
        result_path=process_job,
        depends_on=[process_job]
    )

    print(f"Workflow started. Final job: {upload_job.job_id}")

if __name__ == "__main__":
    asyncio.run(main())
```

## Example 4: Using Multiple Queues

You can use different queues to prioritize jobs or to dedicate workers to specific types of tasks.

Start workers for each queue in separate terminals:

```bash
# Terminal 1: High-priority worker
naq worker notifications --log-level info

# Terminal 2: Low-priority worker
naq worker data_processing --log-level info
```

Now, you can enqueue jobs to the appropriate queues.

```python
# multi_queue_example.py
import asyncio
from naq import enqueue

async def send_email(address, subject, body):
    print(f"Sending high-priority email to {address}...")
    await asyncio.sleep(0.5)
    return "Email sent."

async def transcode_video(video_id):
    print(f"Starting low-priority video transcoding for {video_id}...")
    await asyncio.sleep(10) # Simulate long-running task
    return "Video transcoded."

async def main():
    # Enqueue a high-priority job
    email_job = await enqueue(
        send_email,
        address="user@example.com",
        subject="Your order",
        body="...",
        queue_name="notifications" # Target the 'notifications' queue
    )
    print(f"Enqueued notification job {email_job.job_id}")

    # Enqueue a low-priority job
    video_job = await enqueue(
        transcode_video,
        video_id=12345,
        queue_name="data_processing" # Target the 'data_processing' queue
    )
    print(f"Enqueued data processing job {video_job.job_id}")

if __name__ == "__main__":
    asyncio.run(main())