---
title: "Usage Examples"
---

This page provides practical examples for some of `naq`'s key features.

## Example 1: Scheduled and Recurring Jobs

`naq` allows you to schedule jobs to run at a specific time in the future or on a recurring basis using cron expressions.

To run these examples, you need both a worker and the scheduler process running:

```bash
# Terminal 1: Start the scheduler
naq scheduler

# Terminal 2: Start a worker
naq worker scheduled_queue
```

### One-Time Scheduled Job

You can enqueue a job to run after a specific delay or at a precise time.

```python
# schedule_task.py
import asyncio
from datetime import datetime, timedelta
from naq import enqueue_at, enqueue_in

async def send_reminder(user_id, message):
    print(f"Sending reminder to {user_id}: {message}")
    # Add logic to send an email or push notification
    return f"Reminder sent to {user_id}"

async def main():
    # Schedule a job to run in 5 minutes
    run_in_5_min = await enqueue_in(
        send_reminder,
        delay=timedelta(minutes=5),
        user_id="user123",
        message="Your meeting starts in 5 minutes.",
        queue_name="scheduled_queue"
    )
    print(f"Job {run_in_5_min.job_id} scheduled to run in 5 minutes.")

    # Schedule a job to run at a specific time (UTC)
    run_at_time = datetime.utcnow() + timedelta(hours=1)
    run_at = await enqueue_at(
        send_reminder,
        run_at=run_at_time,
        user_id="user456",
        message="Don't forget your 1-hour follow-up.",
        queue_name="scheduled_queue"
    )
    print(f"Job {run_at.job_id} scheduled to run at {run_at_time.isoformat()}.")

if __name__ == "__main__":
    asyncio.run(main())
```

### Recurring Job (Cron)

For tasks that need to run on a regular schedule (e.g., nightly reports, weekly cleanups), you can use the `schedule` function with a cron string.

```python
# recurring_task.py
import asyncio
from naq import schedule

async def generate_nightly_report():
    print("Generating the nightly sales report...")
    # Logic to aggregate data and create a report
    print("Nightly report complete.")
    return "Report generated successfully."

async def main():
    # Schedule the report to run every day at 2:00 AM UTC
    cron_schedule = await schedule(
        generate_nightly_report,
        cron="0 2 * * *",  # Standard cron format
        schedule_id="nightly-sales-report",
        queue_name="scheduled_queue"
    )
    print(f"Cron job '{cron_schedule.schedule_id}' is now active.")

if __name__ == "__main__":
    asyncio.run(main())
```

## Example 2: Automatic Job Retries

`naq` can automatically retry failed jobs with configurable strategies. This is useful for tasks that might fail due to transient issues, like network hiccups.

```python
# retry_task.py
import asyncio
import random
from naq import enqueue

async def flaky_api_call(request_id):
    """
    This function simulates an API call that sometimes fails.
    """
    print(f"Attempting to call API for request {request_id}...")
    if random.random() > 0.5:
        print("API call successful!")
        return "Success"
    else:
        print("API call failed. Will retry...")
        raise ConnectionError("Could not connect to the API")

async def main():
    # Enqueue the job with a retry policy
    job = await enqueue(
        flaky_api_call,
        request_id="abc-123",
        queue_name="default",
        max_retries=3,          # Attempt the job up to 3 more times
        retry_delay=5,          # Wait 5 seconds between retries
        retry_strategy="linear" # Use a fixed delay
    )
    print(f"Enqueued job {job.job_id} with 3 linear retries.")

if __name__ == "__main__":
    asyncio.run(main())
```

::: {.callout-note}
**Retry Strategies**

`naq` supports both `linear` (fixed delay) and `exponential` backoff strategies for retries. You can also provide a list of integers to `retry_delay` for a custom delay sequence.
:::

## Example 3: Job Dependencies

You can create workflows by making jobs dependent on the successful completion of others. The dependent job will only run after its dependencies have finished.

```python
# dependency_workflow.py
import asyncio
from naq import enqueue

async def download_data(source_url):
    print(f"Downloading data from {source_url}...")
    await asyncio.sleep(2)  # Simulate download
    file_path = f"/tmp/{source_url.split('/')[-1]}.csv"
    print(f"Data downloaded to {file_path}")
    return file_path

async def process_data(file_path):
    print(f"Processing data from {file_path}...")
    await asyncio.sleep(3)  # Simulate processing
    result_path = f"{file_path}.processed"
    print(f"Data processed and saved to {result_path}")
    return result_path

async def upload_results(result_path):
    print(f"Uploading {result_path} to cloud storage...")
    await asyncio.sleep(1)  # Simulate upload
    print("Upload complete.")
    return "Workflow finished successfully."

async def main():
    # Step 1: Download data
    download_job = await enqueue(download_data, source_url="http://example.com/data")

    # Step 2: Process data (depends on download)
    process_job = await enqueue(
        process_data,
        file_path=download_job, # Pass the result of the dependency
        depends_on=[download_job]
    )

    # Step 3: Upload results (depends on processing)
    upload_job = await enqueue(
        upload_results,
        result_path=process_job,
        depends_on=[process_job]
    )

    print(f"Workflow started. Final job: {upload_job.job_id}")

if __name__ == "__main__":
    asyncio.run(main())
```

## Example 4: Using Multiple Queues

You can use different queues to prioritize jobs or to dedicate workers to specific types of tasks.

Start workers for each queue in separate terminals:

```bash
# Terminal 1: High-priority worker
naq worker notifications --log-level info

# Terminal 2: Low-priority worker
naq worker data_processing --log-level info
```

Now, you can enqueue jobs to the appropriate queues.

```python
# multi_queue_example.py
import asyncio
from naq import enqueue

async def send_email(address, subject, body):
    print(f"Sending high-priority email to {address}...")
    await asyncio.sleep(0.5)
    return "Email sent."

async def transcode_video(video_id):
    print(f"Starting low-priority video transcoding for {video_id}...")
    await asyncio.sleep(10) # Simulate long-running task
    return "Video transcoded."

async def main():
    # Enqueue a high-priority job
    email_job = await enqueue(
        send_email,
        address="user@example.com",
        subject="Your order",
        body="...",
        queue_name="notifications" # Target the 'notifications' queue
    )
    print(f"Enqueued notification job {email_job.job_id}")

    # Enqueue a low-priority job
    video_job = await enqueue(
        transcode_video,
        video_id=12345,
        queue_name="data_processing" # Target the 'data_processing' queue
    )
    print(f"Enqueued data processing job {video_job.job_id}")

if __name__ == "__main__":
    asyncio.run(main())

## Example 5: Custom Event Handlers

The event system allows you to build powerful, reactive workflows by responding to job lifecycle events. This example demonstrates how to create a custom event handler that performs an action when a job of a certain type fails.

To run this example, you will need:
1. A NATS server running.
2. Event logging enabled (`NAQ_EVENTS_ENABLED=true`).
3. A worker running to process jobs.
4. This script running to handle events.

```python
# custom_event_handler.py
import asyncio
import os
from naq import enqueue
from naq.events import AsyncJobEventProcessor, NATSJobEventStorage, JobEventType, JobEvent

# Ensure event logging is enabled for this example to work
os.environ["NAQ_EVENTS_ENABLED"] = "true"

# A task that might fail
async def flaky_task(task_id: str):
    print(f"[Task {task_id}] Starting risky operation...")
    await asyncio.sleep(1)
    if task_id == "fail-on-purpose":
        raise ValueError("This task was designed to fail!")
    print(f"[Task {task_id}] Operation completed successfully.")
    return f"Task {task_id} finished."

# Our custom event handler
async def on_job_failure(event: JobEvent):
    """
    This function is called whenever a job fails.
    """
    print("=" * 50)
    print("ðŸš¨ CUSTOM HANDLER TRIGGERED: JOB FAILURE DETECTED! ðŸš¨")
    print("=" * 50)
    print(f"  Job ID:       {event.job_id}")
    print(f"  Error Type:    {event.error_type}")
    print(f"  Error Message: {event.error_message}")
    print(f"  Queue:         {event.queue_name}")
    print(f"  Worker:        {event.worker_id}")
    print(f"  Timestamp:     {event.timestamp}")
    print("=" * 50)
    # Here you could add more complex logic, like:
    # - Sending a notification to Slack or PagerDuty
    # - Writing to a database for analytics
    # - Triggering a recovery workflow

async def main():
    # --- Setup ---
    print("Setting up event processor and enqueuing jobs...")

    # 1. Create the event storage and processor
    storage = NATSJobEventStorage()
    processor = AsyncJobEventProcessor(storage)

    # 2. Register our custom handler for FAILED events
    processor.add_handler(JobEventType.FAILED, on_job_failure)
    
    # 3. Start the processor in the background
    # It will begin listening for events immediately.
    processor_task = asyncio.create_task(processor.start())

    # Give the processor a moment to start up
    await asyncio.sleep(2)

    # --- Enqueue Jobs ---
    print("\nEnqueuing a successful job and a job that will fail...")
    # Enqueue a job that will succeed
    await enqueue(flaky_task, "success-1", queue_name="example_queue")
    
    # Enqueue a job that is designed to fail
    await enqueue(flaky_task, "fail-on-purpose", queue_name="example_queue")

    print("Jobs enqueued. Make sure a worker is running to process them.")
    print("The custom handler will be called when the 'fail-on-purpose' job fails.")

    # --- Keep Running ---
    try:
        # Keep the script running to allow the processor to handle events
        print("\nWaiting for events... Press Ctrl+C to exit.")
        while True:
            await asyncio.sleep(1)
    except KeyboardInterrupt:
        print("\nShutting down...")
    finally:
        # Ensure the processor is stopped cleanly
        await processor.stop()
        await processor_task # Wait for the processor task to finish

if __name__ == "__main__":
    asyncio.run(main())
```

To see this in action:

1.  **Start a worker** in one terminal:
    ```bash
    NAQ_EVENTS_ENABLED=true naq worker example_queue
    ```

2.  **Run the custom handler script** in another terminal:
    ```bash
    python custom_event_handler.py
    ```

You will see the handler script enqueue two jobs. The worker will pick them up. When the `fail-on-purpose` job fails, you will see the custom "ðŸš¨ CUSTOM HANDLER TRIGGERED" message printed by the `on_job_failure` function in the handler script's terminal, demonstrating the reactive power of the event system.

## Example 6: Connection Management with Context Managers

The new connection management system provides context managers for automatic resource management. These examples demonstrate how to use the different context managers available in `naq`.

### Basic Connection Management

```python
# connection_example.py
import asyncio
from naq.connection import nats_connection, nats_jetstream, nats_kv_store

async def basic_connection_example():
    """Example of basic NATS connection management."""
    
    # Simple connection for publishing messages
    async with nats_connection() as conn:
        await conn.publish("hello.world", b"Hello from context manager!")
        print("Message published successfully")
    
    # Connection is automatically closed when exiting the context

async def jetstream_example():
    """Example of JetStream context management."""
    
    # Combined connection and JetStream context
    async with nats_jetstream() as (conn, js):
        # Create a stream
        await js.add_stream(
            name="example_stream",
            subjects=["example.*"]
        )
        print("Stream created successfully")
        
        # Publish a message to the stream
        await js.publish("example.data", b"Stream data")
        print("Message published to stream")
    
    # Both connection and JetStream context are automatically closed

async def kv_store_example():
    """Example of KeyValue store management."""
    
    # KeyValue store context
    async with nats_kv_store("example_bucket") as kv:
        # Store a value
        await kv.put("user:123", '{"name": "Alice", "role": "admin"}')
        print("Value stored in KV bucket")
        
        # Retrieve the value
        result = await kv.get("user:123")
        print(f"Retrieved value: {result.value.decode()}")
        
        # List all keys
        keys = await kv.keys()
        print(f"Keys in bucket: {list(keys)}")
    
    # KV store context is automatically closed

if __name__ == "__main__":
    asyncio.run(basic_connection_example())
    asyncio.run(jetstream_example())
    asyncio.run(kv_store_example())
```

### Connection Management with Decorators

```python
# decorator_example.py
import asyncio
from naq.connection import with_nats_connection, with_jetstream_context

@with_nats_connection()
async def publish_message(conn, subject: str, message: str):
    """Publish a message using the injected connection."""
    await conn.publish(subject, message.encode())
    print(f"Message published to {subject}")

@with_jetstream_context()
async def create_consumer(js, stream_name: str, consumer_name: str):
    """Create a consumer using the injected JetStream context."""
    await js.add_consumer(
        stream_name,
        durable_name=consumer_name,
        ack_policy="explicit"
    )
    print(f"Consumer '{consumer_name}' created for stream '{stream_name}'")

async def decorator_example():
    """Example of using connection decorators."""
    
    # Publish a message - connection is automatically injected
    await publish_message("decorator.test", "Hello from decorator!")
    
    # Create a consumer - JetStream context is automatically injected
    await create_consumer("example_stream", "example_consumer")

if __name__ == "__main__":
    asyncio.run(decorator_example())
```

### Connection Testing and Monitoring

```python
# monitoring_example.py
import asyncio
from naq.connection import (
    test_nats_connection,
    wait_for_nats_connection,
    connection_monitor,
    nats_connection
)

async def monitoring_example():
    """Example of connection testing and monitoring."""
    
    # Test if connection is available
    is_healthy = await test_nats_connection()
    print(f"Connection healthy: {is_healthy}")
    
    # Wait for connection to be ready (useful in production)
    is_ready = await wait_for_nats_connection(timeout=10)
    print(f"Connection ready: {is_ready}")
    
    # Use connection and monitor metrics
    async with nats_connection() as conn:
        # Do some work
        await conn.publish("monitoring.test", b"Test message")
        
        # Check connection metrics
        metrics = connection_monitor.metrics
        print(f"Total connections: {metrics.total_connections}")
        print(f"Active connections: {metrics.active_connections}")
        print(f"Failed connections: {metrics.failed_connections}")
        print(f"Average connection time: {metrics.average_connection_time:.2f}s")

if __name__ == "__main__":
    asyncio.run(monitoring_example())
```

### Custom Configuration

```python
# custom_config_example.py
import asyncio
from naq.connection import Config, nats_connection, nats_jetstream

async def custom_config_example():
    """Example of using custom connection configuration."""
    
    # Create custom configuration
    config = Config(
        servers=["nats://localhost:4222", "nats://localhost:4223"],
        client_name="my-custom-app",
        max_reconnect_attempts=10,
        reconnect_time_wait=5.0
    )
    
    # Use custom configuration
    async with nats_connection(config) as conn:
        await conn.publish("config.test", b"Using custom config")
        print("Message published with custom configuration")
    
    # Use with JetStream
    async with nats_jetstream(config) as (conn, js):
        await js.add_stream(
            name="config_stream",
            subjects=["config.*"]
        )
        print("Stream created with custom configuration")

if __name__ == "__main__":
    asyncio.run(custom_config_example())
```

### Migration from Legacy Patterns

```python
# migration_example.py
import asyncio
from naq.connection import (
    # Legacy functions (still work)
    get_nats_connection,
    get_jetstream_context,
    close_nats_connection,
    
    # New context managers
    nats_connection,
    nats_jetstream
)

async def legacy_pattern():
    """Old way of managing connections."""
    print("=== Legacy Pattern ===")
    
    nc = await get_nats_connection()
    try:
        js = await get_jetstream_context(nc)
        await js.add_stream(name="legacy_stream", subjects=["legacy.*"])
        print("Stream created using legacy pattern")
    finally:
        await close_nats_connection(nc)
        print("Connection closed manually")

async def modern_pattern():
    """New way of managing connections."""
    print("=== Modern Pattern ===")
    
    async with nats_jetstream() as (conn, js):
        await js.add_stream(name="modern_stream", subjects=["modern.*"])
        print("Stream created using modern pattern")
    
    print("Connection closed automatically")

async def migration_example():
    """Example showing both patterns."""
    
    # Legacy pattern (still works)
    await legacy_pattern()
    
    # Modern pattern (recommended)
    await modern_pattern()

if __name__ == "__main__":
    asyncio.run(migration_example())
```

### Production-Ready Connection Management

```python
# production_example.py
import asyncio
from naq.connection import (
    nats_connection,
    nats_jetstream,
    test_nats_connection,
    wait_for_nats_connection,
    connection_monitor,
    Config
)

class ProductionService:
    """Example of production-ready connection management."""
    
    def __init__(self, config: Config):
        self.config = config
    
    async def ensure_connection(self):
        """Ensure connection is available before proceeding."""
        if not await test_nats_connection(self.config):
            print("Waiting for connection...")
            if not await wait_for_nats_connection(self.config, timeout=30):
                raise ConnectionError("Failed to establish connection")
    
    async def process_data(self):
        """Process data with robust connection management."""
        
        # Ensure connection is available
        await self.ensure_connection()
        
        try:
            # Use JetStream with automatic connection management
            async with nats_jetstream(self.config) as (conn, js):
                # Create stream if it doesn't exist
                try:
                    await js.add_stream(
                        name="production_stream",
                        subjects=["production.*"]
                    )
                except Exception as e:
                    print(f"Stream might already exist: {e}")
                
                # Process and publish data
                for i in range(5):
                    data = f"Production data {i}"
                    await js.publish("production.data", data.encode())
                    print(f"Published: {data}")
                
                # Log connection metrics
                metrics = connection_monitor.metrics
                print(f"Connection metrics - Total: {metrics.total_connections}, "
                      f"Active: {metrics.active_connections}, "
                      f"Failed: {metrics.failed_connections}")
        
        except Exception as e:
            print(f"Error in production processing: {e}")
            raise

async def production_example():
    """Example of production-ready connection management."""
    
    # Production configuration
    config = Config(
        servers=["nats://localhost:4222"],
        client_name="production-service",
        max_reconnect_attempts=10,
        reconnect_time_wait=5.0
    )
    
    # Create and run production service
    service = ProductionService(config)
    await service.process_data()

if __name__ == "__main__":
    asyncio.run(production_example())
```

To run these examples:

1.  **Start a NATS server** in one terminal:
    ```bash
    nats-server
    ```

2.  **Run the examples** in another terminal:
    ```bash
    python connection_example.py
    python decorator_example.py
    python monitoring_example.py
    python custom_config_example.py
    python migration_example.py
    python production_example.py
    ```

These examples demonstrate the various ways to use the new connection management system in `naq`, from basic usage to production-ready patterns. The context managers provide automatic resource management, while the decorators offer a convenient way to inject connections into functions.