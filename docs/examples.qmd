---
title: "Usage Examples"
---

This page provides practical examples for some of `naq`'s key features.

## Example 1: Scheduled and Recurring Jobs

`naq` allows you to schedule jobs to run at a specific time in the future or on a recurring basis using cron expressions.

To run these examples, you need both a worker and the scheduler process running:

```bash
# Terminal 1: Start the scheduler
naq scheduler

# Terminal 2: Start a worker
naq worker scheduled_queue
```

### One-Time Scheduled Job

You can enqueue a job to run after a specific delay or at a precise time.

```python
# schedule_task.py
import asyncio
from datetime import datetime, timedelta
from naq import enqueue_at, enqueue_in

async def send_reminder(user_id, message):
    print(f"Sending reminder to {user_id}: {message}")
    # Add logic to send an email or push notification
    return f"Reminder sent to {user_id}"

async def main():
    # Schedule a job to run in 5 minutes
    run_in_5_min = await enqueue_in(
        send_reminder,
        delay=timedelta(minutes=5),
        user_id="user123",
        message="Your meeting starts in 5 minutes.",
        queue_name="scheduled_queue"
    )
    print(f"Job {run_in_5_min.job_id} scheduled to run in 5 minutes.")

    # Schedule a job to run at a specific time (UTC)
    run_at_time = datetime.utcnow() + timedelta(hours=1)
    run_at = await enqueue_at(
        send_reminder,
        run_at=run_at_time,
        user_id="user456",
        message="Don't forget your 1-hour follow-up.",
        queue_name="scheduled_queue"
    )
    print(f"Job {run_at.job_id} scheduled to run at {run_at_time.isoformat()}.")

if __name__ == "__main__":
    asyncio.run(main())
```

### Recurring Job (Cron)

For tasks that need to run on a regular schedule (e.g., nightly reports, weekly cleanups), you can use the `schedule` function with a cron string.

```python
# recurring_task.py
import asyncio
from naq import schedule

async def generate_nightly_report():
    print("Generating the nightly sales report...")
    # Logic to aggregate data and create a report
    print("Nightly report complete.")
    return "Report generated successfully."

async def main():
    # Schedule the report to run every day at 2:00 AM UTC
    cron_schedule = await schedule(
        generate_nightly_report,
        cron="0 2 * * *",  # Standard cron format
        schedule_id="nightly-sales-report",
        queue_name="scheduled_queue"
    )
    print(f"Cron job '{cron_schedule.schedule_id}' is now active.")

if __name__ == "__main__":
    asyncio.run(main())
```

## Example 2: Automatic Job Retries

`naq` can automatically retry failed jobs with configurable strategies. This is useful for tasks that might fail due to transient issues, like network hiccups.

```python
# retry_task.py
import asyncio
import random
from naq import enqueue

async def flaky_api_call(request_id):
    """
    This function simulates an API call that sometimes fails.
    """
    print(f"Attempting to call API for request {request_id}...")
    if random.random() > 0.5:
        print("API call successful!")
        return "Success"
    else:
        print("API call failed. Will retry...")
        raise ConnectionError("Could not connect to the API")

async def main():
    # Enqueue the job with a retry policy
    job = await enqueue(
        flaky_api_call,
        request_id="abc-123",
        queue_name="default",
        max_retries=3,          # Attempt the job up to 3 more times
        retry_delay=5,          # Wait 5 seconds between retries
        retry_strategy="linear" # Use a fixed delay
    )
    print(f"Enqueued job {job.job_id} with 3 linear retries.")

if __name__ == "__main__":
    asyncio.run(main())
```

::: {.callout-note}
**Retry Strategies**

`naq` supports both `linear` (fixed delay) and `exponential` backoff strategies for retries. You can also provide a list of integers to `retry_delay` for a custom delay sequence.
:::

## Example 3: Job Dependencies

You can create workflows by making jobs dependent on the successful completion of others. The dependent job will only run after its dependencies have finished.

```python
# dependency_workflow.py
import asyncio
from naq import enqueue

async def download_data(source_url):
    print(f"Downloading data from {source_url}...")
    await asyncio.sleep(2)  # Simulate download
    file_path = f"/tmp/{source_url.split('/')[-1]}.csv"
    print(f"Data downloaded to {file_path}")
    return file_path

async def process_data(file_path):
    print(f"Processing data from {file_path}...")
    await asyncio.sleep(3)  # Simulate processing
    result_path = f"{file_path}.processed"
    print(f"Data processed and saved to {result_path}")
    return result_path

async def upload_results(result_path):
    print(f"Uploading {result_path} to cloud storage...")
    await asyncio.sleep(1)  # Simulate upload
    print("Upload complete.")
    return "Workflow finished successfully."

async def main():
    # Step 1: Download data
    download_job = await enqueue(download_data, source_url="http://example.com/data")

    # Step 2: Process data (depends on download)
    process_job = await enqueue(
        process_data,
        file_path=download_job, # Pass the result of the dependency
        depends_on=[download_job]
    )

    # Step 3: Upload results (depends on processing)
    upload_job = await enqueue(
        upload_results,
        result_path=process_job,
        depends_on=[process_job]
    )

    print(f"Workflow started. Final job: {upload_job.job_id}")

if __name__ == "__main__":
    asyncio.run(main())
```

## Example 4: Using Multiple Queues

You can use different queues to prioritize jobs or to dedicate workers to specific types of tasks.

Start workers for each queue in separate terminals:

```bash
# Terminal 1: High-priority worker
naq worker notifications --log-level info

# Terminal 2: Low-priority worker
naq worker data_processing --log-level info
```

Now, you can enqueue jobs to the appropriate queues.

```python
# multi_queue_example.py
import asyncio
from naq import enqueue

async def send_email(address, subject, body):
    print(f"Sending high-priority email to {address}...")
    await asyncio.sleep(0.5)
    return "Email sent."

async def transcode_video(video_id):
    print(f"Starting low-priority video transcoding for {video_id}...")
    await asyncio.sleep(10) # Simulate long-running task
    return "Video transcoded."

async def main():
    # Enqueue a high-priority job
    email_job = await enqueue(
        send_email,
        address="user@example.com",
        subject="Your order",
        body="...",
        queue_name="notifications" # Target the 'notifications' queue
    )
    print(f"Enqueued notification job {email_job.job_id}")

    # Enqueue a low-priority job
    video_job = await enqueue(
        transcode_video,
        video_id=12345,
        queue_name="data_processing" # Target the 'data_processing' queue
    )
    print(f"Enqueued data processing job {video_job.job_id}")

if __name__ == "__main__":
    asyncio.run(main())
```

## Example 3: Event Monitoring and Analytics

NAQ's event-driven architecture provides comprehensive monitoring and analytics capabilities. This example shows how to leverage events for system observability and custom business logic.

### Real-time Event Monitoring

Monitor job events programmatically for custom processing:

```python
# event_monitor.py
import asyncio
from naq.events import AsyncJobEventProcessor, JobEventType

async def monitor_jobs():
    """Monitor job events and implement custom logic."""
    processor = AsyncJobEventProcessor()
    
    # Global handler for all events
    def log_all_events(event):
        print(f"[{event.timestamp}] {event.event_type}: {event.job_id}")
    
    # Specific handler for failed jobs
    def handle_failures(event):
        print(f"‚ùå Job {event.job_id} failed: {event.error_message}")
        # Send alert, create ticket, etc.
        
    # Performance monitoring handler  
    def track_performance(event):
        if event.duration_ms:
            if event.duration_ms > 10000:  # > 10 seconds
                print(f"‚ö†Ô∏è  Slow job detected: {event.job_id} took {event.duration_ms}ms")
    
    # Register event handlers
    processor.add_global_handler(log_all_events)
    processor.add_handler(JobEventType.FAILED, handle_failures)
    processor.add_handler(JobEventType.COMPLETED, track_performance)
    
    # Start monitoring
    await processor.start()
    
    try:
        print("üîç Monitoring events... (Press Ctrl+C to stop)")
        # Stream events with filtering
        async for event in processor.stream_job_events():
            # Events are processed by registered handlers
            pass
    except KeyboardInterrupt:
        print("\nüëã Stopping event monitor")
    finally:
        await processor.stop()

if __name__ == "__main__":
    asyncio.run(monitor_jobs())
```

### Worker Status Monitoring

Monitor worker health and performance:

```python
# worker_monitor.py
import asyncio
from naq.events import AsyncJobEventProcessor
from collections import defaultdict
from datetime import datetime

class WorkerMonitor:
    def __init__(self):
        self.worker_stats = defaultdict(lambda: {
            'jobs_processed': 0,
            'last_heartbeat': None,
            'status': 'unknown',
            'errors': 0
        })
    
    def handle_worker_event(self, event):
        """Handle worker-specific events."""
        if not (hasattr(event, 'details') and event.details and 
               event.details.get('event_category') == 'worker'):
            return
            
        worker_id = event.worker_id
        stats = self.worker_stats[worker_id]
        
        if 'worker_started' in event.event_type.value:
            stats['status'] = 'active'
            print(f"üü¢ Worker {worker_id} started")
            
        elif 'worker_stopped' in event.event_type.value:
            stats['status'] = 'stopped'
            print(f"üî¥ Worker {worker_id} stopped")
            
        elif 'worker_heartbeat' in event.event_type.value:
            stats['last_heartbeat'] = datetime.now()
            if event.details.get('active_jobs') is not None:
                active = event.details['active_jobs']
                limit = event.details.get('concurrency_limit', 0)
                print(f"üíì {worker_id}: {active}/{limit} jobs active")
                
        elif 'worker_error' in event.event_type.value:
            stats['errors'] += 1
            print(f"‚ö†Ô∏è  Worker {worker_id} error: {event.error_message}")
    
    def handle_job_event(self, event):
        """Handle job events for worker tracking."""
        if event.event_type == JobEventType.COMPLETED and event.worker_id:
            self.worker_stats[event.worker_id]['jobs_processed'] += 1
    
    def print_stats(self):
        """Print current worker statistics."""
        print("\nüìä Worker Statistics:")
        print("-" * 60)
        for worker_id, stats in self.worker_stats.items():
            print(f"Worker: {worker_id}")
            print(f"  Status: {stats['status']}")
            print(f"  Jobs Processed: {stats['jobs_processed']}")
            print(f"  Errors: {stats['errors']}")
            if stats['last_heartbeat']:
                print(f"  Last Heartbeat: {stats['last_heartbeat']}")
            print()

async def monitor_workers():
    """Monitor worker status and health."""
    monitor = WorkerMonitor()
    processor = AsyncJobEventProcessor()
    
    # Register handlers
    processor.add_global_handler(monitor.handle_worker_event)
    processor.add_global_handler(monitor.handle_job_event)
    
    await processor.start()
    
    try:
        print("üë∑ Monitoring workers... (Press Ctrl+C to stop)")
        
        # Print stats every 30 seconds
        import time
        last_stats = time.time()
        
        async for event in processor.stream_job_events():
            # Check if it's time to print stats
            if time.time() - last_stats > 30:
                monitor.print_stats()
                last_stats = time.time()
                
    except KeyboardInterrupt:
        print("\nüëã Stopping worker monitor")
        monitor.print_stats()
    finally:
        await processor.stop()

if __name__ == "__main__":
    asyncio.run(monitor_workers())
```

### Custom Event-Driven Application

Build applications that react to job lifecycle events:

```python
# notification_system.py
import asyncio
from naq.events import AsyncJobEventProcessor, JobEventType
from naq import enqueue

class NotificationSystem:
    """Example system that reacts to job events."""
    
    def __init__(self):
        self.failed_jobs = []
        self.slow_jobs = []
    
    async def handle_job_completed(self, event):
        """Handle successful job completion."""
        if event.duration_ms and event.duration_ms > 5000:
            self.slow_jobs.append({
                'job_id': event.job_id,
                'duration': event.duration_ms,
                'queue': event.queue_name
            })
            
            # Maybe enqueue a follow-up optimization job
            await enqueue(
                self.analyze_slow_job,
                job_id=event.job_id,
                duration=event.duration_ms,
                queue_name="analytics"
            )
    
    async def handle_job_failed(self, event):
        """Handle job failures."""
        self.failed_jobs.append({
            'job_id': event.job_id,
            'error': event.error_message,
            'worker': event.worker_id,
            'timestamp': event.timestamp
        })
        
        # Enqueue failure analysis
        await enqueue(
            self.analyze_failure,
            job_id=event.job_id,
            error_message=event.error_message,
            queue_name="support"
        )
        
        print(f"üìß Failure notification sent for job {event.job_id}")
    
    async def analyze_slow_job(self, job_id, duration):
        """Analyze why a job was slow."""
        print(f"üîç Analyzing slow job {job_id} (took {duration}ms)")
        # Implement analysis logic
        return {"analysis": "completed", "recommendations": ["optimize_query"]}
    
    async def analyze_failure(self, job_id, error_message):
        """Analyze job failure."""
        print(f"üîç Analyzing failed job {job_id}: {error_message}")
        # Implement failure analysis logic
        return {"analysis": "completed", "likely_cause": "timeout"}
    
    async def get_health_report(self):
        """Generate system health report."""
        return {
            "failed_jobs_count": len(self.failed_jobs),
            "slow_jobs_count": len(self.slow_jobs),
            "recent_failures": self.failed_jobs[-5:],
            "recent_slow_jobs": self.slow_jobs[-5:]
        }

async def run_notification_system():
    """Run the notification system."""
    notification_system = NotificationSystem()
    processor = AsyncJobEventProcessor()
    
    # Register event handlers
    processor.add_handler(JobEventType.COMPLETED, notification_system.handle_job_completed)
    processor.add_handler(JobEventType.FAILED, notification_system.handle_job_failed)
    
    await processor.start()
    
    try:
        print("üì¢ Notification system active... (Press Ctrl+C to stop)")
        async for event in processor.stream_job_events():
            pass
    except KeyboardInterrupt:
        print("\nüëã Stopping notification system")
        # Print final health report
        health_report = await notification_system.get_health_report()
        print(f"üìä Final Health Report: {health_report}")
    finally:
        await processor.stop()

if __name__ == "__main__":
    asyncio.run(run_notification_system())
```

### Schedule Management with Event Tracking

Monitor and manage scheduled jobs:

```python
# schedule_manager.py
import asyncio
from datetime import datetime, timedelta
from naq import Queue
from naq.events import AsyncJobEventProcessor, JobEventType

async def scheduled_task(task_name, run_count):
    """Example scheduled task."""
    print(f"üöÄ Running {task_name} (execution #{run_count})")
    await asyncio.sleep(2)
    return f"Task {task_name} completed"

async def demo_schedule_management():
    """Demonstrate schedule management with event tracking."""
    queue = Queue("scheduled_demo")
    processor = AsyncJobEventProcessor()
    
    # Track schedule events
    def track_schedule_events(event):
        if 'schedule' in event.event_type.value:
            print(f"üìÖ {event.event_type}: {event.job_id} - {event.message}")
    
    processor.add_global_handler(track_schedule_events)
    await processor.start()
    
    try:
        # Schedule a recurring job (every 30 seconds, 3 times)
        scheduled_job = await queue.schedule(
            scheduled_task,
            task_name="data_sync",
            run_count=1,
            interval=timedelta(seconds=30),
            repeat=3
        )
        
        print(f"üìã Scheduled job {scheduled_job.job_id}")
        
        # Let it run for a bit
        await asyncio.sleep(45)
        
        # Pause the scheduled job
        print("‚è∏Ô∏è  Pausing scheduled job...")
        await queue.pause_scheduled_job(scheduled_job.job_id)
        
        await asyncio.sleep(10)
        
        # Resume it
        print("‚ñ∂Ô∏è  Resuming scheduled job...")
        await queue.resume_scheduled_job(scheduled_job.job_id)
        
        await asyncio.sleep(45)
        
        # Finally cancel it
        print("‚ùå Cancelling scheduled job...")
        await queue.cancel_scheduled_job(scheduled_job.job_id)
        
    finally:
        await processor.stop()
        await queue.close()

if __name__ == "__main__":
    asyncio.run(demo_schedule_management())
```

These examples show how NAQ's event system enables:

- **Custom Monitoring**: Build tailored monitoring solutions
- **Reactive Applications**: Create systems that respond to job events  
- **Performance Analytics**: Track and analyze system performance
- **Automated Management**: Implement automated responses to events
- **Business Intelligence**: Extract insights from job processing patterns