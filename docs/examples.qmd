---
title: "Usage Examples"
---

This page provides practical examples for some of `naq`'s key features.

## Example 1: Scheduled and Recurring Jobs

`naq` allows you to schedule jobs to run at a specific time in the future or on a recurring basis using cron expressions.

To run these examples, you need both a worker and the scheduler process running:

```bash
# Terminal 1: Start the scheduler
naq scheduler

# Terminal 2: Start a worker
naq worker scheduled_queue
```

### One-Time Scheduled Job

You can enqueue a job to run after a specific delay or at a precise time. With the service layer, scheduling operations are performed through a Queue instance that uses a ServiceManager.

```python
# schedule_task.py
import asyncio
from datetime import datetime, timedelta
from naq.queue import Queue
from naq.services import ServiceManager

async def send_reminder(user_id, message):
    print(f"Sending reminder to {user_id}: {message}")
    # Add logic to send an email or push notification
    return f"Reminder sent to {user_id}"

async def main():
    # Create a Queue instance with ServiceManager
    async with ServiceManager() as services:
        queue = Queue(name="scheduled_queue", services=services)
        
        # Schedule a job to run in 5 minutes
        run_in_5_min = await queue.enqueue_in(
            send_reminder,
            delay=timedelta(minutes=5),
            user_id="user123",
            message="Your meeting starts in 5 minutes."
        )
        print(f"Job {run_in_5_min.job_id} scheduled to run in 5 minutes.")

        # Schedule a job to run at a specific time (UTC)
        run_at_time = datetime.utcnow() + timedelta(hours=1)
        run_at = await queue.enqueue_at(
            send_reminder,
            run_at=run_at_time,
            user_id="user456",
            message="Don't forget your 1-hour follow-up."
        )
        print(f"Job {run_at.job_id} scheduled to run at {run_at_time.isoformat()}.")

if __name__ == "__main__":
    asyncio.run(main())
```

### Recurring Job (Cron)

For tasks that need to run on a regular schedule (e.g., nightly reports, weekly cleanups), you can use the `schedule` method with a cron string through a Queue instance that uses a ServiceManager.

```python
# recurring_task.py
import asyncio
from naq.queue import Queue
from naq.services import ServiceManager

async def generate_nightly_report():
    print("Generating the nightly sales report...")
    # Logic to aggregate data and create a report
    print("Nightly report complete.")
    return "Report generated successfully."

async def main():
    # Create a Queue instance with ServiceManager
    async with ServiceManager() as services:
        queue = Queue(name="scheduled_queue", services=services)
        
        # Schedule the report to run every day at 2:00 AM UTC
        cron_schedule = await queue.schedule(
            generate_nightly_report,
            cron="0 2 * * *",  # Standard cron format
            schedule_id="nightly-sales-report"
        )
        print(f"Cron job '{cron_schedule.schedule_id}' is now active.")

if __name__ == "__main__":
    asyncio.run(main())
```

## Example 2: Automatic Job Retries

`naq` can automatically retry failed jobs with configurable strategies. This is useful for tasks that might fail due to transient issues, like network hiccups.

```python
# retry_task.py
import asyncio
import random
from naq import enqueue

async def flaky_api_call(request_id):
    """
    This function simulates an API call that sometimes fails.
    """
    print(f"Attempting to call API for request {request_id}...")
    if random.random() > 0.5:
        print("API call successful!")
        return "Success"
    else:
        print("API call failed. Will retry...")
        raise ConnectionError("Could not connect to the API")

async def main():
    # Enqueue the job with a retry policy
    job = await enqueue(
        flaky_api_call,
        request_id="abc-123",
        queue_name="default",
        max_retries=3,          # Attempt the job up to 3 more times
        retry_delay=5,          # Wait 5 seconds between retries
        retry_strategy="linear" # Use a fixed delay
    )
    print(f"Enqueued job {job.job_id} with 3 linear retries.")

if __name__ == "__main__":
    asyncio.run(main())
```

::: {.callout-note}
**Retry Strategies**

`naq` supports both `linear` (fixed delay) and `exponential` backoff strategies for retries. You can also provide a list of integers to `retry_delay` for a custom delay sequence.
:::

## Example 3: Job Dependencies

You can create workflows by making jobs dependent on the successful completion of others. The dependent job will only run after its dependencies have finished.

```python
# dependency_workflow.py
import asyncio
from naq import enqueue

async def download_data(source_url):
    print(f"Downloading data from {source_url}...")
    await asyncio.sleep(2)  # Simulate download
    file_path = f"/tmp/{source_url.split('/')[-1]}.csv"
    print(f"Data downloaded to {file_path}")
    return file_path

async def process_data(file_path):
    print(f"Processing data from {file_path}...")
    await asyncio.sleep(3)  # Simulate processing
    result_path = f"{file_path}.processed"
    print(f"Data processed and saved to {result_path}")
    return result_path

async def upload_results(result_path):
    print(f"Uploading {result_path} to cloud storage...")
    await asyncio.sleep(1)  # Simulate upload
    print("Upload complete.")
    return "Workflow finished successfully."

async def main():
    # Step 1: Download data
    download_job = await enqueue(download_data, source_url="http://example.com/data")

    # Step 2: Process data (depends on download)
    process_job = await enqueue(
        process_data,
        file_path=download_job, # Pass the result of the dependency
        depends_on=[download_job]
    )

    # Step 3: Upload results (depends on processing)
    upload_job = await enqueue(
        upload_results,
        result_path=process_job,
        depends_on=[process_job]
    )

    print(f"Workflow started. Final job: {upload_job.job_id}")

if __name__ == "__main__":
    asyncio.run(main())
```

## Example 4: Using Multiple Queues

You can use different queues to prioritize jobs or to dedicate workers to specific types of tasks.

Start workers for each queue in separate terminals:

```bash
# Terminal 1: High-priority worker
naq worker notifications --log-level info

# Terminal 2: Low-priority worker
naq worker data_processing --log-level info
```

Now, you can enqueue jobs to the appropriate queues.

```python
# multi_queue_example.py
import asyncio
from naq import enqueue

async def send_email(address, subject, body):
    print(f"Sending high-priority email to {address}...")
    await asyncio.sleep(0.5)
    return "Email sent."

async def transcode_video(video_id):
    print(f"Starting low-priority video transcoding for {video_id}...")
    await asyncio.sleep(10) # Simulate long-running task
    return "Video transcoded."

async def main():
    # Enqueue a high-priority job
    email_job = await enqueue(
        send_email,
        address="user@example.com",
        subject="Your order",
        body="...",
        queue_name="notifications" # Target the 'notifications' queue
    )
    print(f"Enqueued notification job {email_job.job_id}")

    # Enqueue a low-priority job
    video_job = await enqueue(
        transcode_video,
        video_id=12345,
        queue_name="data_processing" # Target the 'data_processing' queue
    )
    print(f"Enqueued data processing job {video_job.job_id}")

if __name__ == "__main__":
    asyncio.run(main())

## Example 5: Custom Event Handlers

The event system allows you to build powerful, reactive workflows by responding to job lifecycle events. This example demonstrates how to create a custom event handler that performs an action when a job of a certain type fails, using the service layer for event management.

To run this example, you will need:
1. A NATS server running.
2. Event logging enabled (`NAQ_EVENTS_ENABLED=true`).
3. A worker running to process jobs.
4. This script running to handle events.

```python
# custom_event_handler.py
import asyncio
import os
from naq.queue import Queue
from naq.services import ServiceManager
from naq.events import AsyncJobEventProcessor, JobEventType, JobEvent

# Ensure event logging is enabled for this example to work
os.environ["NAQ_EVENTS_ENABLED"] = "true"

# A task that might fail
async def flaky_task(task_id: str):
    print(f"[Task {task_id}] Starting risky operation...")
    await asyncio.sleep(1)
    if task_id == "fail-on-purpose":
        raise ValueError("This task was designed to fail!")
    print(f"[Task {task_id}] Operation completed successfully.")
    return f"Task {task_id} finished."

# Our custom event handler
async def on_job_failure(event: JobEvent):
    """
    This function is called whenever a job fails.
    """
    print("=" * 50)
    print("ðŸš¨ CUSTOM HANDLER TRIGGERED: JOB FAILURE DETECTED! ðŸš¨")
    print("=" * 50)
    print(f"  Job ID:       {event.job_id}")
    print(f"  Error Type:    {event.error_type}")
    print(f"  Error Message: {event.error_message}")
    print(f"  Queue:         {event.queue_name}")
    print(f"  Worker:        {event.worker_id}")
    print(f"  Timestamp:     {event.timestamp}")
    print("=" * 50)
    # Here you could add more complex logic, like:
    # - Sending a notification to Slack or PagerDuty
    # - Writing to a database for analytics
    # - Triggering a recovery workflow

async def main():
    # --- Setup ---
    print("Setting up service manager and event processor...")

    # Create ServiceManager
    async with ServiceManager() as services:
        # Get EventService from the service manager
        event_service = await services.get_service(EventService)
        
        # Create event processor using the EventService
        processor = AsyncJobEventProcessor(event_service.get_event_storage())

        # 2. Register our custom handler for FAILED events
        processor.add_handler(JobEventType.FAILED, on_job_failure)
        
        # 3. Start the processor in the background
        # It will begin listening for events immediately.
        processor_task = asyncio.create_task(processor.start())

        # Give the processor a moment to start up
        await asyncio.sleep(2)

        # --- Enqueue Jobs ---
        print("\nEnqueuing a successful job and a job that will fail...")
        
        # Create Queue instance to enqueue jobs
        queue = Queue(name="example_queue", services=services)
        
        # Enqueue a job that will succeed
        await queue.enqueue(flaky_task, "success-1")
        
        # Enqueue a job that is designed to fail
        await queue.enqueue(flaky_task, "fail-on-purpose")

        print("Jobs enqueued. Make sure a worker is running to process them.")
        print("The custom handler will be called when the 'fail-on-purpose' job fails.")

        # --- Keep Running ---
        try:
            # Keep the script running to allow the processor to handle events
            print("\nWaiting for events... Press Ctrl+C to exit.")
            while True:
                await asyncio.sleep(1)
        except KeyboardInterrupt:
            print("\nShutting down...")
        finally:
            # Ensure the processor is stopped cleanly
            await processor.stop()
            await processor_task # Wait for the processor task to finish

if __name__ == "__main__":
    asyncio.run(main())
```

To see this in action:

1.  **Start a worker** in one terminal:
    ```bash
    NAQ_EVENTS_ENABLED=true naq worker example_queue
    ```

2.  **Run the custom handler script** in another terminal:
    ```bash
    python custom_event_handler.py
    ```

You will see the handler script enqueue two jobs. The worker will pick them up. When the `fail-on-purpose` job fails, you will see the custom "ðŸš¨ CUSTOM HANDLER TRIGGERED" message printed by the `on_job_failure` function in the handler script's terminal, demonstrating the reactive power of the event system.

## Example 6: Connection Management with Context Managers

The connection management system provides context managers for automatic resource management. With the service layer integration, these context managers are primarily used internally by services like ConnectionService. The recommended approach is to use ServiceManager for connection management at the application level.

### Service-Based Connection Management (Recommended)

The preferred way to manage connections is through ServiceManager, which handles connection pooling, reuse, and cleanup automatically.

```python
# service_connection_example.py
import asyncio
from naq.queue import Queue
from naq.services import ServiceManager

async def service_manager_example():
    """Example of connection management through ServiceManager."""
    
    # Create ServiceManager with configuration
    async with ServiceManager() as services:
        # Get a Queue instance that uses the service layer
        queue = Queue(name="example_queue", services=services)
        
        # The Queue internally uses services for connection management
        # Connection pooling, reuse, and cleanup are handled automatically
        job = await queue.enqueue(my_task, arg1, arg2)
        print(f"Job enqueued: {job.job_id}")
        
        # No manual connection management needed at the application level
        # Services handle everything internally

async def multiple_queues_example():
    """Example of managing multiple queues with shared connections."""
    
    async with ServiceManager() as services:
        # Create multiple queues that share connection resources
        high_priority = Queue(name="high_priority", services=services)
        normal_priority = Queue(name="normal_priority", services=services)
        background = Queue(name="background", services=services)
        
        # All queues share the same connection pool managed by ServiceManager
        high_job = await high_priority.enqueue(critical_task, "urgent")
        normal_job = await normal_priority.enqueue(regular_task, "data")
        bg_job = await background.enqueue(low_priority_task, "cleanup")
        
        print(f"Jobs enqueued: {high_job.job_id}, {normal_job.job_id}, {bg_job.job_id}")

if __name__ == "__main__":
    asyncio.run(service_manager_example())
    asyncio.run(multiple_queues_example())
```

### Context Managers (Lower-Level Usage)

Direct usage of context managers is still available but is primarily intended for lower-level operations or service-internal use cases. These provide fine-grained control over NATS connections when needed.

```python
# connection_example.py
import asyncio
from naq.utils.nats_helpers import nats_connection_context, nats_jetstream_context, nats_kv_store_context

async def basic_connection_example():
    """Example of basic NATS connection management."""
    
    # Simple connection for publishing messages
    async with nats_connection_context() as conn:
        await conn.publish("hello.world", b"Hello from context manager!")
        print("Message published successfully")
    
    # Connection is automatically closed when exiting the context

async def jetstream_example():
    """Example of JetStream context management."""
    
    # Combined connection and JetStream context
    async with nats_jetstream_context() as (conn, js):
        # Create a stream
        await js.add_stream(
            name="example_stream",
            subjects=["example.*"]
        )
        print("Stream created successfully")
        
        # Publish a message to the stream
        await js.publish("example.data", b"Stream data")
        print("Message published to stream")
    
    # Both connection and JetStream context are automatically closed

async def kv_store_example():
    """Example of KeyValue store management."""
    
    # KeyValue store context
    async with nats_kv_store_context("example_bucket") as kv:
        # Store a value
        await kv.put("user:123", '{"name": "Alice", "role": "admin"}')
        print("Value stored in KV bucket")
        
        # Retrieve the value
        result = await kv.get("user:123")
        print(f"Retrieved value: {result.value.decode()}")
        
        # List all keys
        keys = await kv.keys()
        print(f"Keys in bucket: {list(keys)}")
    
    # KV store context is automatically closed

if __name__ == "__main__":
    asyncio.run(basic_connection_example())
    asyncio.run(jetstream_example())
    asyncio.run(kv_store_example())
```

### Connection Management with Decorators

```python
# decorator_example.py
import asyncio
from naq.utils.decorators import with_nats_connection, with_jetstream_context

@with_nats_connection()
async def publish_message(conn, subject: str, message: str):
    """Publish a message using the injected connection."""
    await conn.publish(subject, message.encode())
    print(f"Message published to {subject}")

@with_jetstream_context()
async def create_consumer(js, stream_name: str, consumer_name: str):
    """Create a consumer using the injected JetStream context."""
    await js.add_consumer(
        stream_name,
        durable_name=consumer_name,
        ack_policy="explicit"
    )
    print(f"Consumer '{consumer_name}' created for stream '{stream_name}'")

async def decorator_example():
    """Example of using connection decorators."""
    
    # Publish a message - connection is automatically injected
    await publish_message("decorator.test", "Hello from decorator!")
    
    # Create a consumer - JetStream context is automatically injected
    await create_consumer("example_stream", "example_consumer")

if __name__ == "__main__":
    asyncio.run(decorator_example())
```

### Connection Testing and Monitoring

```python
# monitoring_example.py
import asyncio
from naq.utils.nats_helpers import (
    test_nats_connection,
    wait_for_nats_connection,
    get_connection_metrics,
    nats_connection_context
)

async def monitoring_example():
    """Example of connection testing and monitoring."""
    
    # Test if connection is available
    is_healthy = await test_nats_connection()
    print(f"Connection healthy: {is_healthy}")
    
    # Wait for connection to be ready (useful in production)
    is_ready = await wait_for_nats_connection(timeout=10)
    print(f"Connection ready: {is_ready}")
    
    # Use connection and monitor metrics
    async with nats_connection_context() as conn:
        # Do some work
        await conn.publish("monitoring.test", b"Test message")
        
        # Check connection metrics
        metrics = get_connection_metrics()
        print(f"Total connections: {metrics.total_connections}")
        print(f"Active connections: {metrics.active_connections}")
        print(f"Failed connections: {metrics.failed_connections}")
        print(f"Average connection time: {metrics.average_connection_time:.2f}s")

if __name__ == "__main__":
    asyncio.run(monitoring_example())
```

### Custom Configuration

```python
# custom_config_example.py
import asyncio
from naq.config import ConnectionConfig
from naq.utils.nats_helpers import nats_connection_context, nats_jetstream_context

async def custom_config_example():
    """Example of using custom connection configuration."""
    
    # Create custom configuration
    config = ConnectionConfig(
        servers=["nats://localhost:4222", "nats://localhost:4223"],
        client_name="my-custom-app",
        max_reconnect_attempts=10,
        reconnect_time_wait=5.0
    )
    
    # Use custom configuration
    async with nats_connection_context(servers=config.servers, name=config.client_name) as conn:
        await conn.publish("config.test", b"Using custom config")
        print("Message published with custom configuration")
    
    # Use with JetStream
    async with nats_jetstream_context(servers=config.servers, name=config.client_name) as (conn, js):
        await js.add_stream(
            name="config_stream",
            subjects=["config.*"]
        )
        print("Stream created with custom configuration")

if __name__ == "__main__":
    asyncio.run(custom_config_example())
```

### Migration from Legacy Patterns

```python
# migration_example.py
import asyncio
from naq.connection import (
    # Legacy functions (still work)
    get_nats_connection,
    get_jetstream_context,
    close_nats_connection,
)
from naq.utils.nats_helpers import (
    # New context managers
    nats_connection_context,
    nats_jetstream_context
)

async def legacy_pattern():
    """Old way of managing connections."""
    print("=== Legacy Pattern ===")
    
    nc = await get_nats_connection()
    try:
        js = await get_jetstream_context(nc)
        await js.add_stream(name="legacy_stream", subjects=["legacy.*"])
        print("Stream created using legacy pattern")
    finally:
        await close_nats_connection(nc)
        print("Connection closed manually")

async def modern_pattern():
    """New way of managing connections."""
    print("=== Modern Pattern ===")
    
    async with nats_jetstream_context() as (conn, js):
        await js.add_stream(name="modern_stream", subjects=["modern.*"])
        print("Stream created using modern pattern")
    
    print("Connection closed automatically")

async def migration_example():
    """Example showing both patterns."""
    
    # Legacy pattern (still works)
    await legacy_pattern()
    
    # Modern pattern (recommended)
    await modern_pattern()

if __name__ == "__main__":
    asyncio.run(migration_example())
```

### Production-Ready Connection Management

```python
# production_example.py
import asyncio
from naq.utils.nats_helpers import (
    nats_connection_context,
    nats_jetstream_context,
    test_nats_connection,
    wait_for_nats_connection,
    get_connection_metrics
)
from naq.config import ConnectionConfig

class ProductionService:
    """Example of production-ready connection management."""
    
    def __init__(self, config: ConnectionConfig):
        self.config = config
    
    async def ensure_connection(self):
        """Ensure connection is available before proceeding."""
        if not await test_nats_connection(self.config):
            print("Waiting for connection...")
            if not await wait_for_nats_connection(self.config, timeout=30):
                raise ConnectionError("Failed to establish connection")
    
    async def process_data(self):
        """Process data with robust connection management."""
        
        # Ensure connection is available
        await self.ensure_connection()
        
        try:
            # Use JetStream with automatic connection management
            async with nats_jetstream_context(servers=self.config.servers, name=self.config.client_name) as (conn, js):
                # Create stream if it doesn't exist
                try:
                    await js.add_stream(
                        name="production_stream",
                        subjects=["production.*"]
                    )
                except Exception as e:
                    print(f"Stream might already exist: {e}")
                
                # Process and publish data
                for i in range(5):
                    data = f"Production data {i}"
                    await js.publish("production.data", data.encode())
                    print(f"Published: {data}")
                
                # Log connection metrics
                metrics = get_connection_metrics()
                print(f"Connection metrics - Total: {metrics.total_connections}, "
                      f"Active: {metrics.active_connections}, "
                      f"Failed: {metrics.failed_connections}")
        
        except Exception as e:
            print(f"Error in production processing: {e}")
            raise

async def production_example():
    """Example of production-ready connection management."""
    
    # Production configuration
    config = ConnectionConfig(
        servers=["nats://localhost:4222"],
        client_name="production-service",
        max_reconnect_attempts=10,
        reconnect_time_wait=5.0
    )
    
    # Create and run production service
    service = ProductionService(config)
    await service.process_data()

if __name__ == "__main__":
    asyncio.run(production_example())
```

To run these examples:

1.  **Start a NATS server** in one terminal:
    ```bash
    nats-server
    ```

2.  **Run the examples** in another terminal:
    ```bash
    python connection_example.py
    python decorator_example.py
    python monitoring_example.py
    python custom_config_example.py
    python migration_example.py
    python production_example.py
    ```

These examples demonstrate the various ways to use the new connection management system in `naq`, from basic usage to production-ready patterns. The context managers provide automatic resource management, while the decorators offer a convenient way to inject connections into functions.