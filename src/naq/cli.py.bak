import asyncio
import datetime
import time
import nats.js.errors
from datetime import timezone
from typing import List, Optional  # , Dict, Any

import cloudpickle
import typer
from loguru import logger
from rich.console import Console
from rich.table import Table
from rich.panel import Panel
from rich.markdown import Markdown
from rich.text import Text
from rich.style import Style


# from naq.dashboard.app import app as dashboard_app

from . import __version__
from .connection import (
    close_nats_connection,
    get_jetstream_context,
    get_nats_connection,
)
from .queue import Queue
from .scheduler import Scheduler
from .settings import DEFAULT_WORKER_TTL_SECONDS  # Import statuses
from .settings import (
    DEFAULT_NATS_URL,
    DEFAULT_QUEUE_NAME,
    SCHEDULED_JOB_STATUS,
    SCHEDULED_JOBS_KV_NAME,
    WORKER_STATUS,
    NAQ_PREFIX,
)
from .utils import setup_logging
from .worker import Worker
from .events.logger import AsyncJobEventLogger

# import os  # Import os for environment variables


app = typer.Typer(
    name="naq",
    help="A simple NATS-based queueing system, similar to RQ.",
    add_completion=False,
)

# Create a shared console instance for Rich output
console = Console()

# --- Helper Functions ---


def version_callback(value: bool):
    if value:
        console.print(f"[cyan]naq[/cyan] version: [bold]{__version__}[/bold]")
        raise typer.Exit()


# --- CLI Commands ---


@app.command()
def worker(
    queues: List[str] = typer.Argument(
        default=None,
        help="The names of the queues to listen to. Defaults to the configured default queue.",
    ),
    nats_url: str = typer.Option(
        DEFAULT_NATS_URL,
        "--nats-url",
        "-u",
        help="URL of the NATS server.",
        envvar="NAQ_NATS_URL",  # Allow setting via env var
    ),
    concurrency: int = typer.Option(
        10,
        "--concurrency",
        "-c",
        min=1,
        help="Maximum number of concurrent jobs to process.",
    ),
    name: Optional[str] = typer.Option(
        None,
        "--name",
        "-n",
        help="Optional name for this worker instance.",
    ),
    module_paths: Optional[List[str]] = typer.Option(
        None,
        "--module-path",
        "-m",
        help="Additional paths to add to sys.path for module imports. Can be specified multiple times.",
    ),
    log_level: Optional[str] = typer.Option(
        None,
        "--log-level",
        "-l",
        help="Set logging level (e.g., DEBUG, INFO, WARNING, ERROR). Defaults to NAQ_LOG_LEVEL env var or CRITICAL.",
    ),
):
    """
    Starts a naq worker process to listen for and execute jobs on the specified queues.
    """
    setup_logging(log_level if log_level else None)

    # If no queues are provided, let the Worker class handle the default
    if queues is None:
        queues = []

    # Use loguru directly
    logger.info(
        f"Starting worker '{name or 'default'}' for queues: {queues if queues else [DEFAULT_QUEUE_NAME]}"
    )
    logger.info(f"NATS URL: {nats_url}")
    logger.info(f"Concurrency: {concurrency}")

    w = Worker(
        queues=queues,
        nats_url=nats_url,
        concurrency=concurrency,
        worker_name=name,
        module_paths=module_paths,
    )
    try:
        # Use synchronous interface backed by AnyIO BlockingPortal
        w.run_sync()
    except KeyboardInterrupt:
        logger.info("Worker interrupted by user (KeyboardInterrupt). Shutting down.")
    except Exception as e:
        logger.exception(
            f"Worker failed unexpectedly: {e}"
        )  # Use logger.exception for stack trace
        raise typer.Exit(code=1)
    finally:
        logger.info("Worker process finished.")


@app.command()
def purge(
    queues: List[str] = typer.Argument(..., help="The names of the queues to purge."),
    nats_url: str = typer.Option(
        DEFAULT_NATS_URL,
        "--nats-url",
        "-u",
        help="URL of the NATS server.",
        envvar="NAQ_NATS_URL",
    ),
    log_level: Optional[str] = typer.Option(
        None,  # Set default log level to None to use env var
        "--log-level",
        "-l",
        help="Set logging level (e.g., DEBUG, INFO, WARNING, ERROR). Defaults to NAQ_LOG_LEVEL env var or CRITICAL.",
    ),
):
    """
    Removes all jobs from the specified queues.
    """
    setup_logging(log_level if log_level else None)
    logger.info(f"Attempting to purge queues: {queues}")
    logger.info(f"Using NATS URL: {nats_url}")

    # Use synchronous helper for purge (reuses thread-local NATS connection)
    from .queue import purge_queue_sync

    results = {}
    total_purged = 0
    for queue_name in queues:
        try:
            purged_count = purge_queue_sync(queue_name=queue_name, nats_url=nats_url)
            results[queue_name] = {"status": "success", "count": purged_count}
            total_purged += purged_count
        except Exception as e:
            results[queue_name] = {"status": "error", "error": str(e)}
            logger.error(f"Failed to purge queue '{queue_name}': {e}")

    # --- Report Results using Rich ---
    success_count = sum(1 for r in results.values() if r["status"] == "success")
    error_count = len(results) - success_count

    console.print("\n[bold]Purge Results:[/bold]")
    for name, result in results.items():
        if result["status"] == "success":
            console.print(
                f"  - [green]Queue '{name}': Purged {result['count']} jobs.[/green]"
            )
        else:
            console.print(f"  - [red]Queue '{name}': Failed - {result['error']}[/red]")

    # --- Summary Panel ---
    summary_color = (
        "green" if error_count == 0 else ("yellow" if success_count > 0 else "red")
    )
    summary_text = f"Total jobs removed: {total_purged}\nQueues processed: {len(results)}\nSuccessful purges: {success_count}\nFailed purges: {error_count}"
    console.print(
        Panel(summary_text, title="Purge Summary", style=summary_color, expand=False)
    )
    # --- End Reporting ---


@app.command()
def scheduler(
    nats_url: str = typer.Option(
        DEFAULT_NATS_URL,
        "--nats-url",
        "-u",
        help="URL of the NATS server.",
        envvar="NAQ_NATS_URL",
    ),
    poll_interval: float = typer.Option(
        1.0,
        "--poll-interval",
        "-p",
        min=0.1,
        help="Interval in seconds between checks for due jobs.",
    ),
    instance_id: Optional[str] = typer.Option(
        None,
        "--instance-id",
        "-i",
        help="Optional unique ID for this scheduler instance (for high availability).",
    ),
    disable_ha: bool = typer.Option(
        False,
        "--disable-ha",
        help="Disable high availability mode (leader election).",
    ),
    log_level: Optional[str] = typer.Option(
        None,
        "--log-level",
        "-l",
        help="Set logging level (e.g., DEBUG, INFO, WARNING, ERROR). Defaults to NAQ_LOG_LEVEL env var or CRITICAL.",
    ),
):
    """
    Starts a naq scheduler process to execute scheduled jobs at their specified times.

    In high availability mode (default), multiple scheduler instances can be run simultaneously
    and they will coordinate using leader election to ensure jobs are only processed once.
    """
    setup_logging(log_level if log_level else None)
    enable_ha = not disable_ha

    logger.info(
        f"Starting scheduler{f' instance {instance_id}' if instance_id else ''}"
    )
    logger.info(f"NATS URL: {nats_url}")
    logger.info(f"Poll interval: {poll_interval}s")
    logger.info(f"High availability mode: {'enabled' if enable_ha else 'disabled'}")

    s = Scheduler(
        nats_url=nats_url,
        poll_interval=poll_interval,
        instance_id=instance_id,
        enable_ha=enable_ha,
    )
    try:
        asyncio.run(s.run())
    except KeyboardInterrupt:
        logger.info("Scheduler interrupted by user (KeyboardInterrupt). Shutting down.")
    except Exception as e:
        logger.exception(f"Scheduler failed unexpectedly: {e}")
        raise typer.Exit(code=1)
    finally:
        logger.info("Scheduler process finished.")


@app.command("list-scheduled")
def list_scheduled_jobs(
    nats_url: str = typer.Option(
        DEFAULT_NATS_URL,
        "--nats-url",
        "-u",
        help="URL of the NATS server.",
        envvar="NAQ_NATS_URL",
    ),
    status: Optional[str] = typer.Option(
        None,
        "--status",
        "-s",
        help=f"Filter by job status: '{SCHEDULED_JOB_STATUS.ACTIVE}', '{SCHEDULED_JOB_STATUS.PAUSED}', or '{SCHEDULED_JOB_STATUS.FAILED}'",
    ),
    job_id: Optional[str] = typer.Option(
        None,
        "--job-id",
        "-j",
        help="Filter by job ID",
    ),
    queue: str = typer.Option(
        DEFAULT_QUEUE_NAME,
        "--queue",
        "-q",
        help="Filter by queue name",
    ),
    detailed: bool = typer.Option(
        False,
        "--detailed",
        "-d",
        help="Show detailed job information",
    ),
    log_level: Optional[str] = typer.Option(
        None,
        "--log-level",
        "-l",
        help="Set logging level (e.g., DEBUG, INFO, WARNING, ERROR). Defaults to NAQ_LOG_LEVEL env var or CRITICAL.",
    ),
):
    """
    Lists all scheduled jobs with their status and next run time.
    """
    setup_logging(log_level if log_level else None)
    logger.info(f"Listing scheduled jobs from NATS at {nats_url}")

    # Use sync Key-Value access via thread-local connection for simplicity
    from .queue import Queue
    from .connection import get_nats_connection, get_jetstream_context  # types only

    # We'll use run_async_from_sync through Queue helpers where available,
    # but this command mostly reads KV directly; keep the logic synchronous by
    # reusing thread-local connections within run_async_from_sync.
    def _list_sync():
        return asyncio.run(_list_scheduled_jobs_async())

    async def _list_scheduled_jobs_async():
        try:
            nc = await get_nats_connection(url=nats_url)
            js = await get_jetstream_context(nc=nc)

            try:
                kv = await js.key_value(bucket=SCHEDULED_JOBS_KV_NAME)
            except Exception as e:
                logger.error(
                    f"Failed to access KV store '{SCHEDULED_JOBS_KV_NAME}': {e}"
                )
                console.print(
                    "[yellow]No scheduled jobs found or cannot access job store.[/yellow]"
                )
                return

            # Get all keys
            try:
                keys = await kv.keys()
                if not keys:
                    console.print("[yellow]No scheduled jobs found.[/yellow]")
                    return
            except nats.js.errors.NoKeysError:
                console.print("[yellow]No scheduled jobs found.[/yellow]")
                return

            jobs_data = []

            for key_bytes in keys:
                key = None
                try:
                    key = (
                        key_bytes.decode("utf-8")
                        if isinstance(key_bytes, bytes)
                        else key_bytes
                    )
                    if job_id and job_id != key:
                        continue

                    entry = await kv.get(key_bytes)
                    if not entry:
                        continue

                    job_data = cloudpickle.loads(entry.value)

                    current_status = job_data.get("status")
                    if status and current_status != status:
                        continue
                    if queue and job_data.get("queue_name") != queue:
                        continue

                    jobs_data.append(job_data)
                except Exception as e:
                    key_repr = key if key is not None else repr(key_bytes)
                    logger.error(
                        f"Error processing scheduled job entry '{key_repr}': {e}"
                    )
                    continue

            jobs_data.sort(key=lambda j: j.get("scheduled_timestamp_utc", 0))

            if detailed:
                table = Table(
                    title="NAQ Scheduled Jobs",
                    show_header=True,
                    header_style="bold cyan",
                )
                table.add_column("JOB ID", style="dim", width=36)
                table.add_column("QUEUE", width=15)
                table.add_column("STATUS", width=10)
                table.add_column("NEXT RUN", width=25)
                table.add_column("SCHEDULE TYPE", width=15)
                table.add_column("REPEATS LEFT", width=12)
                table.add_column("DETAILS")
            else:
                table = Table(
                    title="NAQ Scheduled Jobs",
                    show_header=True,
                    header_style="bold cyan",
                )
                table.add_column("JOB ID", style="dim", width=36)
                table.add_column("QUEUE", width=15)
                table.add_column("STATUS", width=10)
                table.add_column("NEXT RUN", width=25)
                table.add_column("SCHEDULE TYPE", width=15)

            for job in jobs_data:
                job_id_local = job.get("job_id", "unknown")
                queue_name = job.get("queue_name", "unknown")
                current_job_status = job.get("status", SCHEDULED_JOB_STATUS.ACTIVE)

                status_style = "green"
                if current_job_status == SCHEDULED_JOB_STATUS.PAUSED:
                    status_style = "yellow"
                elif current_job_status == SCHEDULED_JOB_STATUS.FAILED:
                    status_style = "red"

                next_run_ts = job.get("scheduled_timestamp_utc")
                if next_run_ts:
                    next_run = datetime.datetime.fromtimestamp(
                        next_run_ts, timezone.utc
                    ).strftime("%Y-%m-%d %H:%M:%S UTC")
                else:
                    next_run = "unknown"

                if job.get("cron"):
                    schedule_type = "cron"
                elif job.get("interval_seconds"):
                    schedule_type = "interval"
                else:
                    schedule_type = "one-time"

                if detailed:
                    repeats = (
                        "infinite"
                        if job.get("repeat") is None
                        else str(job.get("repeat", 0))
                    )
                    details = []
                    if job.get("cron"):
                        details.append(f"cron='{job.get('cron')}'")
                    if job.get("interval_seconds"):
                        details.append(f"interval={job.get('interval_seconds')}s")
                    if job.get("schedule_failure_count", 0) > 0:
                        details.append(f"failures={job.get('schedule_failure_count')}")
                    if job.get("last_enqueued_utc"):
                        last_run = datetime.datetime.fromtimestamp(
                            job.get("last_enqueued_utc"), timezone.utc
                        ).strftime("%Y-%m-%d %H:%M:%S UTC")
                        details.append(f"last_run={last_run}")

                    details_str = ", ".join(details)
                    table.add_row(
                        job_id_local,
                        queue_name,
                        f"[{status_style}]{current_job_status}[/{status_style}]",
                        next_run,
                        schedule_type,
                        repeats,
                        details_str,
                    )
                else:
                    table.add_row(
                        job_id_local,
                        queue_name,
                        f"[{status_style}]{current_job_status}[/{status_style}]",
                        next_run,
                        schedule_type,
                    )

            console.print(table)
            console.print(f"\n[bold]Total:[/bold] {len(jobs_data)} scheduled job(s)")
        except Exception as e:
            logger.exception(f"Error listing scheduled jobs: {e}")
            console.print(f"[red]Error listing scheduled jobs: {str(e)}[/red]")
        finally:
            await close_nats_connection()

    # Run the async routine
    asyncio.run(_list_scheduled_jobs_async())


@app.command("job-control")
def job_control(
    job_id: str = typer.Argument(..., help="The ID of the scheduled job to control"),
    action: str = typer.Argument(
        ...,
        help="Action to perform: 'cancel', 'pause', 'resume', or 'reschedule'",
        show_choices=True,
    ),
    nats_url: str = typer.Option(
        DEFAULT_NATS_URL,
        "--nats-url",
        "-u",
        help="URL of the NATS server.",
        envvar="NAQ_NATS_URL",
    ),
    cron: Optional[str] = typer.Option(
        None,
        "--cron",
        help="New cron expression for reschedule action",
    ),
    interval: Optional[float] = typer.Option(
        None,
        "--interval",
        help="New interval in seconds for reschedule action",
    ),
    repeat: Optional[int] = typer.Option(
        None,
        "--repeat",
        help="New repeat count for reschedule action",
    ),
    next_run: Optional[str] = typer.Option(
        None,
        "--next-run",
        help="Next run time (ISO format, e.g. '2023-01-01T12:00:00Z') for reschedule action",
    ),
    log_level: Optional[str] = typer.Option(
        None,
        "--log-level",
        "-l",
        help="Set logging level (e.g., DEBUG, INFO, WARNING, ERROR). Defaults to NAQ_LOG_LEVEL env var or CRITICAL.",
    ),
):
    """
    Controls scheduled jobs: cancel, pause, resume, or modify scheduling parameters.
    """
    setup_logging(log_level if log_level else None)

    # Validate action
    if action not in ["cancel", "pause", "resume", "reschedule"]:
        logger.error(
            f"Invalid action '{action}'. Must be one of: cancel, pause, resume, reschedule"
        )
        raise typer.Exit(code=1)

    # Validate parameters for reschedule
    if action == "reschedule":
        if not any([cron, interval, repeat, next_run]):
            logger.error(
                "Reschedule action requires at least one scheduling parameter: --cron, --interval, --repeat, or --next-run"
            )
            raise typer.Exit(code=1)
        if cron and interval:
            logger.error(
                "Cannot specify both --cron and --interval. Choose one scheduling method."
            )
            raise typer.Exit(code=1)

    logger.info(f"Performing {action} on job {job_id}")

    # Use synchronous helpers provided by Queue sync wrappers
    from .queue import (
        cancel_scheduled_job_sync,
        pause_scheduled_job_sync,
        resume_scheduled_job_sync,
        modify_scheduled_job_sync,
    )

    try:
        if action == "cancel":
            result = cancel_scheduled_job_sync(job_id, nats_url=nats_url)
            if result:
                console.print(f"[green]Job {job_id} cancelled successfully.[/green]")
            else:
                console.print(
                    f"[yellow]Job {job_id} not found or already cancelled.[/yellow]"
                )

        elif action == "pause":
            result = pause_scheduled_job_sync(job_id, nats_url=nats_url)
            if result:
                console.print(f"[green]Job {job_id} paused successfully.[/green]")
            else:
                console.print(
                    f"[yellow]Failed to pause job {job_id}. Job might not exist or was already paused.[/yellow]"
                )

        elif action == "resume":
            result = resume_scheduled_job_sync(job_id, nats_url=nats_url)
            if result:
                console.print(f"[green]Job {job_id} resumed successfully.[/green]")
            else:
                console.print(
                    f"[yellow]Failed to resume job {job_id}. Job might not exist or was not paused.[/yellow]"
                )

        elif action == "reschedule":
            updates = {}
            if cron:
                updates["cron"] = cron
            if interval is not None:
                updates["interval"] = interval
            if repeat is not None:
                updates["repeat"] = repeat
            if next_run:
                try:
                    next_run_dt = datetime.datetime.fromisoformat(
                        next_run.replace("Z", "+00:00")
                    )
                    updates["scheduled_timestamp_utc"] = next_run_dt.timestamp()
                except ValueError as e:
                    logger.error(
                        f"Invalid next_run format: {e}. Use ISO format (e.g., '2023-01-01T12:00:00Z')"
                    )
                    raise typer.Exit(code=1)

            result = modify_scheduled_job_sync(job_id, nats_url=nats_url, **updates)
            if result:
                console.print(f"[green]Job {job_id} rescheduled successfully.[/green]")

                change_summary = []
                if cron:
                    change_summary.append(f"cron='{cron}'")
                if interval is not None:
                    change_summary.append(f"interval={interval}s")
                if repeat is not None:
                    change_summary.append(f"repeat={repeat}")
                if next_run:
                    change_summary.append(f"next_run={next_run}")

                if change_summary:
                    console.print(
                        Panel(
                            Text("\n".join(f"• {change}" for change in change_summary)),
                            title="Applied Changes",
                            expand=False,
                        )
                    )
            else:
                console.print(
                    f"[yellow]Failed to reschedule job {job_id}. Job might not exist.[/yellow]"
                )
    except Exception as e:
        logger.exception(f"Error controlling job {job_id}: {e}")
        console.print(f"[red]Error: {str(e)}[/red]")


@app.command("list-workers")
def list_workers_command(
    nats_url: str = typer.Option(
        DEFAULT_NATS_URL,
        "--nats-url",
        "-u",
        help="URL of the NATS server.",
        envvar="NAQ_NATS_URL",
    ),
    log_level: Optional[str] = typer.Option(
        None,
        "--log-level",
        "-l",
        help="Set logging level (e.g., DEBUG, INFO, WARNING, ERROR). Defaults to NAQ_LOG_LEVEL env var or CRITICAL.",
    ),
):
    """
    Lists all currently active workers registered in the system.
    """
    setup_logging(log_level if log_level else None)
    logger.info(f"Listing active workers from NATS at {nats_url}")

    try:
        # Use synchronous interface to list workers
        workers = Worker.list_workers_sync(nats_url=nats_url)
        if not workers:
            console.print("[yellow]No active workers found.[/yellow]")
            return

        # Sort workers by ID for consistent output
        workers.sort(key=lambda w: w.get("worker_id", ""))

        table = Table(title="NAQ Workers", show_header=True, header_style="bold cyan")

        # Add columns
        table.add_column("WORKER ID", style="dim", width=45)
        table.add_column("STATUS", width=10)
        table.add_column("QUEUES", width=30)
        table.add_column("CURRENT JOB", width=37)
        table.add_column("LAST HEARTBEAT", width=25)

        # Add rows to the table
        now = time.time()
        for worker in workers:
            worker_id = worker.get("worker_id", "unknown")
            status = worker.get("status", "?")

            # Determine status style
            status_style = "green"
            if status == "busy":
                status_style = "yellow"
            elif status in ["stopping", "starting"]:
                status_style = "blue"

            queues = ", ".join(worker.get("queues", []))
            current_job = (
                worker.get("current_job_id", "-")
                if status == WORKER_STATUS.BUSY
                else "-"
            )

            # Format last heartbeat
            last_hb_ts = worker.get("last_heartbeat_utc")
            if last_hb_ts:
                hb_dt = datetime.datetime.fromtimestamp(last_hb_ts, timezone.utc)
                hb_str = hb_dt.strftime("%Y-%m-%d %H:%M:%S UTC")

                # Check if heartbeat is stale
                if now - last_hb_ts > DEFAULT_WORKER_TTL_SECONDS:
                    hb_str = f"[red]{hb_str} (STALE)[/red]"
            else:
                hb_str = "[italic]never[/italic]"

            # Add row to table
            table.add_row(
                worker_id,
                f"[{status_style}]{status}[/{status_style}]",
                queues,
                current_job,
                hb_str,
            )

        # Print the table
        console.print(table)
        console.print(f"\n[bold]Total:[/bold] {len(workers)} active worker(s)")

    except Exception as e:
        logger.exception(f"Error listing workers: {e}")
        console.print(f"[red]Error listing workers: {str(e)}[/red]")
    finally:
        # Close any NATS connection if opened by the sync wrapper
        try:
            asyncio.run(close_nats_connection())
        except Exception:
            pass


# --- Dashboard Command (Optional) ---
# try:
# Only define the command if dashboard dependencies are installed


@app.command()
def dashboard(
    host: str = typer.Option(
        "127.0.0.1",
        "--host",
        "-h",
        help="Host to bind the dashboard server to.",
        envvar="NAQ_DASHBOARD_HOST",
    ),
    port: int = typer.Option(
        8080,
        "--port",
        "-p",
        help="Port to run the dashboard server on.",
        envvar="NAQ_DASHBOARD_PORT",
    ),
    log_level: Optional[str] = typer.Option(
        None,
        "--log-level",
        "-l",
        help="Set logging level for the dashboard server. Defaults to NAQ_LOG_LEVEL env var or CRITICAL.",
    ),
    # Add NATS URL option if dashboard needs direct NATS access later
    # nats_url: Optional[str] = typer.Option(...)
):
    """
    Starts the NAQ web dashboard (requires 'dashboard' extras).
    """
    import uvicorn  # Use uvicorn to run Sanic

    setup_logging(log_level if log_level else None)  # Setup naq logging if needed
    logger.info(f"Starting NAQ Dashboard server on http://{host}:{port}")
    logger.info("Ensure NATS server is running and accessible.")

    # Configure uvicorn logging level based on input
    uvicorn_log_level = log_level.lower() if log_level else "critical"

    # Run Sanic app using uvicorn
    uvicorn.run(
        "naq.dashboard.app:app",  # Path to the Sanic app instance
        host=host,
        port=port,
        log_level=uvicorn_log_level,
        reload=False,  # Disable auto-reload for production-like command
        # workers=1 # Can configure workers if needed
    )


# except ImportError:
#     @app.command()
#     def dashboard():
#         """
#         Starts the NAQ web dashboard (requires 'dashboard' extras).
#         """
#         console.print("[red]Error:[/red] Dashboard dependencies not installed.")
#         console.print("Please run: [bold cyan]pip install naq[dashboard][/bold cyan]")
#         raise typer.Exit(code=1)


@app.command()
def events(
    nats_url: str = typer.Option(
        DEFAULT_NATS_URL,
        "--nats-url", 
        "-u",
        help="URL of the NATS server.",
        envvar="NAQ_NATS_URL",
    ),
    job_id: Optional[str] = typer.Option(
        None,
        "--job-id",
        "-j",
        help="Filter events for a specific job ID.",
    ),
    event_type: Optional[str] = typer.Option(
        None,
        "--event-type",
        "-e", 
        help="Filter events by type (enqueued, started, completed, failed, retry_scheduled, scheduled, schedule_triggered).",
    ),
    queue_name: Optional[str] = typer.Option(
        None,
        "--queue",
        "-q",
        help="Filter events for a specific queue.",
    ),
    worker_id: Optional[str] = typer.Option(
        None,
        "--worker",
        "-w",
        help="Filter events for a specific worker.",
    ),
    format_output: str = typer.Option(
        "table",
        "--format",
        "-f",
        help="Output format: table, json, or raw.",
    ),
):
    """
    Monitor job events in real-time.
    
    This command connects to the NATS event stream and displays job lifecycle
    events as they occur. Supports filtering by job ID, event type, queue, and worker.
    """
    from .events.processor import AsyncJobEventProcessor
    from .models import JobEventType
    import json
    
    # Validate event type
    event_type_enum = None
    if event_type:
        try:
            event_type_enum = JobEventType(event_type.lower())
        except ValueError:
            console.print(f"[red]Error:[/red] Invalid event type '{event_type}'")
            console.print(f"Valid types: {', '.join([e.value for e in JobEventType])}")
            raise typer.Exit(code=1)
    
    async def run_events_monitor():
        """Run the events monitor."""
        processor = AsyncJobEventProcessor(nats_url=nats_url)
        
        # Create a simple handler to display events
        def display_event(event):
            if format_output == "json":
                console.print(json.dumps(event.to_dict(), indent=2))
            elif format_output == "raw":
                console.print(f"{event.timestamp:.2f} {event.job_id} {event.event_type.value} {event.message or ''}")
            else:  # table format
                table = Table(show_header=True, header_style="bold magenta")
                table.add_column("Time", style="dim")
                table.add_column("Job ID", style="cyan")
                table.add_column("Event", style="green")
                table.add_column("Queue", style="blue")
                table.add_column("Worker", style="yellow")
                table.add_column("Message")
                
                # Format timestamp
                dt = datetime.datetime.fromtimestamp(event.timestamp, tz=timezone.utc)
                time_str = dt.strftime("%H:%M:%S.%f")[:-3]  # Include milliseconds
                
                table.add_row(
                    time_str,
                    event.job_id[:12] + "..." if len(event.job_id) > 15 else event.job_id,
                    event.event_type.value,
                    event.queue_name or "-",
                    event.worker_id[:8] + "..." if event.worker_id and len(event.worker_id) > 11 else (event.worker_id or "-"),
                    event.message or "-"
                )
                console.print(table)
        
        # Register the display handler
        processor.add_global_handler(display_event)
        
        try:
            # Start the processor
            await processor.start()
            
            console.print(Panel(
                "[bold green]NAQ Event Monitor Started[/bold green]\n"
                f"NATS URL: {nats_url}\n"
                f"Filters: job_id={job_id or 'all'}, event_type={event_type or 'all'}, "
                f"queue={queue_name or 'all'}, worker={worker_id or 'all'}\n"
                f"Format: {format_output}\n\n"
                "[dim]Press Ctrl+C to stop[/dim]",
                title="Event Monitor",
                border_style="green"
            ))
            
            # Stream events with filters
            async for event in processor.stream_job_events(
                job_id=job_id,
                event_type=event_type_enum,
                queue_name=queue_name,
                worker_id=worker_id
            ):
                # The display_event handler will be called automatically
                pass
                
        except KeyboardInterrupt:
            console.print("\n[yellow]Event monitoring stopped by user[/yellow]")
        except Exception as e:
            console.print(f"[red]Error:[/red] {e}")
            raise typer.Exit(code=1)
        finally:
            await processor.stop()
    
    # Run the async function
    asyncio.run(run_events_monitor())


@app.command("event-history")
def event_history(
    nats_url: str = typer.Option(
        DEFAULT_NATS_URL,
        "--nats-url", 
        "-u",
        help="URL of the NATS server.",
        envvar="NAQ_NATS_URL",
    ),
    job_id: str = typer.Argument(
        help="Job ID to get event history for.",
    ),
    limit: int = typer.Option(
        50,
        "--limit",
        "-l",
        help="Maximum number of events to retrieve.",
    ),
):
    """
    Get event history for a specific job.
    
    This command retrieves the complete event history for a job from the event stream,
    showing all lifecycle events in chronological order.
    """
    from .events.storage import NATSJobEventStorage
    import json
    
    async def get_job_events():
        """Retrieve events for a specific job."""
        storage = NATSJobEventStorage(nats_url=nats_url)
        
        try:
            events = await storage.get_events(job_id)
            
            if not events:
                console.print(f"[yellow]No events found for job '{job_id}'[/yellow]")
                return
            
            # Limit results
            if len(events) > limit:
                events = events[-limit:]  # Get the most recent events
                console.print(f"[dim]Showing last {limit} events (total: {len(events)})[/dim]\n")
            
            # Create a table for events
            table = Table(show_header=True, header_style="bold magenta", title=f"Event History for Job: {job_id}")
            table.add_column("Timestamp", style="dim")
            table.add_column("Event Type", style="green")
            table.add_column("Queue", style="blue")
            table.add_column("Worker", style="yellow")
            table.add_column("Message")
            table.add_column("Details", style="dim")
            
            for event in events:
                # Format timestamp
                dt = datetime.datetime.fromtimestamp(event.timestamp, tz=timezone.utc)
                time_str = dt.strftime("%Y-%m-%d %H:%M:%S.%f")[:-3]
                
                # Format details
                details_str = ""
                if event.details:
                    key_details = []
                    if event.duration_ms is not None:
                        key_details.append(f"duration: {event.duration_ms}ms")
                    if event.error_type:
                        key_details.append(f"error: {event.error_type}")
                    details_str = ", ".join(key_details)
                
                table.add_row(
                    time_str,
                    event.event_type.value,
                    event.queue_name or "-",
                    (event.worker_id[:8] + "..." if event.worker_id and len(event.worker_id) > 11 else (event.worker_id or "-")),
                    event.message or "-",
                    details_str
                )
            
            console.print(table)
            
        except Exception as e:
            console.print(f"[red]Error retrieving events:[/red] {e}")
            raise typer.Exit(code=1)
        finally:
            await storage.close()
    
    # Run the async function
    asyncio.run(get_job_events())


@app.command("event-stats")
def event_stats(
    nats_url: str = typer.Option(
        DEFAULT_NATS_URL,
        "--nats-url", 
        "-u",
        help="URL of the NATS server.",
        envvar="NAQ_NATS_URL",
    ),
    hours: int = typer.Option(
        24,
        "--hours",
        "-h",
        help="Number of hours to look back for statistics.",
    ),
):
    """
    Show event statistics and system health metrics.
    
    This command provides an overview of system activity by analyzing
    events from the specified time period.
    """
    from .events.storage import NATSJobEventStorage
    from collections import defaultdict, Counter
    import json
    
    async def get_event_statistics():
        """Generate event statistics."""
        storage = NATSJobEventStorage(nats_url=nats_url)
        
        try:
            console.print(f"[dim]Analyzing events from the last {hours} hours...[/dim]\n")
            
            # This would require implementing event querying by time range
            # For now, we'll show a placeholder structure
            
            console.print(Panel(
                "[bold green]Event Statistics[/bold green]\n\n"
                "[yellow]Note:[/yellow] Detailed statistics require time-based event querying,\n"
                "which would be implemented by extending the event storage interface.\n\n"
                "[dim]This would show:[/dim]\n"
                "• Job completion rates\n"
                "• Error rates by queue\n"
                "• Worker activity\n"
                "• Average job duration\n"
                "• Queue throughput",
                title="Event Analytics",
                border_style="blue"
            ))
            
        except Exception as e:
            console.print(f"[red]Error retrieving statistics:[/red] {e}")
            raise typer.Exit(code=1)
        finally:
            await storage.close()
    
    # Run the async function
    asyncio.run(get_event_statistics())


@app.command("worker-events")
def worker_events(
    nats_url: str = typer.Option(
        DEFAULT_NATS_URL,
        "--nats-url", 
        "-u",
        help="URL of the NATS server.",
        envvar="NAQ_NATS_URL",
    ),
    worker_id: Optional[str] = typer.Option(
        None,
        "--worker-id",
        "-w",
        help="Filter events for a specific worker.",
    ),
    format_output: str = typer.Option(
        "table",
        "--format",
        "-f", 
        help="Output format: table, json, or raw.",
    ),
):
    """
    Monitor worker lifecycle events in real-time.
    
    This command shows worker start/stop, status changes, and heartbeat events.
    """
    from .events.processor import AsyncJobEventProcessor
    from .models import JobEventType, WorkerEventType
    import json
    
    async def run_worker_events_monitor():
        """Run the worker events monitor."""
        processor = AsyncJobEventProcessor(nats_url=nats_url)
        
        # Create a handler to display worker events
        def display_worker_event(event):
            # Filter for worker-related events
            if not (hasattr(event, 'details') and event.details and 
                   event.details.get('event_category') == 'worker'):
                return
                
            if format_output == "json":
                console.print(json.dumps(event.to_dict(), indent=2))
            elif format_output == "raw":
                console.print(f"{event.timestamp:.2f} {event.worker_id} {event.event_type.value} {event.message or ''}")
            else:  # table format
                table = Table(show_header=True, header_style="bold cyan")
                table.add_column("Time", style="dim")
                table.add_column("Worker ID", style="cyan")
                table.add_column("Event", style="green")
                table.add_column("Hostname", style="blue")
                table.add_column("Status", style="yellow")
                table.add_column("Message")
                
                # Format timestamp
                dt = datetime.datetime.fromtimestamp(event.timestamp, tz=timezone.utc)
                time_str = dt.strftime("%H:%M:%S.%f")[:-3]
                
                # Extract worker details
                hostname = event.details.get('hostname', '-') if event.details else '-'
                status_info = ""
                if event.details:
                    if event.details.get('active_jobs') is not None:
                        active = event.details.get('active_jobs', 0)
                        limit = event.details.get('concurrency_limit', 0)
                        status_info = f"{active}/{limit} jobs"
                
                table.add_row(
                    time_str,
                    event.worker_id[:12] + "..." if len(event.worker_id) > 15 else event.worker_id,
                    event.event_type.value,
                    hostname,
                    status_info,
                    event.message or "-"
                )
                console.print(table)
        
        # Register the display handler
        processor.add_global_handler(display_worker_event)
        
        try:
            # Start the processor
            await processor.start()
            
            console.print(Panel(
                "[bold cyan]NAQ Worker Events Monitor Started[/bold cyan]\n"
                f"NATS URL: {nats_url}\n"
                f"Worker Filter: {worker_id or 'all'}\n"
                f"Format: {format_output}\n\n"
                "[dim]Press Ctrl+C to stop[/dim]",
                title="Worker Events Monitor",
                border_style="cyan"
            ))
            
            # Stream all events and filter for worker events in the handler
            async for event in processor.stream_job_events():
                # The display_worker_event handler will filter and display relevant events
                pass
                
        except KeyboardInterrupt:
            console.print("\n[yellow]Worker event monitoring stopped by user[/yellow]")
        except Exception as e:
            console.print(f"[red]Error:[/red] {e}")
            raise typer.Exit(code=1)
        finally:
            await processor.stop()
    
    # Run the async function
    asyncio.run(run_worker_events_monitor())


# --- Version Option ---


@app.callback()
def main(
    version: Optional[bool] = typer.Option(
        None,
        "--version",
        callback=version_callback,
        is_eager=True,
        help="Show the application's version and exit.",
    ),
):
    """
    naq CLI entry point.
    """
    pass


# --- Main Execution Guard ---
# (Typer handles this implicitly when run as a script via the entry point)
